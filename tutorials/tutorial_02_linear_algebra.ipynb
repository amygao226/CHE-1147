{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPS9UQeCeDMb0Wxv+2MDZ8H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AI4ChemS/CHE-1147/blob/main/tutorials/tutorial_02_linear_algebra.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Why Linear Algebra Matters for ML\n",
        "\n",
        "Machine learning models are built on **linear algebra**.  \n",
        "Almost everything we do with data involves vectors and matrices:\n",
        "\n",
        "- **Data as vectors:**  \n",
        "  Each sample = a vector of features  \n",
        "  (e.g., a molecule described by [MW, boiling point, density]).\n",
        "\n",
        "- **Datasets as matrices:**  \n",
        "  Rows = samples, Columns = features.  \n",
        "  Example: 100 molecules √ó 10 properties ‚Üí a 100√ó10 matrix.\n",
        "\n",
        "- **Model parameters as matrices:**  \n",
        "  Linear regression, neural networks, PCA, and embeddings all rely on matrix multiplications.\n",
        "\n",
        "- **Optimization:**  \n",
        "  Norms and dot products define distances and similarities.  \n",
        "  Eigenvectors and SVD power dimensionality reduction.\n",
        "\n",
        "üëâ By reviewing a few essentials, you‚Äôll be able to:  \n",
        "- Understand ML math notation quickly.  \n",
        "- Write correct and efficient NumPy code.  \n",
        "- Connect concepts (e.g., regression = solving linear systems, PCA = eigen decomposition).\n"
      ],
      "metadata": {
        "id": "MNGZ5hoEJq4p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Vectors\n",
        "\n",
        "A **vector** is just an ordered list of numbers.  \n",
        "- In math: column of numbers  \n",
        "- In ML: often represents a **feature vector** (properties of a sample)\n",
        "\n",
        "Example: A molecule described by three features  \n",
        "$$\n",
        "  \\text{CO‚ÇÇ} = [\\text{MW}, \\text{Boiling Point}, \\text{Density}]\n",
        "$$\n"
      ],
      "metadata": {
        "id": "vR2KnLWfJ22X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Example: feature vector for CO2\n",
        "co2 = np.array([44.01, -78.5, 1.98])   # MW, boiling point (¬∞C), density (g/L)\n",
        "print(\"CO2 vector:\", co2)\n",
        "print(\"Shape:\", co2.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlsLMUJYJ7yt",
        "outputId": "2902e8f2-f998-4d75-d85d-ad42d7e0ffcd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CO2 vector: [ 44.01 -78.5    1.98]\n",
            "Shape: (3,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Another molecule: H2O\n",
        "h2o = np.array([18.02, 100.0, 0.997])\n",
        "\n",
        "# Vector addition (elementwise)\n",
        "print(\"CO2 + H2O:\", co2 + h2o)\n",
        "\n",
        "# Scalar multiplication\n",
        "print(\"2 * CO2:\", 2 * co2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfSnTaMjJsIJ",
        "outputId": "a0d9bf1a-1884-498a-e86e-7244e7cf7a4a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CO2 + H2O: [62.03  21.5    2.977]\n",
            "2 * CO2: [  88.02 -157.      3.96]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Addition** ‚Üí combining feature contributions  \n",
        "- **Scalar multiplication** ‚Üí rescaling all features (like changing units)  \n",
        "\n",
        "üëâ In ML, vectors represent **data points**.  \n",
        "A dataset = collection of vectors stacked into a matrix.\n"
      ],
      "metadata": {
        "id": "34K57vP6KAid"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mini Exercise\n",
        "\n",
        "1. Create a vector for methane (CH‚ÇÑ): `[16.04, -161.5, 0.717]`.  \n",
        "2. Add it to the CO‚ÇÇ vector. What does the result mean?  \n",
        "3. Multiply the H‚ÇÇO vector by 0.5. Interpret the result (half the feature values).\n"
      ],
      "metadata": {
        "id": "o1_XEWGxKICT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Dot Product & Norms\n",
        "\n",
        "These two concepts are the backbone of ML:\n",
        "\n",
        "- **Dot product:** measures alignment/similarity of two vectors.  \n",
        "- **Norm:** measures the length (magnitude) of a vector.\n",
        "\n",
        "They show up in:\n",
        "- Cosine similarity (used in embeddings, recommendation systems).  \n",
        "- Distances between data points.  \n",
        "- Optimization (gradient descent steps use vector norms).\n"
      ],
      "metadata": {
        "id": "-bsAbZhwKVR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Feature vectors for two molecules\n",
        "co2 = np.array([44.01, -78.5, 1.98])   # MW, BP, density\n",
        "h2o = np.array([18.02, 100.0, 0.997])\n",
        "\n",
        "# Dot product\n",
        "dot = np.dot(co2, h2o)\n",
        "print(\"Dot product:\", dot)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5krUNOlKHlt",
        "outputId": "84e0a134-807d-45b8-809a-cf448cd2f8ac"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dot product: -7054.965740000001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dot product is the sum of elementwise multiplications:\n",
        "\n",
        "$$\n",
        "\\mathbf{a}\\cdot\\mathbf{b}=\\sum_{i=1}^{n} a_i\\,b_i\n",
        "$$\n",
        "\n",
        "\n",
        "- Large positive ‚Üí vectors point in similar directions.  \n",
        "- Near zero ‚Üí vectors are orthogonal (unrelated).  \n",
        "- Negative ‚Üí vectors point in opposite directions.\n"
      ],
      "metadata": {
        "id": "zsBK3lNqKd-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# L2 norm (Euclidean length)\n",
        "norm_co2 = np.linalg.norm(co2)\n",
        "print(\"‚ÄñCO2‚Äñ (L2 norm):\", norm_co2)\n",
        "\n",
        "# L1 norm (sum of absolute values)\n",
        "norm_co2_L1 = np.linalg.norm(co2, ord=1)\n",
        "print(\"‚ÄñCO2‚Äñ (L1 norm):\", norm_co2_L1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9NcCJU5Kk1e",
        "outputId": "83ac87db-474c-4065-a862-7d2c671590b0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ÄñCO2‚Äñ (L2 norm): 90.01694562692072\n",
            "‚ÄñCO2‚Äñ (L1 norm): 124.49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **L2 norm:** usual Euclidean distance (length of vector).  \n",
        "- **L1 norm:** sum of absolute values, often used for sparsity (e.g., Lasso regression).  \n",
        "\n",
        "In ML:\n",
        "- Norms measure the size of weights (regularization).  \n",
        "- Norms measure distances between feature vectors.\n"
      ],
      "metadata": {
        "id": "BIQI2t84KsHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cosine similarity = dot(a, b) / (‚Äña‚Äñ‚Äñb‚Äñ)\n",
        "cos_sim = np.dot(co2, h2o) / (np.linalg.norm(co2) * np.linalg.norm(h2o))\n",
        "print(\"Cosine similarity (CO2 vs H2O):\", cos_sim)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LO4ZSIBEKpe1",
        "outputId": "72c25bc5-a632-4e74-b2a7-5dc782152d79"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity (CO2 vs H2O): -0.7712773573100061\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cosine similarity measures the **angle** between vectors.  \n",
        "- 1 = pointing in the same direction (very similar).  \n",
        "- 0 = orthogonal (no relation).  \n",
        "- -1 = opposite directions.  \n",
        "\n",
        "üëâ In ML, cosine similarity is used for:\n",
        "- Word embeddings (NLP)  \n",
        "- Chemical/material embeddings  \n",
        "- Nearest-neighbor search\n"
      ],
      "metadata": {
        "id": "SaBCdaXvK1ym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mini Exercise\n",
        "\n",
        "1. Create a vector for methane: `[16.04, -161.5, 0.717]`.  \n",
        "2. Compute:\n",
        "   - Dot product with CO‚ÇÇ.  \n",
        "   - L2 norm.  \n",
        "   - Cosine similarity with H‚ÇÇO.  \n",
        "3. Interpret the results: what does similarity mean in this feature space?\n"
      ],
      "metadata": {
        "id": "IEyrwqdeK5cB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Matrices\n",
        "\n",
        "A **matrix** is a 2D collection of numbers.  \n",
        "- In ML: rows = samples, columns = features.  \n",
        "- Example: dataset of molecules with 3 properties each ‚Üí a matrix.\n",
        "\n",
        "Notation:\n",
        "- Vector = 1D array (shape `(n,)`)  \n",
        "- Matrix = 2D array (shape `(m, n)`)  \n"
      ],
      "metadata": {
        "id": "cq1ggiUKLZ7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Each row = [MW, Boiling Point, Density]\n",
        "data = np.array([\n",
        "    [44.01, -78.5, 1.98],   # CO2\n",
        "    [18.02, 100.0, 0.997],  # H2O\n",
        "    [16.04, -161.5, 0.717], # CH4\n",
        "    [32.00, -183.0, 1.429]  # O2\n",
        "])\n",
        "\n",
        "print(\"Dataset matrix:\\n\", data)\n",
        "print(\"Shape:\", data.shape)   # 4 samples √ó 3 features\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFdnaeROLcCm",
        "outputId": "7718c4c4-2b36-4867-9a6b-fbe202026ffd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset matrix:\n",
            " [[  44.01   -78.5      1.98 ]\n",
            " [  18.02   100.       0.997]\n",
            " [  16.04  -161.5      0.717]\n",
            " [  32.    -183.       1.429]]\n",
            "Shape: (4, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Indexing & slicing\n",
        "\n",
        "# First row (CO2 vector)\n",
        "print(\"CO2:\", data[0])\n",
        "\n",
        "# First column (all MWs)\n",
        "print(\"Molecular weights:\", data[:, 0])\n",
        "\n",
        "# Submatrix: first 2 molecules, first 2 features\n",
        "print(\"Submatrix:\\n\", data[0:2, 0:2])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0uCy41NLenV",
        "outputId": "fbdb9e2a-a65b-4607-9727-0244084a1446"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CO2: [ 44.01 -78.5    1.98]\n",
            "Molecular weights: [44.01 18.02 16.04 32.  ]\n",
            "Submatrix:\n",
            " [[ 44.01 -78.5 ]\n",
            " [ 18.02 100.  ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transpose swaps rows and columns\n",
        "print(\"Original shape:\", data.shape)\n",
        "print(\"Transpose shape:\", data.T.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_vq_3vsLiHY",
        "outputId": "ffe606fb-c934-4325-d5de-bf9fbd7b6e24"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original shape: (4, 3)\n",
            "Transpose shape: (3, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üëâ Transpose is common in ML when we switch between:\n",
        "- Data in **row-major form** (samples = rows).  \n",
        "- Weight matrices that expect column vectors.  \n"
      ],
      "metadata": {
        "id": "QSNbKXmaLpsW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Row** = one molecule‚Äôs feature vector.  \n",
        "- **Column** = one property across all molecules.\n",
        "\n",
        "This dual view is why matrix algebra is so central:  \n",
        "- Operations can apply across rows (samples) or columns (features).\n"
      ],
      "metadata": {
        "id": "6wQCoK7oLsTs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mini Exercise\n",
        "\n",
        "1. Print the boiling points column (2nd column).  \n",
        "2. What is the average boiling point?  \n",
        "3. Extract the submatrix of `[MW, Density]` for H2O and CH4 only.  \n"
      ],
      "metadata": {
        "id": "OyAfsYOnLvvV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Matrix Multiplication\n",
        "\n",
        "Matrix multiplication is everywhere in ML:\n",
        "- Linear regression ‚Üí predictions = `X @ w`\n",
        "- Neural networks ‚Üí layer outputs = `X @ W + b`\n",
        "- PCA ‚Üí projecting data onto principal components\n",
        "\n",
        "**Rule:** To multiply `A (m √ó n)` and `B (n √ó p)`,  \n",
        "the inner dimensions must match (`n` = `n`), result is `m √ó p`.\n"
      ],
      "metadata": {
        "id": "wD7nyLphLzVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "A = np.array([[1, 2],\n",
        "              [3, 4]])\n",
        "B = np.array([[5, 6],\n",
        "              [7, 8]])\n",
        "\n",
        "print(\"A:\\n\", A)\n",
        "print(\"B:\\n\", B)\n",
        "\n",
        "# Matrix multiplication\n",
        "C = A @ B   # same as np.dot(A, B)\n",
        "print(\"A @ B:\\n\", C)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrSFvR6TL4a5",
        "outputId": "b09144dd-7d90-446d-d9fd-55ec5bc5368b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A:\n",
            " [[1 2]\n",
            " [3 4]]\n",
            "B:\n",
            " [[5 6]\n",
            " [7 8]]\n",
            "A @ B:\n",
            " [[19 22]\n",
            " [43 50]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset: rows = molecules, cols = features\n",
        "X = np.array([\n",
        "    [44.01, -78.5, 1.98],   # CO2\n",
        "    [18.02, 100.0, 0.997],  # H2O\n",
        "    [16.04, -161.5, 0.717], # CH4\n",
        "    [32.00, -183.0, 1.429]  # O2\n",
        "])\n",
        "\n",
        "# Weight vector (3 features ‚Üí 1 output)\n",
        "w = np.array([0.01, 0.05, 2.0])\n",
        "\n",
        "# Prediction = X @ w\n",
        "y_pred = X @ w\n",
        "print(\"Predictions:\", y_pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17PmGUpTL7wJ",
        "outputId": "e58e8883-3a99-46ab-912d-da0422aa1434"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [ 0.4751  7.1742 -6.4806 -5.972 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here:\n",
        "- `X` = data matrix (molecules √ó features)  \n",
        "- `w` = weight vector (features ‚Üí output)  \n",
        "- `X @ w` = linear model predictions  \n",
        "\n",
        "üëâ This is exactly what happens in **linear regression** and in every layer of a neural network.\n"
      ],
      "metadata": {
        "id": "CkxISbBML-3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppose we predict 2 different properties\n",
        "W = np.array([\n",
        "    [0.01, 0.02],   # feature 1 weights\n",
        "    [0.05, -0.03],  # feature 2 weights\n",
        "    [2.0, 1.0]      # feature 3 weights\n",
        "])\n",
        "\n",
        "Y_pred = X @ W\n",
        "print(\"Predictions (4 samples √ó 2 outputs):\\n\", Y_pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOBHpMfzMBuU",
        "outputId": "828eef39-4334-404d-873f-fdf140f767dc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions (4 samples √ó 2 outputs):\n",
            " [[ 0.4751  5.2152]\n",
            " [ 7.1742 -1.6426]\n",
            " [-6.4806  5.8828]\n",
            " [-5.972   7.559 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üëâ Each column of `Y_pred` is a different predicted property.  \n",
        "This is how neural nets handle **multi-output prediction**.\n"
      ],
      "metadata": {
        "id": "szgm1TSZMDTY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Linear Systems (Ax = b)\n",
        "\n",
        "Many ML problems boil down to solving a system of linear equations:\n",
        "\n",
        "\\[\n",
        "A x = b\n",
        "\\]\n",
        "\n",
        "- **A** = matrix (data or coefficients)  \n",
        "- **x** = unknown vector (parameters/weights)  \n",
        "- **b** = output vector (observations)\n",
        "\n",
        "Examples in ML:\n",
        "- Linear regression (solving for weights that best fit data).  \n",
        "- Least squares (finding the best approximate solution when the system doesn‚Äôt have an exact solution).\n"
      ],
      "metadata": {
        "id": "qO79Ro-JMJkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Solve:\n",
        "# 2x + y = 5\n",
        "# x - y = 1\n",
        "\n",
        "A = np.array([[2, 1],\n",
        "              [1, -1]])\n",
        "b = np.array([5, 1])\n",
        "\n",
        "x = np.linalg.solve(A, b)\n",
        "print(\"Solution [x, y]:\", x)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpGwZYbVK2F-",
        "outputId": "f0255324-ec0f-474d-a6e4-bf999ec91fbd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solution [x, y]: [2. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: more equations than unknowns\n",
        "A = np.array([[1, 1],\n",
        "              [1, 2],\n",
        "              [1, 3]])\n",
        "b = np.array([1, 2, 2.5])\n",
        "\n",
        "# Least squares solution (linear regression style)\n",
        "x, residuals, rank, s = np.linalg.lstsq(A, b, rcond=None)\n",
        "print(\"Best-fit solution [intercept, slope]:\", x)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wECuEHGIMMjL",
        "outputId": "63339ea0-3629-40a2-96a9-9dfc35e58e89"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best-fit solution [intercept, slope]: [0.33333333 0.75      ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üëâ This is exactly what happens in **linear regression**:  \n",
        "- `A` = design matrix (features)  \n",
        "- `x` = weights (parameters)  \n",
        "- `b` = observed outputs  \n"
      ],
      "metadata": {
        "id": "_DXczl_qMRww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset: simple 1D input\n",
        "X = np.array([[1, 1],\n",
        "              [1, 2],\n",
        "              [1, 3],\n",
        "              [1, 4]])\n",
        "y = np.array([1, 2, 2.5, 4])\n",
        "\n",
        "# Solve for linear regression coefficients\n",
        "beta, _, _, _ = np.linalg.lstsq(X, y, rcond=None)\n",
        "print(\"Linear regression coefficients [intercept, slope]:\", beta)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4fqfCX5MT1t",
        "outputId": "c24e7fb8-053e-46e0-8975-9dbccccb5ef2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear regression coefficients [intercept, slope]: [6.4220175e-16 9.5000000e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mini Exercise\n",
        "\n",
        "1. Solve the system:\n",
        "   - 3x + y = 9\n",
        "   - 2x - y = 8  \n",
        "\n",
        "2. Construct a dataset with inputs `[1, 2, 3, 4, 5]` and outputs `[2, 3, 4.5, 6, 7.5]`.  \n",
        "   - Build the design matrix `X` with a bias column of ones.  \n",
        "   - Use `np.linalg.lstsq` to fit a linear regression model.  \n",
        "   - What are the slope and intercept?  \n"
      ],
      "metadata": {
        "id": "7UvciLi7MWM1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Determinant & Inverse\n",
        "\n",
        "These concepts tell us whether a matrix is \"invertible\" (non-singular).\n",
        "\n",
        "- **Determinant (det A):**\n",
        "  - If det(A) = 0 ‚Üí matrix is singular (no unique solution).\n",
        "  - If det(A) ‚â† 0 ‚Üí matrix is invertible.\n",
        "\n",
        "- **Inverse (A‚Åª¬π):**\n",
        "  - Solves `A x = b` as `x = A‚Åª¬π b`.\n",
        "  - In ML, we rarely compute the inverse directly (unstable, slow).\n",
        "  - Instead, we use numerical solvers (`np.linalg.solve` or `np.linalg.lstsq`).\n"
      ],
      "metadata": {
        "id": "cSkJY7pKMsbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "A = np.array([[2, 3],\n",
        "              [1, 4]])\n",
        "\n",
        "detA = np.linalg.det(A)\n",
        "print(\"Determinant of A:\", detA)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSojNxCgMvu-",
        "outputId": "8ef79a03-2233-4a33-fb3f-cce00ff3948a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Determinant of A: 5.000000000000001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute inverse\n",
        "A_inv = np.linalg.inv(A)\n",
        "print(\"Inverse of A:\\n\", A_inv)\n",
        "\n",
        "# Verify: A @ A_inv ‚âà Identity\n",
        "I = A @ A_inv\n",
        "print(\"A @ A_inv:\\n\", I)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgyDzwcFMO-x",
        "outputId": "813bfb57-8af0-430d-bcad-842ad7210746"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inverse of A:\n",
            " [[ 0.8 -0.6]\n",
            " [-0.2  0.4]]\n",
            "A @ A_inv:\n",
            " [[ 1.00000000e+00 -1.11022302e-16]\n",
            " [ 0.00000000e+00  1.00000000e+00]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚ö†Ô∏è In ML:\n",
        "- Don‚Äôt compute `A‚Åª¬π` directly for regression (numerical instability).\n",
        "- Use **solvers** instead:\n",
        "  - `np.linalg.solve(A, b)` for exact systems\n",
        "  - `np.linalg.lstsq(A, b)` for least squares\n",
        "\n",
        "üëâ But knowing about determinants & inverses helps understand why some problems have no unique solution.\n"
      ],
      "metadata": {
        "id": "C1XQ-TcaMzLL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mini Exercise\n",
        "\n",
        "1. Compute the determinant of:\n",
        "\n",
        "$$\n",
        "B = \\begin{bmatrix} 1 & 2 \\\\ 2 & 4 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "What does it tell you about invertibility?\n",
        "\n",
        "2. Try calling `np.linalg.inv(B)`. What happens?\n"
      ],
      "metadata": {
        "id": "EcgzDrDcM22X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vsKGB4HnNF5h"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H-jPeiMKMxg3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}