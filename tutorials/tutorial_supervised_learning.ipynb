{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ys6S13KhpjW7"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AI4ChemS/CHE-1147/blob/main/tutorials/tutorial_supervised_learning.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkW4Vw5RpjW_"
      },
      "source": [
        "# Supervised Learning Workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BGmNEnBpjW_"
      },
      "source": [
        "## Machine learning for discovery of MOFs for gas separation applications\n",
        "\n",
        "In this notebook we will build machine learning models that can predict the gas uptake (carbon dioxide and methane) of metal-organic frameworks (MOFs), which are crystalline materials consisting of inorganic metal nodes linked by organic linkers. The discovery of MOFs for carbon capture is needed for emission reduction technologies, as these materials can efficiently adsorb and store greenhouse gases like CO$_2$. Machine learning accelerates this discovery process by enabling the prediction of gas uptake properties from structural and chemical descriptors, reducing the need for time-consuming and costly experiments or simulations.\n",
        "\n",
        "机器学习在金属有机框架（MOFs）气体分离应用中的发现\n",
        "在本笔记本中，我们将构建机器学习模型，用于预测金属有机框架（MOFs）的气体吸附量（包括二氧化碳和甲烷）。MOFs 是由无机金属节点通过有机连接体连接形成的晶体材料。为了实现碳捕集，MOFs 的发现对于减排技术至关重要，因为这些材料能够高效吸附和储存温室气体如 CO₂。机器学习通过利用结构和化学描述符预测气体吸附性能，加速了这一发现过程，从而减少了耗时且成本高昂的实验或模拟需求。\n",
        "\n",
        "\n",
        "![MOF building principle](https://github.com/AI4ChemS/CHE-1147/blob/main/assets/mof_building_principle.png?raw=1)\n",
        "\n",
        "There are two main **learning goals** for our tutorial:\n",
        "\n",
        "1. Understand the typical workflow for machine learning in chemistry and materials. We will cover exploratory data analysis (EDA) and supervised learning (KRR).\n",
        "理解化学与材料科学中机器学习的典型工作流程。我们将涵盖探索性数据分析（EDA）和监督学习（KRR）。\n",
        "\n",
        "2. Get familiar with some Python packages that are useful for data analysis and visualization.\n",
        "\n",
        "At the end of the exercise, you will produce plot like the one below, comparing the predictions of your model against computed values from GCMC simulations.\n",
        "The histograms show the distributions of the errors on the training set (left) and on the test set (right).\n",
        "\n",
        "\n",
        "理解化学与材料科学中机器学习的典型工作流程。我们将涵盖探索性数据分析（EDA）和监督学习（KRR）。\n",
        "\n",
        "熟悉一些在数据分析和可视化中非常有用的 Python 包。\n",
        "\n",
        "在练习结束时，你将生成如下图所示的图表，将模型的预测结果与 GCMC 模拟计算值进行对比。\n",
        "直方图显示了训练集（左）和测试集（右）上的误差分布。\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/AI4ChemS/CHE-1147/blob/main/assets/result.gif?raw=1\" alt=\"Parity interactive\" width=\"700\"/>\n",
        "\n",
        "We will be using scikit-learn for modeling. The [sklearn documentation](https://scikit-learn.org/stable/user_guide.html) is a great source of reference with many explanations and examples. Also, we use Pandas dataframe (df) for data manipulation. You can select columns using their name by running `df[columnname]`. If at any point you think that the dataset is too large for your computer, you can select a subset using `df.sample()` or by making the test set larger in the train/test split (section 2).\n",
        "<img src=\"https://github.com/AI4ChemS/CHE-1147/blob/main/assets/result.gif?raw=1\" alt=\"Parity interactive\" width=\"700\"/>  \n",
        "\n",
        "我们将使用 scikit-learn 进行建模。[sklearn 文档](https://scikit-learn.org/stable/user_guide.html) 是一个很好的参考来源，包含大量解释和示例。同时，我们使用 Pandas 的 DataFrame（df）进行数据操作。你可以通过运行 `df[columnname]` 来选择指定列。如果在某个阶段你觉得数据集对你的电脑来说过大，可以使用 `df.sample()` 选择一个子集，或者在训练/测试划分（第 2 节）中增大测试集的比例。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKZSf8uTpjXA"
      },
      "source": [
        "> **Credit:** This notebook is adapted from our (Mohamad Moosavi and Kevin Jablonka) lecture and hands-on session in MolSim winter school in Amsterdam.\n",
        "> - [MolSim](https://www.compchem.nl/molsim/) course website.\n",
        "> - [kjappelbaum/ml_molsim](https://github.com/kjappelbaum/ml_molsim) on GitHub."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vO76K4zspjXA"
      },
      "source": [
        "## 0. Setup programming environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0N_5nNipjXB"
      },
      "source": [
        "### 0.1 Installing packages\n",
        "\n",
        "If you are running this notebook locally after cloning [UofT-CHE1147](https://github.com/AI4ChemS/CHE1147) repo, simply install the conda environment\n",
        "\n",
        "```python\n",
        "conda env create -f environment.yml\n",
        "conda activate che1147\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoGZo_TBpjXB"
      },
      "source": [
        "If you are running this notebook on Google Colab, please uncomment the lines below (remove the `#`) and execute the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyxsMij8pjXB"
      },
      "outputs": [],
      "source": [
        "# import os, sys, urllib.request\n",
        "\n",
        "# !pip install -r https://github.com/AI4ChemS/CHE1147/raw/refs/heads/main/requirements_colab.txt\n",
        "\n",
        "# CHE1147_DIR = \"/content/che1147_files\"\n",
        "# os.makedirs(CHE1147_DIR, exist_ok=True)\n",
        "# data_URL = \"https://github.com/AI4ChemS/CHE1147/raw/refs/heads/main/data/MOF_CoRE2019.csv\"\n",
        "# descriptors_URL = \"https://github.com/AI4ChemS/CHE1147/raw/refs/heads/main/tutorials/MOF_descriptors.py\"\n",
        "\n",
        "# local_descriptor_path = os.path.join(CHE1147_DIR, \"MOF_descriptors.py\")\n",
        "# urllib.request.urlretrieve(descriptors_URL, local_descriptor_path)\n",
        "\n",
        "# local_data_path = os.path.join(CHE1147_DIR, \"MOF_CoRE2019.csv\")\n",
        "# urllib.request.urlretrieve(data_URL, os.path.join(CHE1147_DIR, \"MOF_CoRE2019.csv\"))\n",
        "\n",
        "# if CHE1147_DIR not in sys.path:\n",
        "#     sys.path.append(CHE1147_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_L0c29DMpjXD"
      },
      "source": [
        "### 0.2 Import packages we will need"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNw5G6DYpjXD"
      },
      "source": [
        "> Note, if you are using colab, you may need to install some of the packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "J77g1AL5pjXD",
        "outputId": "3e376b1c-0611-4da9-cacd-06a15ef4597d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'ydata_profiling'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-505911195.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# pandas is used to read/process data   #用于 生成数据分析报告，自动展示数据统计信息、缺失值、分布、相关性等。\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mydata_profiling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mProfileReport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# machine learning dependencies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ydata_profiling'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# basics\n",
        "import os\n",
        "import numpy as np\n",
        "import pprint as pp            #导入 pretty print，用于美观地打印复杂的数据结构（如字典、列表），方便调试和查看内容。\n",
        "\n",
        "# pandas is used to read/process data   #用于 生成数据分析报告，自动展示数据统计信息、缺失值、分布、相关性等。\n",
        "from ydata_profiling import ProfileReport\n",
        "\n",
        "# machine learning dependencies\n",
        "# scaling of data 数据标准化 / 缩放\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "\n",
        "# train/test split       用于 把数据集分成训练集和测试集\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# model selection 模型选择与超参数优化\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "# the KRR model\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "# linear model\n",
        "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
        "\n",
        "# pipeline to streamline modeling pipelines 用于 将多个处理步骤串联起来，例如先标准化数据，再做降维，最后训练模型，便于统一管理和交叉验证。\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# principal component analysis ：用于 降维，将高维特征投影到低维空间，同时保留尽可能多的方差信息，减少冗余和噪声。\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# polynomial kernel 计算特征之间的 多项式核函数，在核回归或支持向量机中常用于处理非线性关系。\n",
        "from sklearn.metrics.pairwise import polynomial_kernel\n",
        "\n",
        "# Dummy model as baseline 计算特征之间的 多项式核函数，在核回归或支持向量机中常用于处理非线性关系。\n",
        "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
        "\n",
        "# Variance Threshold for feature selection VarianceThreshold：删除 方差低的特征（几乎没有变化的特征），减少冗余。\n",
        "                                             # SelectFromModel：根据训练好的模型的重要性指标选择特征，例如根据回归系数或树模型的特征重要性。\n",
        "from sklearn.feature_selection import VarianceThreshold, SelectFromModel\n",
        "\n",
        "# metrics to measure model performance. accuracy_score：准确率, precision_score：精确率, recall_score：召回率, f1_score：F1 分数（精确率和召回率的调和平均）,回归指标, mean_absolute_error（MAE）：平均绝对误差\n",
        "\n",
        "# mean_squared_error（MSE）：均方误差, max_error：最大误差, mean_absolute_percentage_error（MAPE）：平均绝对百分比误差\n",
        "\n",
        "#这一段导入的都是 模型、数据处理工具和评价指标，为构建、优化和评估机器学习模型做准备。\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
        "                             mean_absolute_error, mean_squared_error, max_error, mean_absolute_percentage_error)\n",
        "\n",
        "# save/load models 常用于 保存训练好的模型 到文件，或者从文件加载模型，而不必每次都重新训练。\n",
        "import joblib\n",
        "\n",
        "# For the permutation importance implementation Parallel 和 delayed 用于 并行计算，加速耗时操作，例如在多核 CPU 上同时计算每个特征的重要性。\n",
        "from joblib import Parallel\n",
        "from joblib import delayed\n",
        "\n",
        "from sklearn.metrics import check_scoring         #用于 验证和获取模型的评分函数，确保评分指标与模型兼容，例如 MAE、R² 等。\n",
        "from sklearn.utils import Bunch                   #Bunch 是一种类似字典的数据结构，但可以通过 点（.）访问属性，方便存储数据或配置。\n",
        "from sklearn.utils import check_random_state      #check_random_state：确保随机数生成器的一致性和可复现性，例如在交叉验证或打乱数据时使用。\n",
        "from sklearn.utils import check_array             #检查输入数据是否为合法的 NumPy 数组，并可进行必要的类型转换或缺失值处理，保证模型输入安全。\n",
        "\n",
        "# plotting\n",
        "import seaborn as sns           #seaborn：基于 matplotlib 的高级绘图库，方便画出漂亮的统计图（如热力图、分布图、箱线图）。\n",
        "import matplotlib.pyplot as plt     #matplotlib.pyplot：Python 最常用的绘图库，用于绘制各种图表，如折线图、散点图、直方图等。\n",
        "%matplotlib inline       #让绘制的图 直接显示在 Notebook 内 而不是弹出新窗口。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40F9DT9QpjXE"
      },
      "source": [
        "### 0.3 Fix the random seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IGs_0GrqpjXF"
      },
      "outputs": [],
      "source": [
        "# add code here\n",
        "#RANDOM_SEED 目的是控制随机性，保证在不同时间或不同机器上运行代码时，结果保持一致。\n",
        "##FILLME 表示你需要自己填入一个具体的数字\n",
        "\n",
        "#RANDOM_SEED = #FILLME\n",
        "RANDOM_SEED = 3\n",
        "np.random.seed(RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-0Iz0XipjXF"
      },
      "source": [
        " $\\color{Aqua}{\\textsf{Short question}}$\n",
        "- We declared a global variable to fix the random seed (`RANDOM_SEED`). Why did we do this?  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zo-rlgElpjXF"
      },
      "source": [
        "### 0.4 Import the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dda-tNyppjXF"
      },
      "source": [
        "The dataset used in this tutorial originates from the publication [\"Understanding the diversity of the metal-organic framework ecosystem\"](https://doi.org/10.1038/s41467-020-17755-8), which explores the application of machine learning for predicting gas adsorption properties in MOFs. This dataset contains geometric and chemical descriptors, as well as simulated gas uptake values, enabling the development and evaluation of regression models for materials discovery.\n",
        "\n",
        "本教程使用的数据集来源于论文《Understanding the diversity of the metal-organic framework ecosystem》，该论文探讨了机器学习在预测 MOFs 气体吸附性能中的应用。该数据集包含几何和化学描述符，以及模拟的气体吸附量，使得开发和评估用于材料发现的回归模型成为可能。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nT3hj_CjpjXF",
        "outputId": "3b96cf9a-612d-4fd0-dc3d-2f8e236e8822"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🌐 Loaded data from GitHub, shape = (5014, 331)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "# load data locally 检测是否在 Colab 运行\n",
        "#get_ipython() 返回当前的 IPython 环境信息.如果字符串中包含 'google.colab'，说明代码在 Google Colab 中运行。这个变量用于后续选择不同的数据路径。\n",
        "\n",
        "running_in_colab = 'google.colab' in str(get_ipython())\n",
        "\n",
        "\n",
        "#设置数据路径\n",
        "#如果在 Colab：数据存放在 /content/che1147_files 文件夹，完整文件路径：/content/che1147_files/MOF_CoRE2019.csv\n",
        "\n",
        "#如果在 本地：数据存放在 ../data 文件夹（相对路径）。完整文件路径：../data/MOF_CoRE2019.csv\n",
        "\n",
        "# os.path.join() 用于 拼接路径，保证在不同操作系统下都能正确处理斜杠。\n",
        "\n",
        "if running_in_colab:\n",
        "    DATA_DIR = \"/content/che1147_files\"\n",
        "    DATA_FILE = os.path.join(DATA_DIR, \"MOF_CoRE2019.csv\")\n",
        "else:\n",
        "    DATA_DIR = \"../data\"\n",
        "    DATA_FILE = os.path.join(DATA_DIR, \"MOF_CoRE2019.csv\")\n",
        "\n",
        "\n",
        "\n",
        "#尝试从本地读取数据：使用 Pandas 的 read_csv() 读取 CSV 文件，存入 df（DataFrame）。\n",
        "\n",
        "#成功读取后打印 ✅，并显示数据形状 df.shape（行数 × 列数）。\n",
        "#捕获 FileNotFoundError（本地 CSV 不存在）。\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(DATA_FILE)\n",
        "    print(f\"✅ Loaded data from {DATA_FILE}, shape = {df.shape}\")\n",
        "\n",
        "\n",
        "#如果本地文件不存在。尝试从 GitHub 远程 URL 读取数据，保证即使本地没有文件也能继续执行。\n",
        "\n",
        "#成功读取后打印 🌐 并显示数据形状\n",
        "\n",
        "except FileNotFoundError:\n",
        "    # Fallback: try to load from GitHub raw URL\n",
        "    url = \"https://github.com/AI4ChemS/CHE1147/raw/refs/heads/main/data/MOF_CoRE2019.csv\"\n",
        "    try:\n",
        "        df = pd.read_csv(url)\n",
        "        print(f\"🌐 Loaded data from GitHub, shape = {df.shape}\")\n",
        "\n",
        " # 如果远程也失败，打印 ❌ 提示找不到文件，然后重新抛出异常以终止程序。\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Could not find {DATA_FILE} locally or on GitHub.\")\n",
        "        raise e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "id": "WRjXCNmLpjXG",
        "outputId": "c0fd9bb1-b5ad-4208-df54-87bba67f942e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          MOFname  ASA [m^2/cm^3]       Df       Di      Dif  NASA [m^2/cm^3]  \\\n",
              "0    XAGCUE_clean            0.00  2.87682  6.41175  6.41175          487.410   \n",
              "1    SOBZEQ_clean         2298.52  5.44324  7.06044  7.04565            0.000   \n",
              "2  AVAQIX01_clean            0.00  3.61603  5.36267  5.34980          600.353   \n",
              "3    INURIS_clean            0.00  3.09799  5.07769  4.57779          253.440   \n",
              "4    KEDNOY_clean            0.00  3.63243  4.98967  4.98020          519.714   \n",
              "\n",
              "   POAV [cm^3/g]   POAVF  PONAV [cm^3/g]   PONAVF  ...  sum-f-lig-T-2  \\\n",
              "0       0.000000  0.0000        0.101882  0.21558  ...        10752.0   \n",
              "1       0.738273  0.6363        0.000000  0.00000  ...         2240.0   \n",
              "2       0.000000  0.0000        0.226377  0.30062  ...         3040.0   \n",
              "3       0.000000  0.0000        0.077296  0.09856  ...        12720.0   \n",
              "4       0.000000  0.0000        0.238159  0.28890  ...         2064.0   \n",
              "\n",
              "   sum-f-lig-T-3  sum-f-lig-S-0  sum-f-lig-S-1  sum-f-lig-S-2  sum-f-lig-S-3  \\\n",
              "0        10752.0       242.0856       523.7232       844.9008      1049.6448   \n",
              "1         2544.0        58.2840       130.2840       194.2144       231.0000   \n",
              "2         2688.0        91.7328       204.2656       276.2848       265.7760   \n",
              "3        13728.0       301.4544       713.6976      1037.0160      1127.9280   \n",
              "4         1920.0        72.0852       153.0144       199.7088       218.5200   \n",
              "\n",
              "   CO2 uptake at 0.15 bar and 298K  CO2 uptake at 16 bar and 298K  \\\n",
              "0                         1.266370                       3.120211   \n",
              "1                         8.224130                      17.486748   \n",
              "2                         3.694178                       5.849020   \n",
              "3                         1.007227                       4.092395   \n",
              "4                         3.617103                       6.170669   \n",
              "\n",
              "   CH4 uptake at 5.8 bar and 298K  CH4 uptake at 65 bar and 298K  \n",
              "0                        1.810078                       2.173372  \n",
              "1                        4.560643                      11.578465  \n",
              "2                        3.859973                       5.251466  \n",
              "3                        2.032925                       3.728986  \n",
              "4                        3.924974                       4.888655  \n",
              "\n",
              "[5 rows x 331 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f3baeb48-cd74-4552-b6b1-b8d942ee2219\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MOFname</th>\n",
              "      <th>ASA [m^2/cm^3]</th>\n",
              "      <th>Df</th>\n",
              "      <th>Di</th>\n",
              "      <th>Dif</th>\n",
              "      <th>NASA [m^2/cm^3]</th>\n",
              "      <th>POAV [cm^3/g]</th>\n",
              "      <th>POAVF</th>\n",
              "      <th>PONAV [cm^3/g]</th>\n",
              "      <th>PONAVF</th>\n",
              "      <th>...</th>\n",
              "      <th>sum-f-lig-T-2</th>\n",
              "      <th>sum-f-lig-T-3</th>\n",
              "      <th>sum-f-lig-S-0</th>\n",
              "      <th>sum-f-lig-S-1</th>\n",
              "      <th>sum-f-lig-S-2</th>\n",
              "      <th>sum-f-lig-S-3</th>\n",
              "      <th>CO2 uptake at 0.15 bar and 298K</th>\n",
              "      <th>CO2 uptake at 16 bar and 298K</th>\n",
              "      <th>CH4 uptake at 5.8 bar and 298K</th>\n",
              "      <th>CH4 uptake at 65 bar and 298K</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>XAGCUE_clean</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.87682</td>\n",
              "      <td>6.41175</td>\n",
              "      <td>6.41175</td>\n",
              "      <td>487.410</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.101882</td>\n",
              "      <td>0.21558</td>\n",
              "      <td>...</td>\n",
              "      <td>10752.0</td>\n",
              "      <td>10752.0</td>\n",
              "      <td>242.0856</td>\n",
              "      <td>523.7232</td>\n",
              "      <td>844.9008</td>\n",
              "      <td>1049.6448</td>\n",
              "      <td>1.266370</td>\n",
              "      <td>3.120211</td>\n",
              "      <td>1.810078</td>\n",
              "      <td>2.173372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SOBZEQ_clean</td>\n",
              "      <td>2298.52</td>\n",
              "      <td>5.44324</td>\n",
              "      <td>7.06044</td>\n",
              "      <td>7.04565</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.738273</td>\n",
              "      <td>0.6363</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>...</td>\n",
              "      <td>2240.0</td>\n",
              "      <td>2544.0</td>\n",
              "      <td>58.2840</td>\n",
              "      <td>130.2840</td>\n",
              "      <td>194.2144</td>\n",
              "      <td>231.0000</td>\n",
              "      <td>8.224130</td>\n",
              "      <td>17.486748</td>\n",
              "      <td>4.560643</td>\n",
              "      <td>11.578465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AVAQIX01_clean</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.61603</td>\n",
              "      <td>5.36267</td>\n",
              "      <td>5.34980</td>\n",
              "      <td>600.353</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.226377</td>\n",
              "      <td>0.30062</td>\n",
              "      <td>...</td>\n",
              "      <td>3040.0</td>\n",
              "      <td>2688.0</td>\n",
              "      <td>91.7328</td>\n",
              "      <td>204.2656</td>\n",
              "      <td>276.2848</td>\n",
              "      <td>265.7760</td>\n",
              "      <td>3.694178</td>\n",
              "      <td>5.849020</td>\n",
              "      <td>3.859973</td>\n",
              "      <td>5.251466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>INURIS_clean</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.09799</td>\n",
              "      <td>5.07769</td>\n",
              "      <td>4.57779</td>\n",
              "      <td>253.440</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.077296</td>\n",
              "      <td>0.09856</td>\n",
              "      <td>...</td>\n",
              "      <td>12720.0</td>\n",
              "      <td>13728.0</td>\n",
              "      <td>301.4544</td>\n",
              "      <td>713.6976</td>\n",
              "      <td>1037.0160</td>\n",
              "      <td>1127.9280</td>\n",
              "      <td>1.007227</td>\n",
              "      <td>4.092395</td>\n",
              "      <td>2.032925</td>\n",
              "      <td>3.728986</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>KEDNOY_clean</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.63243</td>\n",
              "      <td>4.98967</td>\n",
              "      <td>4.98020</td>\n",
              "      <td>519.714</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.238159</td>\n",
              "      <td>0.28890</td>\n",
              "      <td>...</td>\n",
              "      <td>2064.0</td>\n",
              "      <td>1920.0</td>\n",
              "      <td>72.0852</td>\n",
              "      <td>153.0144</td>\n",
              "      <td>199.7088</td>\n",
              "      <td>218.5200</td>\n",
              "      <td>3.617103</td>\n",
              "      <td>6.170669</td>\n",
              "      <td>3.924974</td>\n",
              "      <td>4.888655</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 331 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f3baeb48-cd74-4552-b6b1-b8d942ee2219')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f3baeb48-cd74-4552-b6b1-b8d942ee2219 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f3baeb48-cd74-4552-b6b1-b8d942ee2219');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-87c06e43-6dc1-4145-bcff-f4f07c7082ee\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-87c06e43-6dc1-4145-bcff-f4f07c7082ee')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-87c06e43-6dc1-4145-bcff-f4f07c7082ee button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "df.head()\n",
        "#显示数据表前 5 行。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0eaVNItpjXG"
      },
      "source": [
        "<details>\n",
        "<summary> <font color='green'>Click here for a hint</font></summary>\n",
        "<ul>\n",
        "    <li>Use something like <code>pd.options.display.max_columns=100</code> to adjust how many columns are shown.<code>pd.options.display.max_columns=100</code>  would show at maximum 100 columns. </li>\n",
        "</ul>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pI66g7K9pjXG"
      },
      "source": [
        "Let's also get some basic information ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndXjE2UxpjXG",
        "outputId": "ce2495f4-faf2-4c0c-91c1-53233a349265"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5014 entries, 0 to 5013\n",
            "Columns: 331 entries, MOFname to CH4 uptake at 65 bar and 298K\n",
            "dtypes: float64(330), object(1)\n",
            "memory usage: 12.7+ MB\n"
          ]
        }
      ],
      "source": [
        "df.info()\n",
        "#快速查看数据的总体信息，尤其是列名、数据类型、非空值数量和内存使用情况。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSxZYT4RpjXG"
      },
      "source": [
        " $\\color{Aqua}{\\textsf{Short question}}$\n",
        "- How many materials are in the dataset?\n",
        "- Which datatypes do we deal with?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXunWl-hpjXH"
      },
      "source": [
        "### 0.5 Understanding the data\n",
        "作为描述符，我们将使用孔隙几何描述符，例如密度、孔体积等，以及修正自相关函数（RACs）来描述 MOFs 的化学特性。我们的数据集包含 MOFs 的四个性能属性：\n",
        "\n",
        "* CO₂ 在 0.15 bar 和 298K 下的吸附量\n",
        "* CO₂ 在 16 bar 和 298K 下的吸附量\n",
        "* CH₄ 在 5.8 bar 和 298K 下的吸附量\n",
        "* CH₄ 在 65 bar 和 298K 下的吸附量\n",
        "\n",
        "As descriptors we will use pore geometric descriptors, such as density, pore volume, etc. and [revised autocorrelation functions](https://www.nature.com/articles/s41467-020-17755-8) (RACs) for describing chemistry of MOFs. Our dataset has four properties for MOFs:\n",
        "- CO2 uptake at 0.15 bar and 298K\n",
        "- CO2 uptake at 16 bar and 298K\n",
        "- CH4 uptake at 5.8 bar and 298K\n",
        "- CH4 uptake at 65 bar and 298K\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Let's start with a simple one that is the high-pressure CO$_2$ uptake `df[\"CO2 uptake at 16 bar and 298K\"]`. This is the amount of CO$_2$ (mmol) the MOF can load per gram.\n",
        "\n",
        "我们先从一个简单的属性开始，即高压下的 CO₂ 吸附量：`df[\"CO2 uptake at 16 bar and 298K\"]`。它表示 MOF 每克材料能够吸附的 CO₂ 数量（单位：mmol）。\n",
        "\n",
        "\n",
        "Below, we define three global variables (hence upper case), which are the *names* of our feature and target columns. We will use the `TARGET` for the actual regression and the `TARGET_BINARY` only for the stratified train/test split. The `FEATURES` variable is a list of column names of our dataframe. We imported the names of descriptors from `MOF_descriptors.py`.\n",
        "\n",
        "\n",
        "下面，我们定义了三个全局变量（因此使用大写字母），它们分别是特征列和目标列的名称。\n",
        "\n",
        "* `TARGET` 用于实际的回归模型\n",
        "* `TARGET_BINARY` 仅用于分层训练/测试集划分\n",
        "* `FEATURES` 是数据框中描述符列名的列表\n",
        "\n",
        "描述符的列名是从 `MOF_descriptors.py` 文件中导入的。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UqdfJgBpjXH"
      },
      "outputs": [],
      "source": [
        "# name of descriptors\n",
        "#从 MOF_descriptors.py 文件中导入若干个 描述符列表。这些变量中，每一个都代表一组特征（列名），用于表示 MOF 的不同结构或化学特征。\n",
        "from MOF_descriptors import geometric_descriptors, linker_descriptors, metalcenter_descriptors, functionalgroup_descriptors, summed_linker_descriptors, summed_metalcenter_descriptors, summed_functionalgroup_descriptors\n",
        "\n",
        "#定义目标变量（也就是我们要预测的值）。在这里，目标是：MOF 在 16 bar 压力、298K 温度下的 CO₂ 吸附量。\n",
        "TARGET = \"CO2 uptake at 16 bar and 298K\"\n",
        "\n",
        "\n",
        "\n",
        "#将多组描述符列表拼接（合并）在一起，生成一个大的特征列表。\n",
        "#这些特征列将作为输入（X），TARGET 将作为输出（y），供机器学习模型训练使用。\n",
        "FEATURES = (\n",
        "    geometric_descriptors\n",
        "    + summed_functionalgroup_descriptors\n",
        "    + summed_linker_descriptors\n",
        "    + summed_metalcenter_descriptors\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHCc9AX3pjXH"
      },
      "source": [
        "Examples for pore geometry descriptors (in `geometric_descriptors`) include: $D_i$ (the size of the largest included sphere), $D_f$ (the largest free sphere), and $D_{if}$ (the largest included free sphere) along the pore $-$ three ways of characterizing pore size.\n",
        "\n",
        "孔隙几何描述符（`geometric_descriptors`）的示例包括：\n",
        "\n",
        "* **𝐷ᵢ**（最大内含球直径，the size of the largest included sphere），\n",
        "* **𝐷𝑓**（最大自由球直径，the largest free sphere），\n",
        "* **𝐷ᵢ𝑓**（最大内含自由球直径，the largest included free sphere）。\n",
        "\n",
        "这三种参数用于从不同角度表征孔径大小。\n",
        "\n",
        "\n",
        "![pore diameters](https://github.com/AI4ChemS/CHE-1147/blob/main/assets/spheres.png?raw=1)\n",
        "\n",
        "Also included are the surface area (SA) of the pore, and the probe-occupiable pore volume (POV).\n",
        "More details on the description of pore geometries can be found in [Ongari et al.](https://pubs.acs.org/doi/abs/10.1021/acs.langmuir.7b01682)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOup7KrSpjXH"
      },
      "source": [
        "RACs (in the lists starting with `summed_...`) operate on the structure graph and encode information about the metal center, linkers and the functional groups as differences or products of heuristics that are relevant for inorganic chemistry, such as electronegativity ($\\chi$), connectivity ($T$), identity ($I$), covalent radii ($S$), and nuclear charge ($Z$).\n",
        "\n",
        "RACs（在以 `summed_` 开头的列表中）基于结构图进行操作，通过计算与无机化学相关启发式参数的**差值或乘积**，来编码金属中心、连接基和官能团的信息。\n",
        "这些启发式参数包括：\n",
        "\n",
        "* 电负性（𝜒）、\n",
        "* 连接性（𝑇）、\n",
        "* 元素标识（𝐼）、\n",
        "* 共价半径（𝑆）、\n",
        "* 核电荷（𝑍）。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8O7VQEupjXH"
      },
      "source": [
        "\n",
        "<img src=\"https://github.com/AI4ChemS/CHE-1147/blob/main/assets/racs.png?raw=1\" alt=\"RACs scheme from the lecture\" width=\"700\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhp75r44pjXH"
      },
      "source": [
        "The number in the descriptornames shows the coordination shell that was considered in the calculation of the RACs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TujApr4vpjXI"
      },
      "source": [
        "### 0.6 Basic Data Cleaning and Preparation\n",
        "\n",
        "We perform some data cleaning by removing instances with missing value and removing duplicates.\n",
        "\n",
        "我们通过删除包含缺失值的样本和重复项来进行数据清洗。\n",
        "\n",
        " $\\color{Aqua}{\\textsf{Short question}}$\n",
        "- We we identify a missing value, should we remove that cell, the column or the row with missing value?\n",
        "\n",
        "当我们发现数据中有缺失值时，应该删除那个单元格、整列，还是整行呢？\n",
        "\n",
        "在数据清洗时，通常不会只删除单个“单元格”（因为那样数据仍然不完整）。\n",
        "一般有两种常见做法：\n",
        "\n",
        "删除整行（row） — 如果该行只缺少少量不重要的值。\n",
        "👉 适用于缺失值很少的情况。\n",
        "\n",
        "删除整列（column） — 如果这一列缺失太多或不重要。\n",
        "👉 适用于某列大部分值都缺失时。\n",
        "\n",
        "或者，还可以使用其他方法：\n",
        "3. 填补缺失值（imputation） — 用平均值、中位数或预测值代替缺失值。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCu0G0zdpjXI"
      },
      "outputs": [],
      "source": [
        "#这两行代码是用来统计 DataFrame 中有缺失值（NaN）的行数的。\n",
        "#df.isna()  作用：检查整个 DataFrame 中哪些位置是缺失值（NaN）。\n",
        "#.any(axis=1) 作用：按行（axis=1）检查每一行是否“至少有一个”缺失值。\n",
        "#.sum()   因为 True 会被当作 1，False 当作 0，所以对布尔值求和就能得到“True 的数量”。\n",
        "          #✅ True 表示这个位置是缺失值（NaN）#❌ False 表示这个位置不是缺失值（有数据）\n",
        "\n",
        "num_rows_with_nan = df.isna().any(axis=1).sum()\n",
        "print(f\"Number of rows with NaN values: {num_rows_with_nan}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3jHWfIwpjXI"
      },
      "source": [
        "Write a code to remove the NaN:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5SoWG6opjXI"
      },
      "outputs": [],
      "source": [
        "# FILLME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpecWG7dpjXI"
      },
      "source": [
        "Next, we should remove duplicate rows, ensuring dataset contains only unique samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RDuGqVVpjXI"
      },
      "outputs": [],
      "source": [
        "## FILLME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4P0-FvJTpjXJ"
      },
      "source": [
        " $\\color{Aqua}{\\textsf{Short question}}$\n",
        "- How many duplicated entries did we remove?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZ6FD6WmpjXJ"
      },
      "source": [
        "## 1. Split the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gChl4wlupjXJ"
      },
      "source": [
        "As the first step, we split our data into a training set and a test set. In order to prevent *any* information of the test set from leaking into our model, we split *before* starting to analyze or transform our data.\n",
        "> **Note:** Not doing the split at this stage can cause data leakage.\n",
        "\n",
        "第一步，我们将数据划分为训练集（training set）和测试集（test set）。\n",
        "为了防止测试集中的信息泄露到模型中，我们必须在开始分析或转换数据之前就进行划分。\n",
        "\n",
        "⚠️ 注意：\n",
        "如果不在这个阶段进行划分，可能会导致数据泄漏（data leakage）。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jYdBlZApjXJ"
      },
      "source": [
        "## 1.1 Random splitting\n",
        "\n",
        "A common way of splitting data is random splitting. We can use `sklearn's train_test_split` for doing this.\n",
        "\n",
        "\n",
        "\n",
        "一种常见的数据划分方式是**随机划分**。我们可以使用 **sklearn** 库中的 `train_test_split` 函数来实现。\n",
        "\n",
        "Depending on the size of the dataset, we might have different splitting ratio. Common split rations are:\n",
        "- **80/20 Split**: 80% for training, 20% for testing.\n",
        "- **Train/Validation/Test Split**: For larger datasets, a common split is 70% training, 15% validation, and 15% testing.\n",
        "\n",
        "根据数据集的大小，可以采用不同的划分比例。常见的划分比例包括：\n",
        "\n",
        "* **80/20 划分**：80% 的数据用于训练，20% 的数据用于测试。\n",
        "* **训练/验证/测试划分**：对于较大的数据集，常见的比例是 70% 用于训练，15% 用于验证，15% 用于测试。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "gJtKz5-OpjXO",
        "outputId": "dfcf0027-c39c-4f9a-b12f-e9159c745a1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🌐 Loaded data from GitHub, shape = (5014, 331)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'RANDOM_SEED' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2086610501.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m                                  \u001b[0;31m#这是要划分的数据集。在这里，df 是你的完整数据框（DataFrame）。\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# 80% 的数据将被分配给训练集，20% 的数据分配给测试集。\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRANDOM_SEED\u001b[0m  \u001b[0;31m# Ensure reproducibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m )\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'RANDOM_SEED' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "#调用 train_test_split() 函数，把原始数据集 df 随机划分为两个部分：\n",
        "#df_train —— 训练集，df_test —— 测试集\n",
        "df_train, df_test = train_test_split(\n",
        "    df,                                  #这是要划分的数据集。在这里，df 是你的完整数据框（DataFrame）。\n",
        "    test_size=0.2,  # 80% 的数据将被分配给训练集，20% 的数据分配给测试集。\n",
        "    random_state=RANDOM_SEED  # Ensure reproducibility\n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0CNHv53pjXO"
      },
      "source": [
        "### 1.1. Split with stratification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiF3trGWpjXO"
      },
      "source": [
        "Random splitting may lead to imbalanced class distributions between training and test sets, which can result in biased model evaluation and poor generalization.\n",
        "[Stratification](https://en.wikipedia.org/wiki/Stratified_sampling) ensures that the class distributions (ratio of \"good\" to \"bad\" materials) are the same in the training and test set.\n",
        "\n",
        " $\\color{Aqua}{\\textsf{Short question}}$\n",
        "\n",
        "- Why is this important? What could happen if we would not do this?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmTqEo0lpjXO"
      },
      "source": [
        "For stratification to work, we to define what makes a \"good\" or a \"bad\" material. This requires knowing the target property of interest. Let's start with developing a model for a simple target, which is CO2 uptake at room temperature and high pressure (16bar)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdJeiTmPpjXO"
      },
      "source": [
        " $\\color{Aqua}{\\textsf{Short question}}$\n",
        "- How can we choose a good value to for classifying \"good\" and \"bad\" materials?  \n",
        "\n",
        "<details>\n",
        "<summary> <font color='green'>Click here for a hint</font></summary>\n",
        "<ul>\n",
        "    You can choose it based on the histogram of the property of interest.\n",
        "</ul>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sn1yBdcpjXP"
      },
      "outputs": [],
      "source": [
        "# add code here\n",
        "# Plot histogram of the target\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.histplot(df[TARGET], kde=True, bins=30, color='blue')\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel(\"{TARGET}\", fontsize=12)\n",
        "plt.ylabel(\"Frequency\", fontsize=12)\n",
        "plt.title(\"Histogram of {TARGET}\", fontsize=14)\n",
        "\n",
        "# Show the plot\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWjc3dIQpjXP"
      },
      "source": [
        "\n",
        "Based on this histogram and the tail of the distribution, we will use 15 mmol CO$_2$ / g as the threshold for the uptake, thus binarizing our continuous target variable. We use this threshold to define a new column with binary values of 0 and 1 for bad and good materials."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_50CydQXpjXP"
      },
      "source": [
        " $\\color{Aqua}{\\textsf{Short question}}$\n",
        " - add a column 'target_binary' that encodes whether a material is low performing (`0`) or high perfoming (`1`) by comparing the uptake with the `THRESHOLD`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufeuIT_JpjXP"
      },
      "source": [
        "<details>\n",
        "<summary> <font color='green'>Click here for a hint</font></summary>\n",
        "<ul>\n",
        "    <li> you can use <a href='https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.cut.html'>pd.cut</a>,\n",
        "    <a href='https://stackoverflow.com/questions/4406389/if-else-in-a-list-comprehension'>list comprehension</a>, the <a href='https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Binarizer.html#sklearn.preprocessing.Binarizer'> binarizer in sklearn </a>...) </li>\n",
        "    <li> a list comprehension example: <code> [1 if value > THRESHOLD else 0 for value in df[TARGET]] </code> </li>\n",
        "</ul>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsuPrkKdpjXP"
      },
      "outputs": [],
      "source": [
        "TARGET_BINARY = \"target_binned\" # name of the new binary target column\n",
        "THRESHOLD = df[TARGET].quantile(0.9)  # Top 10% of the dataset\n",
        "df[TARGET_BINARY] = (df[TARGET] > THRESHOLD).astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvoFpMdgpjXP"
      },
      "source": [
        "Now, we can perform the actual split into training and test set using stratified sampling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c612Q8JcpjXQ"
      },
      "source": [
        " $\\color{Aqua}{\\textsf{Short question}}$\n",
        "- select reasonable values for `XX` and `XY` and then perform the test/train splits. What do you consider when making this decision (think about what you would do with really small and really big datasets, what happens if you have only one test point, what happens to the model performance if you have more test points than training points)?\n",
        "- why do we need to perform the split into a training and test set?\n",
        "- would we use the test set to tune the hyperparameters of our model?\n",
        "\n",
        "<details>\n",
        "<summary> <font color='green'>Click here for a hint</font></summary>\n",
        "<ul>\n",
        "    <li>The `size` arguments can either be integers or, often more convenient, decimals like 0.1</li>\n",
        "    <li>When you perform the split into training and test set you need to trade-off bias (pessimistic bias due to little training data) and variance (due to little test data) </li>\n",
        "    <li>A typical split cloud be 70/30, but for huge dataset the test set might be too big and for small datasets the training set might be too small in this way </li>\n",
        "</ul>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28TFd-L8pjXQ"
      },
      "outputs": [],
      "source": [
        "# add code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ovZHGiGpjXQ"
      },
      "outputs": [],
      "source": [
        "df_train_stratified, df_test_stratified = train_test_split(\n",
        "    df,\n",
        "    train_size=XX,\n",
        "    test_size=YY,\n",
        "    random_state=RANDOM_SEED,\n",
        "    stratify= # FILLME\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OU59qsJ0pjXQ"
      },
      "source": [
        "Check if splitting is reasonable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fe16gek7pjXQ"
      },
      "outputs": [],
      "source": [
        "df_train_stratified.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXaNDqFzpjXQ"
      },
      "outputs": [],
      "source": [
        "df_test_stratified.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkbRXBYypjXR"
      },
      "source": [
        "## 2. Exploratory data analysis (EDA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6x2PiYXKpjXR"
      },
      "source": [
        "After we have put the test set aside, we can give the training set a closer look."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vj6yxttWpjXR"
      },
      "source": [
        "## 2.1 ydata profiling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9L5dMA4pjXR"
      },
      "source": [
        "> this part takes long so let's do it at home!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6B6fx-rpjXR"
      },
      "outputs": [],
      "source": [
        "# profile = ProfileReport(df_train_stratified, title=\"EDA Report\", explorative=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcJ16uXFpjXR"
      },
      "outputs": [],
      "source": [
        "# profile.to_notebook_iframe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhABMZfGpjXR"
      },
      "outputs": [],
      "source": [
        "# Save to an html file\n",
        "# profile.to_file(\"MOF_eda_report.html\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiqQG4FYpjXS"
      },
      "source": [
        "### 2.2 Removing redundant columns\n",
        "\n",
        "We remove any feature that does not have variance across the dataset to reduce dimensionality.\n",
        "Write a code below that makes a list of features (`redundant_features`) with zero variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "potkKq2KpjXS"
      },
      "outputs": [],
      "source": [
        "# FILLME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBo6SnhapjXS"
      },
      "outputs": [],
      "source": [
        "# update feature set\n",
        "print(f\"Number of features before: {len(FEATURES)}\")\n",
        "FEATURES = [feature for feature in FEATURES if not feature in redundant_features]\n",
        "print(f\"Number of features after: {len(FEATURES)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUApztnPpjXS"
      },
      "source": [
        "### 2.3. Correlations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jm7tSuuspjXS"
      },
      "source": [
        " $\\color{Aqua}{\\textsf{Short question}}$\n",
        "- Plot some features against the target property and calculate the Pearson and Spearman correlation coefficient (what is the different between those correlation coefficients?)\n",
        "- What are the strongest correlations? Did you expect them?\n",
        "- What can be a problem when features are correlated?\n",
        "- *Optional:* Do they change if you switch from CO$_2$ uptake at high pressure to low pressure `CO2 uptake at 0.15 bar and 298K`?  Explain your observation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpMpG0RDpjXS"
      },
      "source": [
        "To get the correlation matrices, you can use the `df.corr(method=)`method on your dataframe (`df`). You might want to calculate not the full correlation matrix but just the correlation of the features with the targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GB1O6LXUpjXS"
      },
      "source": [
        "<details>\n",
        "<summary> <font color='green'>Click here for a hint</font></summary>\n",
        "<ul>\n",
        "    <li> To get the correlation with a target, you can use indexing. E.g. <code>df.corr(method='spearman')[TARGET]</code></li>\n",
        "    <li> use <code>.sort_values()</code> method on the output of `df.corr()` to sort by the value of the correlation coefficient  </li>\n",
        "    <li> Scatter plot of TARGET vs. one descriptor (e.g., Density) </li>\n",
        "</ul>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kx8asGZfpjXT"
      },
      "outputs": [],
      "source": [
        "# add code here\n",
        "# Calculate the correlation of all features with the target\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2K0XAbobpjXT"
      },
      "source": [
        "## 3. Baselines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKFXt0DrpjXT"
      },
      "source": [
        "For machine learning, it is important to have some *baselines* to which one then compares the results of a model. Think of a classification model for some rare disease where we only have 1% postives. A classification model that only predictes the negatives *all the time* will still have a amazingly high accuracy. To be able to understand if our model is really better than such a simple prediction we need to make the simple prediction first. This is what we call a baseline.\n",
        "\n",
        "A baseline could be a really simple model, a basic heuristic or the current state of the art (SOTA).\n",
        "this. We will use a heuristic but if you aim for a publication, a baseline for you will be the state of the art model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cibtn4DMpjXT"
      },
      "source": [
        "For this we use sklearn `Dummy` objects that simply calculate the mean, the median or the most frequent case of the training set, when you run the `fit()` method on them (which takes the features matrix $\\mathbf{X}$ and the labels $\\mathbf{y}$ as arguments.\n",
        "This is, the prediction of a `DummyRegressor` with `mean` strategy will always be the mean, independent of the input (it will not look at the feature matrix!).\n",
        "\n",
        "Instead of using those `sklearn` objects you could also just manually compute the the mean or median of the dataset. But we will use those objects as we can learn in this way how to use estimators in `sklearn` and it is also allows you to test your full pipeline with different (baseline) models.\n",
        "What does this mean? In practice this means that you can use all the regression and classification models shown in the figure below in the same way, they will all have a `fit()` method that accepts `X` and `y` and a predict method that accepts `X` and returns the predictions.\n",
        "\n",
        "\n",
        "<img src=\"https://scikit-learn.org/1.3/_static/ml_map.png\" alt=\"ML Map\" width=\"800\"/>\n",
        "\n",
        "The estimator objects can be always used in the same way\n",
        "\n",
        "<img src=\"https://static.packt-cdn.com/products/9781789800265/graphics/d49a2e95-8f22-42ed-89f1-474b3d028787.png\" alt=\"ML Map\" width=\"400\"/>\n",
        "\n",
        "Using these objects, instead of the mean directly, allows you to easily swap them with other models in pipelines, where one chains many data transformation steps (see section 6)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_tAC4u2pjXT"
      },
      "source": [
        "### 3.1. Build dummy models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVTFu1-ypjXT"
      },
      "source": [
        " $\\color{Aqua}{\\textsf{Short question}}$\n",
        "- If you call `.fit(X, y)` on a `DummyRegressor` does it actually use the `X`? If not, why is there still the place for the `X` in the function? If yes, how does it use it?\n",
        "\n",
        " $\\color{Aqua}{\\textsf{Short question}}$\n",
        "- Create [`DummyRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyRegressor.html) instances for  `mean`, `median`. (e.g. `dummyinstance = DummyRegressor(strategy='mean')`)\n",
        "- Train them on the training data (`dummyinstance.fit(df_train[FEATURES], df_train[TARGET])`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ML_yfblypjXT"
      },
      "source": [
        "<details>\n",
        "<summary> <font color='green'>Click here for hints</font></summary>\n",
        "<ul>\n",
        "    <li> to create <code>DummyRegressor</code> you can for example use <code> dummyregressor_mean = DummyRegressor(strategy='mean') </code> </li>\n",
        "    <li> to see the implementation of the <code>DummyRegressor</code> you can check out <a href=\"https://github.com/scikit-learn/scikit-learn/blob/73732e5a0bc9b72c7049dc699d69aaedbb70ef0a/sklearn/dummy.py#L391\"> the source code on GitHub</a> </li>\n",
        "</ul>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8cmldBtpjXU"
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "# Build DummyRegressors\n",
        "dummyregressor_mean = DummyRegressor(strategy='mean')\n",
        "dummyregressor_median = DummyRegressor( #fillme"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XaGrD0ApjXU"
      },
      "outputs": [],
      "source": [
        "# Fit Dummy Regressors\n",
        "dummyregressor_mean.fit(df_train_stratified[FEATURES], df_train_stratified[TARGET])\n",
        "dummyregressor_median. #fillme"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UO2TFVkTpjXU"
      },
      "source": [
        "#### Evaluate the performance of the dummy models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6D9mRvCpjXU"
      },
      "source": [
        " $\\color{Aqua}{\\textsf{Short questions}}$\n",
        "- Calculate maximum error, mean absolute error and mean square error for the dummy regressors on training and test set. What would you expect those numbers to be?\n",
        "- Do the actual values surprise you?\n",
        "- What does this mean in practice for reporting of metrics/the reasoning behind using baseline models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZuHOeNrpjXU"
      },
      "source": [
        "It can be handy to store our metrics of choice in a nested dictionary ([Python dictionaries are key-value pairs](https://www.tutorialspoint.com/python/python_dictionary.htm)):\n",
        "\n",
        "```python\n",
        "{\n",
        "    'dummyestimator1': {\n",
        "                        'metric_a_key': metric_a_value,\n",
        "                        'metric_b_key': metric_b_value\n",
        "                    },\n",
        "    'dummyestimator2': {\n",
        "                        'metric_a_key': metric_a_value,\n",
        "                        'metric_b_key': metric_b_value\n",
        "                    },\n",
        " }\n",
        "```\n",
        "\n",
        "You will now write functions `get_regression_metrics(model, X, y_true)` that compute the metrics and return this dictionary for a given model. The `predict` method takes the feature matrix $\\mathbf{X}$ as input.\n",
        "\n",
        "In them, we calculate\n",
        "\n",
        "$\\mathrm {MAE} =\\frac{\\sum _{i=1}^{n}\\left|Y_{i}-\\hat{y}_{i}\\right|}{n}.$\n",
        "\n",
        ",\n",
        "\n",
        "$\\mathrm {MSE} = {\\frac {1}{n}}\\sum _{i=1}^{n}(Y_{i}-{\\hat {Y_{i}}})^{2}.$\n",
        "\n",
        "$\\mathrm{MAPE} = \\frac{1}{n} \\sum_{i=1}^{n} \\left| \\frac{Y_i - \\hat{Y}_i}{max (\\epsilon,Y_i)} \\right|$\n",
        "\n",
        "where $\\hat{y}$ are the predictions and, $Y_{i}$ the true values.\n",
        "\n",
        "We also include maximum error which is a good indication for generalization in many cases.\n",
        "\n",
        "See more information on [sklearn's Metrics and scoring: quantifying the quality of predictions](https://scikit-learn.org/stable/modules/model_evaluation.html#mean-absolute-percentage-error)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39z-nUqspjXU"
      },
      "source": [
        "<details>\n",
        "<summary> <font color='green'>Click here for hints</font></summary>\n",
        "<ul>\n",
        "    <li> to perform a prediction using a estimator object, you can call <code> classifier.predict(X) </code> </li>\n",
        "    <li> to calculate metrics, you can for example call <code>accuracy_score(true_values, predicted_values) </code> </li>\n",
        "</ul>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmIt45F0pjXU"
      },
      "outputs": [],
      "source": [
        "def get_regression_metrics(model, X, y_true):\n",
        "    \"\"\"\n",
        "    Get a dicionary with regression metrics:\n",
        "\n",
        "    model: sklearn model with predict method\n",
        "    X: feature matrix\n",
        "    y_true: ground truth labels\n",
        "    \"\"\"\n",
        "    y_predicted = #fillme\n",
        "\n",
        "    mae = #fillme\n",
        "    mse = #fillme\n",
        "    maximum_error = #fillme\n",
        "    mape =  #fillme\n",
        "\n",
        "    metrics_dict = {\n",
        "        'mae': mae,\n",
        "        'mse': mse,\n",
        "        'max_error': maximum_error,\n",
        "        'mape': mape\n",
        "    }\n",
        "\n",
        "    return metrics_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFIeciyJpjXV"
      },
      "outputs": [],
      "source": [
        "dummy_regressors = [\n",
        "    ('mean', dummyregressor_mean),\n",
        "    ('median', dummyregressor_median)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hagqeYjVpjXV"
      },
      "outputs": [],
      "source": [
        "dummy_regressor_results_test = {} # initialize empty dictionary\n",
        "dummy_regressor_results_train = {}\n",
        "\n",
        "# loop over the dummy_regressor list\n",
        "# if you have a tuple regressorname, regressor = (a, b) that is automatically expanded into the variables\n",
        "# a = regressorname, b = regressor\n",
        "for regressorname, regressor in dummy_regressors:\n",
        "    print(f\"Calculating metrics for {regressorname}\")\n",
        "    dummy_regressor_results_test[regressorname] = get_regression_metrics(regressor, df_test[FEATURES], df_test[TARGET])\n",
        "    dummy_regressor_results_train[regressorname] = get_regression_metrics(regressor, df_train[FEATURES], df_train[TARGET])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UX0vq9rfpjXV"
      },
      "outputs": [],
      "source": [
        "print(\"Dummy Regressor Results - Train & Test Set\")\n",
        "\n",
        "for regressorname, metrics in dummy_regressor_results_train.items():\n",
        "    print(f\"{regressorname} (Train): {metrics}\")\n",
        "\n",
        "for regressorname, metrics in dummy_regressor_results_test.items():\n",
        "    print(f\"{regressorname} (Test): {metrics}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sT3OS3AApjXV"
      },
      "source": [
        "## 4. Building a linear regression model\n",
        "\n",
        "In practice, we often rely on optimized libraries such as **scikit-learn** for machine learning models.  \n",
        "They are fast, robust, and widely used.  \n",
        "\n",
        "However, to really understand *what happens under the hood*, it is useful to **implement a simple linear regression model ourselves**.  \n",
        "\n",
        "### Goals for this section:\n",
        "- Implement **gradient descent** for linear regression.\n",
        "- Compare results between **gradient descent** and the **closed-form (normal equation)** solution.\n",
        "- Use **scikit-learn**'s `LinearRegression` as a baseline for comparison.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIrzMMXipjXV"
      },
      "source": [
        "### 4.1 sklearn linear regression\n",
        "\n",
        "Let's first use the sklearn model to get a good baseline for comparison.\n",
        "You can see with few lines below, we can train a machine learning model.\n",
        "The code is very flexible and you can essentially replace the linear regression with other models in sklearn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNzPHk2VpjXW"
      },
      "source": [
        "Write the code below to train and evaluate a linear regression model using sklearn's `LinearRegression` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69EsD4ubpjXW"
      },
      "outputs": [],
      "source": [
        "# Initialize the Linear Regression model\n",
        "linear_regressor = LinearRegression()\n",
        "\n",
        "# Train the model on the training data\n",
        "# FILLME\n",
        "\n",
        "# Evaluate the model on the training and test sets\n",
        "# FILLME\n",
        "\n",
        "# Print the results\n",
        "print(\"Linear Regression Results - Train Set:\", linear_regressor_results_train)\n",
        "print(\"Linear Regression Results - Test Set:\", linear_regressor_results_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4oZz-7CpjXW"
      },
      "source": [
        "### 4.1 Linear regression from scratch\n",
        "\n",
        "Now, let's code a linear regression to see if we understand all parts. Specifically, we are going to code the gradient descent for our regressor to fit its parameters.\n",
        "\n",
        "The class below supports:\n",
        "- `method=\"normal\"`: closed-form solution using the normal equation  \n",
        "- `method=\"gd\"`: gradient descent with configurable learning rate and iterations  \n",
        "- Optional standardization of features for smoother optimization  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a76mt55bpjXW"
      },
      "source": [
        "Code the missing part in the gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJOkbpmhpjXW"
      },
      "outputs": [],
      "source": [
        "class MyLinearRegression:\n",
        "    def __init__(self, fit_intercept=True, standardize=False, method=\"normal\",\n",
        "                 lr=1e-2, n_iters=100, tol=1e-8, random_state=0):\n",
        "        \"\"\"\n",
        "        method: \"normal\" (closed-form) or \"gd\" (gradient descent, MSE loss)\n",
        "        \"\"\"\n",
        "        self.fit_intercept = fit_intercept\n",
        "        self.standardize = standardize\n",
        "        self.method = method\n",
        "        self.lr = lr\n",
        "        self.n_iters = n_iters\n",
        "        self.tol = tol\n",
        "        self.random_state = random_state\n",
        "        self.coef_ = None\n",
        "        self.intercept_ = 0.0\n",
        "        self._x_mean = None\n",
        "        self._x_std = None\n",
        "\n",
        "    def _prepare_X(self, X, fit=False):\n",
        "        X = np.asarray(X, dtype=float)\n",
        "        if fit and self.standardize:\n",
        "            self._x_mean = X.mean(axis=0, keepdims=True)\n",
        "            self._x_std = X.std(axis=0, keepdims=True)\n",
        "            self._x_std[self._x_std == 0] = 1.0\n",
        "        if self.standardize:\n",
        "            Xs = (X - self._x_mean) / self._x_std\n",
        "        else:\n",
        "            Xs = X\n",
        "        if self.fit_intercept:\n",
        "            Xs = np.c_[np.ones((Xs.shape[0], 1)), Xs]\n",
        "        return Xs\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        rng = np.random.default_rng(self.random_state)\n",
        "        X = np.asarray(X, dtype=float)\n",
        "        y = np.asarray(y, dtype=float).reshape(-1,)\n",
        "\n",
        "        Xp = self._prepare_X(X, fit=True)\n",
        "\n",
        "        # NORMAL EQUATION\n",
        "        if self.method == \"normal\":\n",
        "            # theta = (X^T X)^{-1} X^T y  with ridge-like jitter for stability\n",
        "            XT_X = Xp.T @ Xp\n",
        "            # add tiny jitter to diagonal to avoid singular matrix in edge cases\n",
        "            jitter = 1e-12 * np.eye(XT_X.shape[0])\n",
        "            theta = np.linalg.pinv(XT_X + jitter) @ (Xp.T @ y)\n",
        "\n",
        "        # GRADIENT DESCENT\n",
        "        elif self.method == \"gd\":\n",
        "            # Add your code here\n",
        "            n_features = Xp.shape[1]\n",
        "            theta = rng.normal(scale=0.01, size=n_features)\n",
        "            prev_loss = np.inf\n",
        "            for i in range(self.n_iters):\n",
        "                y_pred = Xp @ theta\n",
        "                residuals = y_pred - y\n",
        "                loss = #FILLME\n",
        "                if abs(prev_loss - loss) < self.tol:\n",
        "                    break\n",
        "                prev_loss = loss\n",
        "                grad = (Xp.T @ residuals) / len(y)\n",
        "                theta -= #FILLME\n",
        "        else:\n",
        "            raise ValueError(\"method must be 'normal' or 'gd'\")\n",
        "\n",
        "        # unpack theta into intercept and coefficients\n",
        "        if self.fit_intercept:\n",
        "            self.intercept_ = float(theta[0])\n",
        "            self.coef_ = theta[1:]\n",
        "        else:\n",
        "            self.intercept_ = 0.0\n",
        "            self.coef_ = theta\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.asarray(X, dtype=float)\n",
        "        if self.standardize:\n",
        "            Xs = (X - self._x_mean) / self._x_std\n",
        "        else:\n",
        "            Xs = X\n",
        "        return self.intercept_ + Xs @ self.coef_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAVt0NP8pjXX"
      },
      "outputs": [],
      "source": [
        "# Initialize two Linear Regression model with different optimization methods\n",
        "my_linear_regressor_gd = MyLinearRegression(fit_intercept=True, standardize=True, method=\"gd\")\n",
        "my_linear_regressor_cf = MyLinearRegression(fit_intercept=True, standardize=True, method=\"normal\")\n",
        "\n",
        "# Train the model on the training data\n",
        "my_linear_regressor_gd.fit(df_train_stratified[FEATURES], df_train_stratified[TARGET])\n",
        "my_linear_regressor_cf.fit(df_train_stratified[FEATURES], df_train_stratified[TARGET])\n",
        "\n",
        "# Evaluate the model on the training and test sets\n",
        "my_linear_regressor_gd_results_test = get_regression_metrics(\n",
        "    my_linear_regressor_gd, df_train_stratified[FEATURES], df_train_stratified[TARGET]\n",
        ")\n",
        "my_linear_regressor_cf_results_test = get_regression_metrics(\n",
        "    my_linear_regressor_cf, df_test_stratified[FEATURES], df_test_stratified[TARGET]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTDaPhLKpjXX"
      },
      "outputs": [],
      "source": [
        "# comparing results\n",
        "print(\"Linear Regression Results - sklearn:\", linear_regressor_results_test)\n",
        "print(\"Linear Regression Results - cf:\", my_linear_regressor_cf_results_test)\n",
        "print(\"Linear Regression Results - gd:\", my_linear_regressor_gd_results_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHdZ8YKhpjXX"
      },
      "source": [
        "We can see how gradient descent results would be changing its parameters, namely learning rate and number of iterations.\n",
        "Use the sliders to change the **learning rate** and the **number of iterations**.  \n",
        "We’ll refit our custom gd model, plot the **loss vs. iteration**, and print evaluation metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wyf0jYXFpjXX"
      },
      "outputs": [],
      "source": [
        "my_linear_regressor_gd = MyLinearRegression(fit_intercept=True, standardize=True, method=\"gd\",lr=1e-2, n_iters=1000)\n",
        "my_linear_regressor_gd.fit(df_train_stratified[FEATURES], df_train_stratified[TARGET])\n",
        "my_linear_regressor_gd_results_test = get_regression_metrics(\n",
        "    my_linear_regressor_gd, df_train_stratified[FEATURES], df_train_stratified[TARGET]\n",
        ")\n",
        "print(\"Linear Regression Results - gd:\", my_linear_regressor_gd_results_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8A48G35pjXX"
      },
      "outputs": [],
      "source": [
        "from ipywidgets import interact, FloatLogSlider, IntSlider, fixed\n",
        "from IPython.display import display\n",
        "\n",
        "def run_gd_and_plot(lr, n_iters, df, FEATURES, TARGET, get_regression_metrics):\n",
        "    model = MyLinearRegression(\n",
        "        fit_intercept=True,\n",
        "        standardize=True,\n",
        "        method=\"gd\",\n",
        "        lr=lr,\n",
        "        n_iters=n_iters,\n",
        "        tol=1e-12,\n",
        "        random_state=0\n",
        "    )\n",
        "    model.fit(df[FEATURES].values, df[TARGET].values)\n",
        "\n",
        "    metrics = get_regression_metrics(model, df[FEATURES], df[TARGET])\n",
        "\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.xlabel(\"Iteration\")\n",
        "    plt.ylabel(\"Loss (MSE/2)\")\n",
        "    plt.title(f\"lr={lr:.1e}, iters={n_iters}, MAE={metrics['mae']:.2f}, MAPE={metrics['mape']:.2f}\")\n",
        "    plt.plot(df[TARGET].values, model.predict(df[FEATURES].values), 'o', alpha=0.3)\n",
        "    plt.plot(df[TARGET].values, df[TARGET].values, 'k--', label=\"y=x\")\n",
        "    plt.xlim(0.95*df[TARGET].min(), 1.05*df[TARGET].max())\n",
        "    plt.ylim(0.95*df[TARGET].min(), 1.05*df[TARGET].max())\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ap-J7nqOpjXX"
      },
      "outputs": [],
      "source": [
        "interact(\n",
        "    run_gd_and_plot,\n",
        "    lr=FloatLogSlider(value=1e-4, base=10, min=-5, max=0, step=0.1, description=\"learning rate\"),\n",
        "    n_iters=IntSlider(value=100, min=10, max=10000, step=50, description=\"iterations\"),\n",
        "    df=fixed(df_train_stratified),\n",
        "    FEATURES=fixed(FEATURES),\n",
        "    TARGET=fixed(TARGET),\n",
        "    get_regression_metrics=fixed(get_regression_metrics)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKEJv0DmpjXY"
      },
      "source": [
        "You can find more information about gradient descent in this paper: [A high-bias, low-variance introduction to Machine Learning for physicists](https://doi.org/10.1016/j.physrep.2019.03.001)\n",
        "\n",
        "Below is a schematic illustration from the paper showing how learning rate can affect the outcome of optimization.\n",
        "\n",
        "![Gradient Descent Illustration](https://github.com/AI4ChemS/CHE-1147/blob/main/assets/GD_figure.jpg?raw=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wC2KxnsApjXY"
      },
      "source": [
        "$\\color{Aqua}{\\textsf{Short Exercise}}$\n",
        "- what happens when the learning rate is too small?\n",
        "- What about when it is too large?\n",
        "\n",
        "<details>\n",
        "<summary> <font color='green'>Click here for a hint</font></summary>\n",
        "<ul>\n",
        "    <li> Too small leads to extremely slow convergance and too large can lead to overshoot!</li>\n",
        "</ul>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EojN6-jpjXY"
      },
      "source": [
        "## 5. Build a Kernel Ridge Regression Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9iI5kfGpjXY"
      },
      "source": [
        "Let's build a simple [kernel ridge regression (KRR)](https://emtiyaz.github.io/pcml15/kernel-ridge-regression.pdf) machine learning model and train it with our raw data.\n",
        "You can try different kernels, but we recommend to start with the Gaussian radial basis function ('rbf') kernel.\n",
        "\n",
        " $\\color{Aqua}{\\textsf{Short question}}$\n",
        "- Do you expect this model to perform better than the dummy models?\n",
        "- Train it and then calculate the performance metrics on the training and test set. How do they compare to the performance of the dummy models?\n",
        "- What is the shape of the Kernel and of the weights? (you can check your answer by looking at the `dual_coef_` attribute of the KRR instance. You can get shapes of objects using the `shape` atrribute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9_igu7GpjXY"
      },
      "outputs": [],
      "source": [
        "# Train the model with a Gaussian kernel\n",
        "krr = KernelRidge(kernel='rbf')\n",
        "krr.fit(#fillme)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GLPsC25pjXY"
      },
      "outputs": [],
      "source": [
        "# get the metrics on the train and the test set using the get_regression_metrics functions (as above)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7NI5WhSpjXY"
      },
      "source": [
        "## 6. Evaluate the model performance in detail"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jX6-WKpCpjXZ"
      },
      "source": [
        "We have trained our first machine learning model!\n",
        "We'll first have a closer look at its performance, before learning how to improve it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOqNGs5hpjXZ"
      },
      "source": [
        " $\\color{Aqua}{\\textsf{Short Exercise}}$\n",
        "- Create a parity plot (true values against predictions)for the training and test data\n",
        "- Plot a histogram of the distribution of the training and test errors on the training and test set. Plot the errors also as a function of the true value\n",
        "- Let's assume we would like to use our model for pre-screening a library of millions of porous materials to zoom-in on those with the most promising gas uptake. Could you tolerate the errors of your model?\n",
        "- Compare the parity plots for this model with the ones for the dummy models.\n",
        "Use the plotting functions below the evaluate all the following models you train.\n",
        "\n",
        "For this exercise, it can be handy to save the results in a dictionary, e.g.\n",
        "```(python)\n",
        "res_train = {\n",
        "    'y true': [],\n",
        "    'y pred': []\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuEK9qYXpjXZ"
      },
      "source": [
        "<details>\n",
        "<summary> <font color='green'>Click here for hints for plotting</font></summary>\n",
        "<ul>\n",
        "    <li> If you want to use matplotlib to make the parity plots, you can use the <a href=\"https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.hist2d.html\">hist2d function</a> </li>\n",
        "    <li> To create the frequencies and the edges of a histogram, one can use <code>np.histogram</code></li>\n",
        "</ul>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxrFl0FQpjXZ"
      },
      "outputs": [],
      "source": [
        "# Create dictionaries with training and test results to create parity plots\n",
        "res_train = {\n",
        "    'y true': # fillme using the dataframe,\n",
        "    'y pred': # fillme using the model prediction\n",
        "}\n",
        "\n",
        "res_test = {\n",
        "    'y true': # fillme using the dataframe\n",
        "    'y pred': # fillme using the model prediction\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHo84C35pjXZ"
      },
      "source": [
        "Now, lets calculate the errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYZD5H_IpjXZ"
      },
      "outputs": [],
      "source": [
        "res_train[\"error\"] = res_train[\"y true\"] - res_train[\"y pred\"]\n",
        "res_test[\"error\"] = res_test[\"y true\"] - res_test[\"y pred\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flNhG6N-pjXZ"
      },
      "source": [
        "Now, plot the parity plots and error distributions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGhHF_JppjXZ"
      },
      "source": [
        "<details>\n",
        "<summary> <font color='green'>Click here for hints for plotting</font></summary>\n",
        "If you want interactive plots, you can use the following code:\n",
        "<pre><code>\n",
        "hv.extension(\"bokeh\")\n",
        "hex_train = hv.HexTiles(res_train, [\"y true\", \"y pred\"]).hist(\n",
        "    dimension=[\"y true\", \"y pred\"]\n",
        ")\n",
        "hex_test = hv.HexTiles(res_test, [\"y true\", \"y pred\"]).hist(\n",
        "    dimension=[\"y true\", \"y pred\"]\n",
        ")\n",
        "hex_train + hex_test\n",
        "</code>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJBw5EMypjXZ"
      },
      "outputs": [],
      "source": [
        "# plot it\n",
        "hist_density(res_train['y true'], res_train['y pred'], xlabel='y true', ylabel='y pred', title='Train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AeVmGUirpjXa"
      },
      "outputs": [],
      "source": [
        "hist_density(res_test['y true'], res_test['y pred'], xlabel='y true', ylabel='y pred', title='Test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PV7jZO5hpjXa"
      },
      "source": [
        "## 7. Improve the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uTd7_oTpjXa"
      },
      "source": [
        "Our training set still has a couple of issues you might have noticed:\n",
        "- The feature values are not scaled (different features are measured in different units ...)\n",
        "- Some features are basically constant, i.e. do not contain relevant information and just increase the dimensionality of the problem\n",
        "- Some feature distributions are skewed (which is more relevant for some models than for others ...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oagcFk8PpjXa"
      },
      "source": [
        "$\\color{Aqua}{\\textsf{Short Question}}$\n",
        "- Why might the scaling of the features be relevant for a machine learning model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ok_8SRzspjXa"
      },
      "source": [
        "### 7.1. Standard scaling and building a first pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxtW2qv2pjXa"
      },
      "source": [
        "Given that we will now go beyond training a single model, we will build [Pipelines](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html), which are objects that can collect a selection of transformations and estimators. This makes it quite easy to apply the same set of operations to different datasets. A simple pipeline might be built as follows\n",
        "\n",
        "<img src=\"https://vitalflux.com/wp-content/uploads/2020/08/ML-Pipeline-Page-2-1024x307.png\" alt=\"Pipeline\" width=\"800\"/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_KaZtYmpjXa"
      },
      "source": [
        " $\\color{Aqua}{\\textsf{Short Exercise}}$\n",
        "- Build a pipline that first performs standard scaling and then fits a KRR. Call it `pipe_w_scaling`.\n",
        "- Fit it on the training set\n",
        "- Make predictions, calculate the errors and make the parity plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3p93_TympjXa"
      },
      "source": [
        "<details>\n",
        "<summary> <font color='green'>Click here for hints</font></summary>\n",
        "<ul>\n",
        "    <li> the <code>fit</code>, <code>predict</code> methods also work for pipelines </li>\n",
        "</ul>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ghqPWwXpjXb"
      },
      "outputs": [],
      "source": [
        "pipe_w_scaling = Pipeline(\n",
        "   [\n",
        "       ('scaling', StandardScaler()),\n",
        "       ('krr', #fillme)\n",
        "   ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iz3Ajxm0pjXb"
      },
      "source": [
        "### 7.2. Hyperparameter optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_MUkDMEpjXb"
      },
      "source": [
        "A key component we did not optimize so far are hyperparameters. Those are parameters of the model that we usually cannot learn from the data but have to fix before we train the model.\n",
        "Since we cannot learn those parameters it is not trivial to select them. Hence, what we typically do in practice is to create another set, a \"validation set\", and use it to test models trained with different hyperparameters.\n",
        "\n",
        "The most common approach to hyperparameter optimization is to define a grid of all relevant parameters and to search over the grid for the best model performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjqZYQVDpjXb"
      },
      "source": [
        "$\\color{Aqua}{\\textsf{Short Exercise}}$\n",
        "- Think about which parameters you could optimize in the pipeline. Note that your KRR model has two parameters you can optimize. You can also switch off some steps by setting them to `None'.\n",
        "- For each parameter you need to define a resonable grid to search over.\n",
        "- Recall, what k-fold cross-validation does. Run the hyperparameter optimization using 5-fold cross-validation (you can adjust the number of folds according to your computational resources/impatience. It turns out at k=10 is the [best tradeoff between variance and bias](https://arxiv.org/abs/1811.12808)).\n",
        "Tune the hyperparameters until you are statisfied (e.g., until you cannot improve the cross validated error any more)\n",
        "- Why don't we use the test set for hyperparameter tuning but instead test on the validation set?\n",
        "- Evaluate the model performance by calculating the performance metrics (MAE, MSE, max error) on the training and the test set.\n",
        "- *Optional:* Instead of grid search, try to use random search on the same grid (`RandomizedSearchCV`) and fix the number of evaluations (`n_iter`) to a fraction of the number of evaluations of grid search. What do you observe and conclude?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wf0wzTgfpjXb"
      },
      "source": [
        " $\\color{DarkRed}{\\textsf{Tips}}$\n",
        "- If you want to see what is happening, set the `verbosity` argument of the `GridSearchCV` object to a higher number.\n",
        "\n",
        "- If you want to speed up the optimization, you can run it in parallel by setting the `n_jobs` argument to the number of workers. If you set it to -1 it will use all available cores. *Using all cores might freeze your computer if you do not have enough memory*\n",
        "\n",
        "- If the optimization is too slow, reduce the number of data points in your set, the number of folds or the grid size. Note that it can also be a feasible strategy to first use a coarser grid and the a finer grid for fine-tuning.\n",
        "\n",
        "- For grid search, you need to define a parameter grid, which is a dictionary of the following form:\n",
        "```(python)\n",
        "param_grid = {\n",
        "                    'pipelinestage__parameter': np.logspace(-4,1,10),\n",
        "                    'pipelinestage': [None, TransformerA(), TransformerB()]\n",
        "            }\n",
        "```\n",
        "\n",
        "- After the search, you can access the best model with `.best_estimator_` and the best parameters with `.best_params_` on the GridSearchCV instance. For example `grid_krr.best_estimator_`\n",
        "\n",
        "- If you initialize the GridSearchCV instance with `refit=True` it will automatically train the model with all training data (and not only the training folds from cross-validations)\n",
        "\n",
        "The double underscore (dunder) notation works recursively and specifies the parameters for any pipeline stage.\n",
        "For example, `ovasvm__estimator__cls__C` would specifiy the `C` parameter of the estimator in the one-versus-rest classifier `ovasvm`.\n",
        "\n",
        "You can print all parameters of the pipeline using `print(sorted(pipeline.get_params().keys()))`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBrJTS8RpjXb"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "Be aware that tight grids will drastically increase the number of experiments you will run! In some cases, it can be useful to perform the optimization in steps, i.e., first use a coarse grid and then refine in interesting regions.\n",
        "Alternatively, there are approached like <a href=\"https://www.jmlr.org/papers/volume18/16-558/16-558.pdf\"> hyperband <a> that dynamically adjust the number of data points.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GP8Ji5LJpjXb"
      },
      "source": [
        "<details>\n",
        "<summary> <font color='green'>Click here for hints about pipelines and grid search</font></summary>\n",
        "<ul>\n",
        "    <li> You can use the <code>np.logspace</code> function to generate a grid for values that you want to vary on a logarithmic scale </li>\n",
        "    <li> There are two hyperparameters for KRR: the regularization strength <code>alpha</code> and the Gaussian width  <code>gamma</code> </li>\n",
        "    <li> For the regularization strength, values between 1 and 1e-3 can be reasonable. For gamma you can use the median heuristic, gamma = 1 / median, or values between 1e-3 and 1e3</li>\n",
        "</ul>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32VztEr5pjXc"
      },
      "outputs": [],
      "source": [
        "# Define the parameter grid and the grid search object\n",
        "param_grid = {\n",
        "                    'scaling': [MinMaxScaler(), StandardScaler()], # test different scaling methods\n",
        "                    'krr__alpha': #fillme,\n",
        "                    'krr__#fillme': #fillme\n",
        "            }\n",
        "\n",
        "grid_krr = GridSearchCV(#your pipeline, param_grid=param_grid,\n",
        "                        cv=#number of folds, verbose=2, n_jobs=2)\n",
        "\n",
        "# optional random search\n",
        "#random_krr = RandomizedSearchCV(#your pipeline, param_distributions=param_grid, n_iter=#number of evaluations,\n",
        "#                        cv=#number of folds, verbose=2, n_jobs=2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6JR7YVUpjXc"
      },
      "outputs": [],
      "source": [
        "# run the grid search by calling the fit method\n",
        "grid_krr.fit(#fillme)\n",
        "# optional random search\n",
        "# random_krr.fit(#fillme)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSVrXWYqpjXc"
      },
      "outputs": [],
      "source": [
        "# get the performance metrics\n",
        "get_regression_metrics(#fillme)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQzPpR3VpjXc"
      },
      "source": [
        "<details>\n",
        "<summary> <font color='green'>Click here for some more information about hyperparameter optimization</font></summary>\n",
        "Grid search is not the most efficient way to perform hyperparamter optimization. Even <a href=\"http://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf\">random search was shown to be more efficient</a>. Really efficient though are Bayesian optimization approaches like <a href='https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf)'>TPE</a>. This is implemented in the hyperopt library, which is also installed in your conda environment.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSpCwU0gpjXc"
      },
      "source": [
        "<details>\n",
        "<summary> <font color='green'>Click here for hyperparameter optimization with hyperopt (advanded and optional outlook)</font></summary>\n",
        "    \n",
        "<b>Import the tools we need</b>\n",
        "<code>\n",
        "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, mix, rand, anneal, space_eval\n",
        "from functools import partial\n",
        "</code>    \n",
        "\n",
        "<b>Define the grid</b>\n",
        "<code>\n",
        "param_hyperopt = {\n",
        "    \"krr__alpha\": hp.loguniform(\"krr__alpha\", np.log(0.001), np.log(10)),\n",
        "    \"krr__gamma\": hp.loguniform(\"krr__gamma\", np.log(0.001), np.log(10)),\n",
        "}\n",
        "</code>\n",
        "\n",
        "<b>Define the objective function</b>\n",
        "<code>\n",
        "def objective_function(params):\n",
        "    pipe.set_params(\n",
        "        **{\n",
        "            \"krr__alpha\": params[\"krr__alpha\"],\n",
        "            \"krr__gamma\": params[\"krr__gamma\"],\n",
        "        }\n",
        "    )\n",
        "    score = cross_val_score(\n",
        "        pipe, X_train, y_train, cv=10, scoring=\"neg_mean_absolute_error\"\n",
        "    ).mean()\n",
        "    return {\"loss\": -score, \"status\": STATUS_OK}\n",
        "</code>\n",
        "\n",
        "<b>We will use a search in which we mix random search, annealing and tpe</b>\n",
        "<code>\n",
        "trials = Trials()\n",
        "mix_search = partial(\n",
        "   mix.suggest,\n",
        "   p_suggest=[(0.15, rand.suggest), (0.15, anneal.suggest), (0.7, tpe.suggest)],\n",
        ")\n",
        "</code>\n",
        "\n",
        "<b>Now, we can minimize the objective function.</b>\n",
        "<code>\n",
        "best_param = fmin(\n",
        "        objective_function,\n",
        "        param_hyperopt,\n",
        "        algo=mix_search,\n",
        "        max_evals=MAX_EVALES,\n",
        "        trials=trials,\n",
        "        rstate=np.random.RandomState(RANDOM_SEED),\n",
        "    )\n",
        "</code>\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGdaNdDppjXc"
      },
      "source": [
        "## 10. Influence of Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3VqXghupjXc"
      },
      "source": [
        "## 8. Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAMGeq_fpjXc"
      },
      "source": [
        "Finally, we would like to remove features with low variance. This can be done by setting a variance threshold."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebseeUunpjXd"
      },
      "source": [
        "$\\color{Aqua}{\\textsf{Short Question}}$\n",
        "    \n",
        "- What is the reasoning behind doing this?\n",
        "- When might it go wrong and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uX5CeCuApjXd"
      },
      "source": [
        " $\\color{Aqua}{\\textsf{Short Exercise}}$\n",
        "- Add a variance threshold to the pipeline (select the correct function argument)\n",
        "- Use random search for hyperparameter optimization, retrain the pipeline, and calculate the performance metrics (max error, MAE, MSE) on the training and test set\n",
        "- If you could improve the predictive performance, do not forget to also run the model for the Kaggle competition!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9a_pL_epjXd"
      },
      "outputs": [],
      "source": [
        "# Define the pipeline\n",
        "pipe_variance_threshold = Pipeline(\n",
        "    # fillme with the pipeline steps\n",
        "    [\n",
        "        ('variance_treshold', VarianceThreshold(#fillme with threshold)),\n",
        "        #fillme with remaining pipeline steps\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzo97iPzpjXd"
      },
      "outputs": [],
      "source": [
        "param_grid_variance_threshold = {\n",
        "                    'scaling': [None, StandardScaler()],\n",
        "                    'krr__alpha': #fillme,\n",
        "                    'krr__#fillme': #fillme,\n",
        "                    'variance_treshold__threshold': #fillme\n",
        "            }\n",
        "\n",
        "random_variance_treshold = RandomizedSearchCV(#your pipeline, param_distributions=param_grid, n_iter=#number of evaluations,\n",
        "                        cv=#number of folds, verbose=2, n_jobs=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gqnzTtdpjXd"
      },
      "outputs": [],
      "source": [
        "# Fit the pipeline and run the evaluation\n",
        "random_variance_treshold.fit(#fillme)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oN7GiLOwpjXd"
      },
      "source": [
        "$\\color{Aqua}{\\textsf{Short Exercise (optional)}}$\n",
        "- replace the variance threshold with a model-based feature selection\n",
        "`('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\")))` or [any feature selection method that you would like to try](https://scikit-learn.org/stable/modules/feature_selection.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hi6Ysb1rpjXd"
      },
      "source": [
        "## 9. Saving the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqSaaOREpjXd"
      },
      "source": [
        "Now, that we spent so much time in optimizing our model, we do not want to loose it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPKJGpvypjXd"
      },
      "source": [
        " $\\color{Aqua}{\\textsf{Short Exercise}}$\n",
        "- use the [joblib library](https://scikit-learn.org/stable/modules/model_persistence.html) to save your model\n",
        "- make sure you can load it again\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SI3NWskgpjXe"
      },
      "outputs": [],
      "source": [
        "# Dump your model\n",
        "joblib.dump(model, filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1p952S6pjXe"
      },
      "outputs": [],
      "source": [
        "# Try to load it again\n",
        "model_loaded = joblib.load(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-V3c20N0pjXe"
      },
      "source": [
        " $\\color{Aqua}{\\textsf{Short Exercise}}$\n",
        "- what happens if you set $\\alpha$  to a really small or to large value? Why is this the case explain what the parameter means using the equation derived in the lectures?\n",
        "\n",
        " To test this, fix this value in one of your pipelines, retrain the models (re-optimizing the other hyperparameters) and rerun the performance evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81a6gjt7pjXe"
      },
      "source": [
        "<details>\n",
        "<summary> <font color='green'>Click here for hints</font></summary>\n",
        "<ul>\n",
        "    <li> Check the derivation for ridge regression and KRR in the notes. </li>\n",
        "    <li> Also remember the loss landscapes we discussed in the lectures about LASSO. </li>\n",
        "</ul>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpBnDMjBpjXe"
      },
      "source": [
        "## 11. Submit your best model to Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_khTbXGpjXe"
      },
      "source": [
        "Join the [Kaggle competition](https://www.kaggle.com/competitions/molsim-2024-ml-challenge/host/launch-checklist) for this course!\n",
        "For this you can:\n",
        "- try to continue optimizing your KRR model\n",
        "- try to use a new model ([browse the sklearn documentation](https://scikit-learn.org/) for ideas or check out [xgboost](https://xgboost.readthedocs.io/en/stable/)\n",
        "\n",
        "The important parts for us here are:\n",
        "- that you make an attempt to improve your model, discuss this attempt, and use proper models to measure potential improvement\n",
        "- we will not grade you based on how \"fancy\" or model is or how well it performs but rather on whether you do something reasonable that is well motivated in your discussion\n",
        "- you do not need to try both a model and continue optimizing your model. Doing one of them is, in principle, \"enough\"\n",
        "\n",
        "Use then your best model to create a `submission.csv` with your predictions to join the competition and upload it to the competition site.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJChlU8tpjXe"
      },
      "outputs": [],
      "source": [
        "kaggle_data = pd.read_csv('data/features.csv')\n",
        "kaggle_predictions = #fillme.predict(kaggle_data[FEATURES])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpwyGiVMpjXe"
      },
      "outputs": [],
      "source": [
        "submission = pd.DataFrame({\"id\": kaggle_data[\"id\"], \"prediction\": kaggle_predictions})\n",
        "\n",
        "submission.to_csv(\"submission.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZR3snQ5epjXf"
      },
      "source": [
        "## 12. Interpreting the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeLeB912pjXf"
      },
      "source": [
        "Now, that our model performs decently, we would like to know which features are mainly responsible for this, i.e. how the model performs its reasoning.\n",
        "> In CHE1147, we get back to this part in a couple of weeks. See course schedule for details.\n",
        "\n",
        "One method to do so is the [permutation feature importance technique](https://christophm.github.io/interpretable-ml-book/feature-importance.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUiv4S2mpjXf"
      },
      "source": [
        "$\\color{Aqua}{\\textsf{Short question}}$\n",
        "\n",
        "We use both descriptors that encode the pore geometry (density, pore diameters, surface areas) as well as some that describe the chemistry of the MOF (the RACs).\n",
        "- Would you expect the relative importance of these features to be different for prediction of gas adsorption at high vs low gas pressure?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5o81eyjpjXf"
      },
      "source": [
        "<details>\n",
        "<summary> <font color='green'>Click here for a hint</font></summary>\n",
        "<ul>\n",
        "    <li> <a href=\"https://pubs.acs.org/doi/abs/10.1021/acs.chemmater.8b02257\">An article from Diego et al.</a> (10.1021/acs.chemmater.8b02257) gives some hints.</li>\n",
        "</ul>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qPKxysjpjXf"
      },
      "source": [
        " $\\color{Aqua}{\\textsf{Short Exercise}}$\n",
        "- Complete the function `_calculate_permutation_scores` (which we took from the `sklearn` package) and which is needed to calculate the permutation feature importance using the `permutation_importance` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8kC4gozpjXf"
      },
      "outputs": [],
      "source": [
        "def _calculate_permutation_scores(estimator, X, y, col_idx, random_state,\n",
        "                                  n_repeats, scorer):\n",
        "    \"\"\"Calculate score when `col_idx` is permuted. Based on the sklearn implementation\n",
        "\n",
        "    estimator: sklearn estimator object\n",
        "    X: pd.Dataframe or np.array\n",
        "    y: pd.Dataframe or np.array\n",
        "    col_idx: int\n",
        "    random_state: int\n",
        "    n_repeats: int\n",
        "    scorer: function that takes model, X and y_true as arguments\n",
        "    \"\"\"\n",
        "    random_state = check_random_state(random_state)\n",
        "\n",
        "    X_permuted = X.copy()\n",
        "    scores = np.zeros(n_repeats)\n",
        "    # get the indices\n",
        "    shuffling_idx = np.arange(X.shape[0])\n",
        "    for n_round in range(n_repeats):\n",
        "        # FILL BELOW HERE\n",
        "        # shuffle them (fill in what you want to shuffle)\n",
        "        random_state.shuffle(#fillme)\n",
        "\n",
        "        # Deal with dataframes\n",
        "        if hasattr(X_permuted, \"iloc\"):\n",
        "            # .iloc selects the indices from a dataframe and you give it [row, column]\n",
        "            col = X_permuted.iloc[shuffling_idx, col_idx]\n",
        "            col.index = X_permuted.index\n",
        "            X_permuted.iloc[:, col_idx] = col\n",
        "\n",
        "        # Deal with numpy arrays\n",
        "        else:\n",
        "            # FILL BELOW HERE\n",
        "            # array indexing is [row, column]\n",
        "            X_permuted[:, col_idx] = X_permuted[#fillme]\n",
        "\n",
        "        # Get the scores\n",
        "        feature_score = scorer(estimator, X_permuted, y)\n",
        "\n",
        "        # record the scores in array\n",
        "        scores[n_round] = feature_score\n",
        "\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCnNmxWSpjXf"
      },
      "source": [
        "Nothing to change in the function below, it just call the `_calculate_permutation_scores` function you just completed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLmwcAd8pjXf"
      },
      "outputs": [],
      "source": [
        "def permutation_importance(\n",
        "    estimator,\n",
        "    X,\n",
        "    y,\n",
        "    scoring=\"neg_mean_absolute_error\",\n",
        "    n_repeats=5,\n",
        "    n_jobs=2,\n",
        "    random_state=None,\n",
        "):\n",
        "    \"\"\"Permutation importance for feature evaluation\n",
        "    estimator : object\n",
        "        An estimator that has already been :term:`fitted` and is compatible\n",
        "        with :term:`scorer`.\n",
        "    X : ndarray or DataFrame, shape (n_samples, n_features)\n",
        "        Data on which permutation importance will be computed.\n",
        "    y : array-like or None, shape (n_samples, ) or (n_samples, n_classes)\n",
        "        Targets for supervised or `None` for unsupervised.\n",
        "    scoring : string, callable or None, default=None\n",
        "        Scorer to use. It can be a single\n",
        "        string (see :ref:`scoring_parameter`) or a callable (see\n",
        "        :ref:`scoring`). If None, the estimator's default scorer is used.\n",
        "    n_repeats : int, default=5\n",
        "        Number of times to permute a feature.\n",
        "    n_jobs : int or None, default=2\n",
        "        The number of jobs to use for the computation.\n",
        "        `None` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
        "        `-1` means using all processors. See :term:`Glossary <n_jobs>`\n",
        "        for more details.\n",
        "    random_state : int, RandomState instance, or None, default=None\n",
        "        Pseudo-random number generator to control the permutations of each\n",
        "        feature. See :term:`random_state`.\n",
        "    \"\"\"\n",
        "    # Deal with dataframes\n",
        "    if not hasattr(X, \"iloc\"):\n",
        "        X = check_array(X, force_all_finite=\"allow-nan\", dtype=None)\n",
        "\n",
        "    # Precompute random seed from the random state to be used\n",
        "    # to get a fresh independent RandomState instance for each\n",
        "    # parallel call to _calculate_permutation_scores, irrespective of\n",
        "    # the fact that variables are shared or not depending on the active\n",
        "    # joblib backend (sequential, thread-based or process-based).\n",
        "    random_state = check_random_state(random_state)\n",
        "    random_seed = random_state.randint(np.iinfo(np.int32).max + 1)\n",
        "\n",
        "    # Determine scorer from user options.\n",
        "    scorer = check_scoring(estimator, scoring=scoring)\n",
        "    # get the performance score on the unpermuted data\n",
        "    baseline_score = scorer(estimator, X, y)\n",
        "\n",
        "    # run the permuted evaluations in parallel for each column\n",
        "    scores = Parallel(n_jobs=n_jobs)(\n",
        "        delayed(_calculate_permutation_scores)(\n",
        "            estimator, X, y, col_idx, random_seed, n_repeats, scorer\n",
        "        )\n",
        "        for col_idx in range(X.shape[1])\n",
        "    )\n",
        "\n",
        "    # get difference two\n",
        "    importances = baseline_score - np.array(scores)\n",
        "\n",
        "    # return the results (dictionary)\n",
        "    return Bunch(\n",
        "        importances_mean=np.mean(importances, axis=1),\n",
        "        importances_std=np.std(importances, axis=1),\n",
        "        importances=importances,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FW83-u9pjXg"
      },
      "source": [
        " $\\color{Aqua}{\\textsf{Short Exercise}}$\n",
        "- Use your function to find the five most important features.\n",
        "- Which are they? Did you expect this result?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcMSfCvLpjXg"
      },
      "outputs": [],
      "source": [
        "permutation_results = permutation_importance(#fillme)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKK88mVKpjXg"
      },
      "outputs": [],
      "source": [
        "permutation_results[\"features\"] = FEATURES\n",
        "bars = hv.Bars(\n",
        "    permutation_results, \"features\", [\"importances_mean\", \"importances_std\"]\n",
        ").sort(\"importances_mean\", reverse=True)\n",
        "errors = hv.ErrorBars(\n",
        "    permutation_results, \"features\", vdims=[\"importances_mean\", \"importances_std\"]\n",
        ").sort(\"importances_mean\", reverse=True)\n",
        "\n",
        "bars * errors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nqnelg44pjXg"
      },
      "source": [
        "<details>\n",
        "<summary> <font color='green'>Click here for hints</font></summary>\n",
        "<ul>\n",
        "    <li> To get the top <emph>n</emph> indices of an array <code>a</code>, you can use <code>np.argsort(a)[-n:]</code></li>\n",
        "    <li> Get the feature names from the <code>FEATURES</code> list </li>\n",
        "    <li> combined this might look like <code>np.array(FEATURES)[np.argsort(a)[-n:]]</code></li>\n",
        "</ul>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fbSomZ1pjXg"
      },
      "source": [
        "<details>\n",
        "<summary> <font color='green'>Click here for more information on model interpretation</font></summary>\n",
        "The permutation feature importance technique is not a silver bullet, e.g. there are issues with correlated features.\n",
        "However, it is likely <a href='https://explained.ai/rf-importance/'>a better choice than feature importance, like impurity decrease, derived from random forests</a>).\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iifHn7dpjXg"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUPtkD_BpjXg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gx7LIb8VpjXh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "che1147",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}