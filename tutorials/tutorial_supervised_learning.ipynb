{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ys6S13KhpjW7"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AI4ChemS/CHE-1147/blob/main/tutorials/tutorial_supervised_learning.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkW4Vw5RpjW_"
      },
      "source": [
        "# Supervised Learning Workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BGmNEnBpjW_"
      },
      "source": [
        "## Machine learning for discovery of MOFs for gas separation applications\n",
        "\n",
        "In this notebook we will build machine learning models that can predict the gas uptake (carbon dioxide and methane) of metal-organic frameworks (MOFs), which are crystalline materials consisting of inorganic metal nodes linked by organic linkers. The discovery of MOFs for carbon capture is needed for emission reduction technologies, as these materials can efficiently adsorb and store greenhouse gases like CO$_2$. Machine learning accelerates this discovery process by enabling the prediction of gas uptake properties from structural and chemical descriptors, reducing the need for time-consuming and costly experiments or simulations.\n",
        "\n",
        "æœºå™¨å­¦ä¹ åœ¨é‡‘å±æœ‰æœºæ¡†æ¶ï¼ˆMOFsï¼‰æ°”ä½“åˆ†ç¦»åº”ç”¨ä¸­çš„å‘ç°\n",
        "åœ¨æœ¬ç¬”è®°æœ¬ä¸­ï¼Œæˆ‘ä»¬å°†æ„å»ºæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œç”¨äºé¢„æµ‹é‡‘å±æœ‰æœºæ¡†æ¶ï¼ˆMOFsï¼‰çš„æ°”ä½“å¸é™„é‡ï¼ˆåŒ…æ‹¬äºŒæ°§åŒ–ç¢³å’Œç”²çƒ·ï¼‰ã€‚MOFs æ˜¯ç”±æ— æœºé‡‘å±èŠ‚ç‚¹é€šè¿‡æœ‰æœºè¿æ¥ä½“è¿æ¥å½¢æˆçš„æ™¶ä½“ææ–™ã€‚ä¸ºäº†å®ç°ç¢³æ•é›†ï¼ŒMOFs çš„å‘ç°å¯¹äºå‡æ’æŠ€æœ¯è‡³å…³é‡è¦ï¼Œå› ä¸ºè¿™äº›ææ–™èƒ½å¤Ÿé«˜æ•ˆå¸é™„å’Œå‚¨å­˜æ¸©å®¤æ°”ä½“å¦‚ COâ‚‚ã€‚æœºå™¨å­¦ä¹ é€šè¿‡åˆ©ç”¨ç»“æ„å’ŒåŒ–å­¦æè¿°ç¬¦é¢„æµ‹æ°”ä½“å¸é™„æ€§èƒ½ï¼ŒåŠ é€Ÿäº†è¿™ä¸€å‘ç°è¿‡ç¨‹ï¼Œä»è€Œå‡å°‘äº†è€—æ—¶ä¸”æˆæœ¬é«˜æ˜‚çš„å®éªŒæˆ–æ¨¡æ‹Ÿéœ€æ±‚ã€‚\n",
        "\n",
        "\n",
        "![MOF building principle](https://github.com/AI4ChemS/CHE-1147/blob/main/assets/mof_building_principle.png?raw=1)\n",
        "\n",
        "There are two main **learning goals** for our tutorial:\n",
        "\n",
        "1. Understand the typical workflow for machine learning in chemistry and materials. We will cover exploratory data analysis (EDA) and supervised learning (KRR).\n",
        "ç†è§£åŒ–å­¦ä¸ææ–™ç§‘å­¦ä¸­æœºå™¨å­¦ä¹ çš„å…¸å‹å·¥ä½œæµç¨‹ã€‚æˆ‘ä»¬å°†æ¶µç›–æ¢ç´¢æ€§æ•°æ®åˆ†æï¼ˆEDAï¼‰å’Œç›‘ç£å­¦ä¹ ï¼ˆKRRï¼‰ã€‚\n",
        "\n",
        "2. Get familiar with some Python packages that are useful for data analysis and visualization.\n",
        "\n",
        "At the end of the exercise, you will produce plot like the one below, comparing the predictions of your model against computed values from GCMC simulations.\n",
        "The histograms show the distributions of the errors on the training set (left) and on the test set (right).\n",
        "\n",
        "\n",
        "ç†è§£åŒ–å­¦ä¸ææ–™ç§‘å­¦ä¸­æœºå™¨å­¦ä¹ çš„å…¸å‹å·¥ä½œæµç¨‹ã€‚æˆ‘ä»¬å°†æ¶µç›–æ¢ç´¢æ€§æ•°æ®åˆ†æï¼ˆEDAï¼‰å’Œç›‘ç£å­¦ä¹ ï¼ˆKRRï¼‰ã€‚\n",
        "\n",
        "ç†Ÿæ‚‰ä¸€äº›åœ¨æ•°æ®åˆ†æå’Œå¯è§†åŒ–ä¸­éå¸¸æœ‰ç”¨çš„ Python åŒ…ã€‚\n",
        "\n",
        "åœ¨ç»ƒä¹ ç»“æŸæ—¶ï¼Œä½ å°†ç”Ÿæˆå¦‚ä¸‹å›¾æ‰€ç¤ºçš„å›¾è¡¨ï¼Œå°†æ¨¡å‹çš„é¢„æµ‹ç»“æœä¸ GCMC æ¨¡æ‹Ÿè®¡ç®—å€¼è¿›è¡Œå¯¹æ¯”ã€‚\n",
        "ç›´æ–¹å›¾æ˜¾ç¤ºäº†è®­ç»ƒé›†ï¼ˆå·¦ï¼‰å’Œæµ‹è¯•é›†ï¼ˆå³ï¼‰ä¸Šçš„è¯¯å·®åˆ†å¸ƒã€‚\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/AI4ChemS/CHE-1147/blob/main/assets/result.gif?raw=1\" alt=\"Parity interactive\" width=\"700\"/>\n",
        "\n",
        "We will be using scikit-learn for modeling. The [sklearn documentation](https://scikit-learn.org/stable/user_guide.html) is a great source of reference with many explanations and examples. Also, we use Pandas dataframe (df) for data manipulation. You can select columns using their name by running `df[columnname]`. If at any point you think that the dataset is too large for your computer, you can select a subset using `df.sample()` or by making the test set larger in the train/test split (section 2).\n",
        "<img src=\"https://github.com/AI4ChemS/CHE-1147/blob/main/assets/result.gif?raw=1\" alt=\"Parity interactive\" width=\"700\"/>  \n",
        "\n",
        "æˆ‘ä»¬å°†ä½¿ç”¨ scikit-learn è¿›è¡Œå»ºæ¨¡ã€‚[sklearn æ–‡æ¡£](https://scikit-learn.org/stable/user_guide.html) æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„å‚è€ƒæ¥æºï¼ŒåŒ…å«å¤§é‡è§£é‡Šå’Œç¤ºä¾‹ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬ä½¿ç”¨ Pandas çš„ DataFrameï¼ˆdfï¼‰è¿›è¡Œæ•°æ®æ“ä½œã€‚ä½ å¯ä»¥é€šè¿‡è¿è¡Œ `df[columnname]` æ¥é€‰æ‹©æŒ‡å®šåˆ—ã€‚å¦‚æœåœ¨æŸä¸ªé˜¶æ®µä½ è§‰å¾—æ•°æ®é›†å¯¹ä½ çš„ç”µè„‘æ¥è¯´è¿‡å¤§ï¼Œå¯ä»¥ä½¿ç”¨ `df.sample()` é€‰æ‹©ä¸€ä¸ªå­é›†ï¼Œæˆ–è€…åœ¨è®­ç»ƒ/æµ‹è¯•åˆ’åˆ†ï¼ˆç¬¬ 2 èŠ‚ï¼‰ä¸­å¢å¤§æµ‹è¯•é›†çš„æ¯”ä¾‹ã€‚\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKZSf8uTpjXA"
      },
      "source": [
        "> **Credit:** This notebook is adapted from our (Mohamad Moosavi and Kevin Jablonka) lecture and hands-on session in MolSim winter school in Amsterdam.\n",
        "> - [MolSim](https://www.compchem.nl/molsim/) course website.\n",
        "> - [kjappelbaum/ml_molsim](https://github.com/kjappelbaum/ml_molsim) on GitHub."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vO76K4zspjXA"
      },
      "source": [
        "## 0. Setup programming environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0N_5nNipjXB"
      },
      "source": [
        "### 0.1 Installing packages\n",
        "\n",
        "If you are running this notebook locally after cloning [UofT-CHE1147](https://github.com/AI4ChemS/CHE1147) repo, simply install the conda environment\n",
        "\n",
        "```python\n",
        "conda env create -f environment.yml\n",
        "conda activate che1147\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoGZo_TBpjXB"
      },
      "source": [
        "If you are running this notebook on Google Colab, please uncomment the lines below (remove the `#`) and execute the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyxsMij8pjXB"
      },
      "outputs": [],
      "source": [
        "# import os, sys, urllib.request\n",
        "\n",
        "# !pip install -r https://github.com/AI4ChemS/CHE1147/raw/refs/heads/main/requirements_colab.txt\n",
        "\n",
        "# CHE1147_DIR = \"/content/che1147_files\"\n",
        "# os.makedirs(CHE1147_DIR, exist_ok=True)\n",
        "# data_URL = \"https://github.com/AI4ChemS/CHE1147/raw/refs/heads/main/data/MOF_CoRE2019.csv\"\n",
        "# descriptors_URL = \"https://github.com/AI4ChemS/CHE1147/raw/refs/heads/main/tutorials/MOF_descriptors.py\"\n",
        "\n",
        "# local_descriptor_path = os.path.join(CHE1147_DIR, \"MOF_descriptors.py\")\n",
        "# urllib.request.urlretrieve(descriptors_URL, local_descriptor_path)\n",
        "\n",
        "# local_data_path = os.path.join(CHE1147_DIR, \"MOF_CoRE2019.csv\")\n",
        "# urllib.request.urlretrieve(data_URL, os.path.join(CHE1147_DIR, \"MOF_CoRE2019.csv\"))\n",
        "\n",
        "# if CHE1147_DIR not in sys.path:\n",
        "#     sys.path.append(CHE1147_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_L0c29DMpjXD"
      },
      "source": [
        "### 0.2 Import packages we will need"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNw5G6DYpjXD"
      },
      "source": [
        "> Note, if you are using colab, you may need to install some of the packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "J77g1AL5pjXD",
        "outputId": "3e376b1c-0611-4da9-cacd-06a15ef4597d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'ydata_profiling'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-505911195.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# pandas is used to read/process data   #ç”¨äº ç”Ÿæˆæ•°æ®åˆ†ææŠ¥å‘Šï¼Œè‡ªåŠ¨å±•ç¤ºæ•°æ®ç»Ÿè®¡ä¿¡æ¯ã€ç¼ºå¤±å€¼ã€åˆ†å¸ƒã€ç›¸å…³æ€§ç­‰ã€‚\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mydata_profiling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mProfileReport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# machine learning dependencies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ydata_profiling'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# basics\n",
        "import os\n",
        "import numpy as np\n",
        "import pprint as pp            #å¯¼å…¥ pretty printï¼Œç”¨äºç¾è§‚åœ°æ‰“å°å¤æ‚çš„æ•°æ®ç»“æ„ï¼ˆå¦‚å­—å…¸ã€åˆ—è¡¨ï¼‰ï¼Œæ–¹ä¾¿è°ƒè¯•å’ŒæŸ¥çœ‹å†…å®¹ã€‚\n",
        "\n",
        "# pandas is used to read/process data   #ç”¨äº ç”Ÿæˆæ•°æ®åˆ†ææŠ¥å‘Šï¼Œè‡ªåŠ¨å±•ç¤ºæ•°æ®ç»Ÿè®¡ä¿¡æ¯ã€ç¼ºå¤±å€¼ã€åˆ†å¸ƒã€ç›¸å…³æ€§ç­‰ã€‚\n",
        "from ydata_profiling import ProfileReport\n",
        "\n",
        "# machine learning dependencies\n",
        "# scaling of data æ•°æ®æ ‡å‡†åŒ– / ç¼©æ”¾\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "\n",
        "# train/test split       ç”¨äº æŠŠæ•°æ®é›†åˆ†æˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# model selection æ¨¡å‹é€‰æ‹©ä¸è¶…å‚æ•°ä¼˜åŒ–\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "# the KRR model\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "# linear model\n",
        "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
        "\n",
        "# pipeline to streamline modeling pipelines ç”¨äº å°†å¤šä¸ªå¤„ç†æ­¥éª¤ä¸²è”èµ·æ¥ï¼Œä¾‹å¦‚å…ˆæ ‡å‡†åŒ–æ•°æ®ï¼Œå†åšé™ç»´ï¼Œæœ€åè®­ç»ƒæ¨¡å‹ï¼Œä¾¿äºç»Ÿä¸€ç®¡ç†å’Œäº¤å‰éªŒè¯ã€‚\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# principal component analysis ï¼šç”¨äº é™ç»´ï¼Œå°†é«˜ç»´ç‰¹å¾æŠ•å½±åˆ°ä½ç»´ç©ºé—´ï¼ŒåŒæ—¶ä¿ç•™å°½å¯èƒ½å¤šçš„æ–¹å·®ä¿¡æ¯ï¼Œå‡å°‘å†—ä½™å’Œå™ªå£°ã€‚\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# polynomial kernel è®¡ç®—ç‰¹å¾ä¹‹é—´çš„ å¤šé¡¹å¼æ ¸å‡½æ•°ï¼Œåœ¨æ ¸å›å½’æˆ–æ”¯æŒå‘é‡æœºä¸­å¸¸ç”¨äºå¤„ç†éçº¿æ€§å…³ç³»ã€‚\n",
        "from sklearn.metrics.pairwise import polynomial_kernel\n",
        "\n",
        "# Dummy model as baseline è®¡ç®—ç‰¹å¾ä¹‹é—´çš„ å¤šé¡¹å¼æ ¸å‡½æ•°ï¼Œåœ¨æ ¸å›å½’æˆ–æ”¯æŒå‘é‡æœºä¸­å¸¸ç”¨äºå¤„ç†éçº¿æ€§å…³ç³»ã€‚\n",
        "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
        "\n",
        "# Variance Threshold for feature selection VarianceThresholdï¼šåˆ é™¤ æ–¹å·®ä½çš„ç‰¹å¾ï¼ˆå‡ ä¹æ²¡æœ‰å˜åŒ–çš„ç‰¹å¾ï¼‰ï¼Œå‡å°‘å†—ä½™ã€‚\n",
        "                                             # SelectFromModelï¼šæ ¹æ®è®­ç»ƒå¥½çš„æ¨¡å‹çš„é‡è¦æ€§æŒ‡æ ‡é€‰æ‹©ç‰¹å¾ï¼Œä¾‹å¦‚æ ¹æ®å›å½’ç³»æ•°æˆ–æ ‘æ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§ã€‚\n",
        "from sklearn.feature_selection import VarianceThreshold, SelectFromModel\n",
        "\n",
        "# metrics to measure model performance. accuracy_scoreï¼šå‡†ç¡®ç‡, precision_scoreï¼šç²¾ç¡®ç‡, recall_scoreï¼šå¬å›ç‡, f1_scoreï¼šF1 åˆ†æ•°ï¼ˆç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡ï¼‰,å›å½’æŒ‡æ ‡, mean_absolute_errorï¼ˆMAEï¼‰ï¼šå¹³å‡ç»å¯¹è¯¯å·®\n",
        "\n",
        "# mean_squared_errorï¼ˆMSEï¼‰ï¼šå‡æ–¹è¯¯å·®, max_errorï¼šæœ€å¤§è¯¯å·®, mean_absolute_percentage_errorï¼ˆMAPEï¼‰ï¼šå¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®\n",
        "\n",
        "#è¿™ä¸€æ®µå¯¼å…¥çš„éƒ½æ˜¯ æ¨¡å‹ã€æ•°æ®å¤„ç†å·¥å…·å’Œè¯„ä»·æŒ‡æ ‡ï¼Œä¸ºæ„å»ºã€ä¼˜åŒ–å’Œè¯„ä¼°æœºå™¨å­¦ä¹ æ¨¡å‹åšå‡†å¤‡ã€‚\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
        "                             mean_absolute_error, mean_squared_error, max_error, mean_absolute_percentage_error)\n",
        "\n",
        "# save/load models å¸¸ç”¨äº ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹ åˆ°æ–‡ä»¶ï¼Œæˆ–è€…ä»æ–‡ä»¶åŠ è½½æ¨¡å‹ï¼Œè€Œä¸å¿…æ¯æ¬¡éƒ½é‡æ–°è®­ç»ƒã€‚\n",
        "import joblib\n",
        "\n",
        "# For the permutation importance implementation Parallel å’Œ delayed ç”¨äº å¹¶è¡Œè®¡ç®—ï¼ŒåŠ é€Ÿè€—æ—¶æ“ä½œï¼Œä¾‹å¦‚åœ¨å¤šæ ¸ CPU ä¸ŠåŒæ—¶è®¡ç®—æ¯ä¸ªç‰¹å¾çš„é‡è¦æ€§ã€‚\n",
        "from joblib import Parallel\n",
        "from joblib import delayed\n",
        "\n",
        "from sklearn.metrics import check_scoring         #ç”¨äº éªŒè¯å’Œè·å–æ¨¡å‹çš„è¯„åˆ†å‡½æ•°ï¼Œç¡®ä¿è¯„åˆ†æŒ‡æ ‡ä¸æ¨¡å‹å…¼å®¹ï¼Œä¾‹å¦‚ MAEã€RÂ² ç­‰ã€‚\n",
        "from sklearn.utils import Bunch                   #Bunch æ˜¯ä¸€ç§ç±»ä¼¼å­—å…¸çš„æ•°æ®ç»“æ„ï¼Œä½†å¯ä»¥é€šè¿‡ ç‚¹ï¼ˆ.ï¼‰è®¿é—®å±æ€§ï¼Œæ–¹ä¾¿å­˜å‚¨æ•°æ®æˆ–é…ç½®ã€‚\n",
        "from sklearn.utils import check_random_state      #check_random_stateï¼šç¡®ä¿éšæœºæ•°ç”Ÿæˆå™¨çš„ä¸€è‡´æ€§å’Œå¯å¤ç°æ€§ï¼Œä¾‹å¦‚åœ¨äº¤å‰éªŒè¯æˆ–æ‰“ä¹±æ•°æ®æ—¶ä½¿ç”¨ã€‚\n",
        "from sklearn.utils import check_array             #æ£€æŸ¥è¾“å…¥æ•°æ®æ˜¯å¦ä¸ºåˆæ³•çš„ NumPy æ•°ç»„ï¼Œå¹¶å¯è¿›è¡Œå¿…è¦çš„ç±»å‹è½¬æ¢æˆ–ç¼ºå¤±å€¼å¤„ç†ï¼Œä¿è¯æ¨¡å‹è¾“å…¥å®‰å…¨ã€‚\n",
        "\n",
        "# plotting\n",
        "import seaborn as sns           #seabornï¼šåŸºäº matplotlib çš„é«˜çº§ç»˜å›¾åº“ï¼Œæ–¹ä¾¿ç”»å‡ºæ¼‚äº®çš„ç»Ÿè®¡å›¾ï¼ˆå¦‚çƒ­åŠ›å›¾ã€åˆ†å¸ƒå›¾ã€ç®±çº¿å›¾ï¼‰ã€‚\n",
        "import matplotlib.pyplot as plt     #matplotlib.pyplotï¼šPython æœ€å¸¸ç”¨çš„ç»˜å›¾åº“ï¼Œç”¨äºç»˜åˆ¶å„ç§å›¾è¡¨ï¼Œå¦‚æŠ˜çº¿å›¾ã€æ•£ç‚¹å›¾ã€ç›´æ–¹å›¾ç­‰ã€‚\n",
        "%matplotlib inline       #è®©ç»˜åˆ¶çš„å›¾ ç›´æ¥æ˜¾ç¤ºåœ¨ Notebook å†… è€Œä¸æ˜¯å¼¹å‡ºæ–°çª—å£ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40F9DT9QpjXE"
      },
      "source": [
        "### 0.3 Fix the random seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IGs_0GrqpjXF"
      },
      "outputs": [],
      "source": [
        "# add code here\n",
        "#RANDOM_SEED ç›®çš„æ˜¯æ§åˆ¶éšæœºæ€§ï¼Œä¿è¯åœ¨ä¸åŒæ—¶é—´æˆ–ä¸åŒæœºå™¨ä¸Šè¿è¡Œä»£ç æ—¶ï¼Œç»“æœä¿æŒä¸€è‡´ã€‚\n",
        "##FILLME è¡¨ç¤ºä½ éœ€è¦è‡ªå·±å¡«å…¥ä¸€ä¸ªå…·ä½“çš„æ•°å­—\n",
        "\n",
        "#RANDOM_SEED = #FILLME\n",
        "RANDOM_SEED = 3\n",
        "np.random.seed(RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-0Iz0XipjXF"
      },
      "source": [
        " $\\color{Aqua}{\\textsf{Short question}}$\n",
        "- We declared a global variable to fix the random seed (`RANDOM_SEED`). Why did we do this?  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zo-rlgElpjXF"
      },
      "source": [
        "### 0.4 Import the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dda-tNyppjXF"
      },
      "source": [
        "The dataset used in this tutorial originates from the publication [\"Understanding the diversity of the metal-organic framework ecosystem\"](https://doi.org/10.1038/s41467-020-17755-8), which explores the application of machine learning for predicting gas adsorption properties in MOFs. This dataset contains geometric and chemical descriptors, as well as simulated gas uptake values, enabling the development and evaluation of regression models for materials discovery.\n",
        "\n",
        "æœ¬æ•™ç¨‹ä½¿ç”¨çš„æ•°æ®é›†æ¥æºäºè®ºæ–‡ã€ŠUnderstanding the diversity of the metal-organic framework ecosystemã€‹ï¼Œè¯¥è®ºæ–‡æ¢è®¨äº†æœºå™¨å­¦ä¹ åœ¨é¢„æµ‹ MOFs æ°”ä½“å¸é™„æ€§èƒ½ä¸­çš„åº”ç”¨ã€‚è¯¥æ•°æ®é›†åŒ…å«å‡ ä½•å’ŒåŒ–å­¦æè¿°ç¬¦ï¼Œä»¥åŠæ¨¡æ‹Ÿçš„æ°”ä½“å¸é™„é‡ï¼Œä½¿å¾—å¼€å‘å’Œè¯„ä¼°ç”¨äºææ–™å‘ç°çš„å›å½’æ¨¡å‹æˆä¸ºå¯èƒ½ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nT3hj_CjpjXF",
        "outputId": "3b96cf9a-612d-4fd0-dc3d-2f8e236e8822"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸŒ Loaded data from GitHub, shape = (5014, 331)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "# load data locally æ£€æµ‹æ˜¯å¦åœ¨ Colab è¿è¡Œ\n",
        "#get_ipython() è¿”å›å½“å‰çš„ IPython ç¯å¢ƒä¿¡æ¯.å¦‚æœå­—ç¬¦ä¸²ä¸­åŒ…å« 'google.colab'ï¼Œè¯´æ˜ä»£ç åœ¨ Google Colab ä¸­è¿è¡Œã€‚è¿™ä¸ªå˜é‡ç”¨äºåç»­é€‰æ‹©ä¸åŒçš„æ•°æ®è·¯å¾„ã€‚\n",
        "\n",
        "running_in_colab = 'google.colab' in str(get_ipython())\n",
        "\n",
        "\n",
        "#è®¾ç½®æ•°æ®è·¯å¾„\n",
        "#å¦‚æœåœ¨ Colabï¼šæ•°æ®å­˜æ”¾åœ¨ /content/che1147_files æ–‡ä»¶å¤¹ï¼Œå®Œæ•´æ–‡ä»¶è·¯å¾„ï¼š/content/che1147_files/MOF_CoRE2019.csv\n",
        "\n",
        "#å¦‚æœåœ¨ æœ¬åœ°ï¼šæ•°æ®å­˜æ”¾åœ¨ ../data æ–‡ä»¶å¤¹ï¼ˆç›¸å¯¹è·¯å¾„ï¼‰ã€‚å®Œæ•´æ–‡ä»¶è·¯å¾„ï¼š../data/MOF_CoRE2019.csv\n",
        "\n",
        "# os.path.join() ç”¨äº æ‹¼æ¥è·¯å¾„ï¼Œä¿è¯åœ¨ä¸åŒæ“ä½œç³»ç»Ÿä¸‹éƒ½èƒ½æ­£ç¡®å¤„ç†æ–œæ ã€‚\n",
        "\n",
        "if running_in_colab:\n",
        "    DATA_DIR = \"/content/che1147_files\"\n",
        "    DATA_FILE = os.path.join(DATA_DIR, \"MOF_CoRE2019.csv\")\n",
        "else:\n",
        "    DATA_DIR = \"../data\"\n",
        "    DATA_FILE = os.path.join(DATA_DIR, \"MOF_CoRE2019.csv\")\n",
        "\n",
        "\n",
        "\n",
        "#å°è¯•ä»æœ¬åœ°è¯»å–æ•°æ®ï¼šä½¿ç”¨ Pandas çš„ read_csv() è¯»å– CSV æ–‡ä»¶ï¼Œå­˜å…¥ dfï¼ˆDataFrameï¼‰ã€‚\n",
        "\n",
        "#æˆåŠŸè¯»å–åæ‰“å° âœ…ï¼Œå¹¶æ˜¾ç¤ºæ•°æ®å½¢çŠ¶ df.shapeï¼ˆè¡Œæ•° Ã— åˆ—æ•°ï¼‰ã€‚\n",
        "#æ•è· FileNotFoundErrorï¼ˆæœ¬åœ° CSV ä¸å­˜åœ¨ï¼‰ã€‚\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(DATA_FILE)\n",
        "    print(f\"âœ… Loaded data from {DATA_FILE}, shape = {df.shape}\")\n",
        "\n",
        "\n",
        "#å¦‚æœæœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨ã€‚å°è¯•ä» GitHub è¿œç¨‹ URL è¯»å–æ•°æ®ï¼Œä¿è¯å³ä½¿æœ¬åœ°æ²¡æœ‰æ–‡ä»¶ä¹Ÿèƒ½ç»§ç»­æ‰§è¡Œã€‚\n",
        "\n",
        "#æˆåŠŸè¯»å–åæ‰“å° ğŸŒ å¹¶æ˜¾ç¤ºæ•°æ®å½¢çŠ¶\n",
        "\n",
        "except FileNotFoundError:\n",
        "    # Fallback: try to load from GitHub raw URL\n",
        "    url = \"https://github.com/AI4ChemS/CHE1147/raw/refs/heads/main/data/MOF_CoRE2019.csv\"\n",
        "    try:\n",
        "        df = pd.read_csv(url)\n",
        "        print(f\"ğŸŒ Loaded data from GitHub, shape = {df.shape}\")\n",
        "\n",
        " # å¦‚æœè¿œç¨‹ä¹Ÿå¤±è´¥ï¼Œæ‰“å° âŒ æç¤ºæ‰¾ä¸åˆ°æ–‡ä»¶ï¼Œç„¶åé‡æ–°æŠ›å‡ºå¼‚å¸¸ä»¥ç»ˆæ­¢ç¨‹åºã€‚\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Could not find {DATA_FILE} locally or on GitHub.\")\n",
        "        raise e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "id": "WRjXCNmLpjXG",
        "outputId": "c0fd9bb1-b5ad-4208-df54-87bba67f942e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          MOFname  ASA [m^2/cm^3]       Df       Di      Dif  NASA [m^2/cm^3]  \\\n",
              "0    XAGCUE_clean            0.00  2.87682  6.41175  6.41175          487.410   \n",
              "1    SOBZEQ_clean         2298.52  5.44324  7.06044  7.04565            0.000   \n",
              "2  AVAQIX01_clean            0.00  3.61603  5.36267  5.34980          600.353   \n",
              "3    INURIS_clean            0.00  3.09799  5.07769  4.57779          253.440   \n",
              "4    KEDNOY_clean            0.00  3.63243  4.98967  4.98020          519.714   \n",
              "\n",
              "   POAV [cm^3/g]   POAVF  PONAV [cm^3/g]   PONAVF  ...  sum-f-lig-T-2  \\\n",
              "0       0.000000  0.0000        0.101882  0.21558  ...        10752.0   \n",
              "1       0.738273  0.6363        0.000000  0.00000  ...         2240.0   \n",
              "2       0.000000  0.0000        0.226377  0.30062  ...         3040.0   \n",
              "3       0.000000  0.0000        0.077296  0.09856  ...        12720.0   \n",
              "4       0.000000  0.0000        0.238159  0.28890  ...         2064.0   \n",
              "\n",
              "   sum-f-lig-T-3  sum-f-lig-S-0  sum-f-lig-S-1  sum-f-lig-S-2  sum-f-lig-S-3  \\\n",
              "0        10752.0       242.0856       523.7232       844.9008      1049.6448   \n",
              "1         2544.0        58.2840       130.2840       194.2144       231.0000   \n",
              "2         2688.0        91.7328       204.2656       276.2848       265.7760   \n",
              "3        13728.0       301.4544       713.6976      1037.0160      1127.9280   \n",
              "4         1920.0        72.0852       153.0144       199.7088       218.5200   \n",
              "\n",
              "   CO2 uptake at 0.15 bar and 298K  CO2 uptake at 16 bar and 298K  \\\n",
              "0                         1.266370                       3.120211   \n",
              "1                         8.224130                      17.486748   \n",
              "2                         3.694178                       5.849020   \n",
              "3                         1.007227                       4.092395   \n",
              "4                         3.617103                       6.170669   \n",
              "\n",
              "   CH4 uptake at 5.8 bar and 298K  CH4 uptake at 65 bar and 298K  \n",
              "0                        1.810078                       2.173372  \n",
              "1                        4.560643                      11.578465  \n",
              "2                        3.859973                       5.251466  \n",
              "3                        2.032925                       3.728986  \n",
              "4                        3.924974                       4.888655  \n",
              "\n",
              "[5 rows x 331 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f3baeb48-cd74-4552-b6b1-b8d942ee2219\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MOFname</th>\n",
              "      <th>ASA [m^2/cm^3]</th>\n",
              "      <th>Df</th>\n",
              "      <th>Di</th>\n",
              "      <th>Dif</th>\n",
              "      <th>NASA [m^2/cm^3]</th>\n",
              "      <th>POAV [cm^3/g]</th>\n",
              "      <th>POAVF</th>\n",
              "      <th>PONAV [cm^3/g]</th>\n",
              "      <th>PONAVF</th>\n",
              "      <th>...</th>\n",
              "      <th>sum-f-lig-T-2</th>\n",
              "      <th>sum-f-lig-T-3</th>\n",
              "      <th>sum-f-lig-S-0</th>\n",
              "      <th>sum-f-lig-S-1</th>\n",
              "      <th>sum-f-lig-S-2</th>\n",
              "      <th>sum-f-lig-S-3</th>\n",
              "      <th>CO2 uptake at 0.15 bar and 298K</th>\n",
              "      <th>CO2 uptake at 16 bar and 298K</th>\n",
              "      <th>CH4 uptake at 5.8 bar and 298K</th>\n",
              "      <th>CH4 uptake at 65 bar and 298K</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>XAGCUE_clean</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.87682</td>\n",
              "      <td>6.41175</td>\n",
              "      <td>6.41175</td>\n",
              "      <td>487.410</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.101882</td>\n",
              "      <td>0.21558</td>\n",
              "      <td>...</td>\n",
              "      <td>10752.0</td>\n",
              "      <td>10752.0</td>\n",
              "      <td>242.0856</td>\n",
              "      <td>523.7232</td>\n",
              "      <td>844.9008</td>\n",
              "      <td>1049.6448</td>\n",
              "      <td>1.266370</td>\n",
              "      <td>3.120211</td>\n",
              "      <td>1.810078</td>\n",
              "      <td>2.173372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SOBZEQ_clean</td>\n",
              "      <td>2298.52</td>\n",
              "      <td>5.44324</td>\n",
              "      <td>7.06044</td>\n",
              "      <td>7.04565</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.738273</td>\n",
              "      <td>0.6363</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>...</td>\n",
              "      <td>2240.0</td>\n",
              "      <td>2544.0</td>\n",
              "      <td>58.2840</td>\n",
              "      <td>130.2840</td>\n",
              "      <td>194.2144</td>\n",
              "      <td>231.0000</td>\n",
              "      <td>8.224130</td>\n",
              "      <td>17.486748</td>\n",
              "      <td>4.560643</td>\n",
              "      <td>11.578465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AVAQIX01_clean</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.61603</td>\n",
              "      <td>5.36267</td>\n",
              "      <td>5.34980</td>\n",
              "      <td>600.353</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.226377</td>\n",
              "      <td>0.30062</td>\n",
              "      <td>...</td>\n",
              "      <td>3040.0</td>\n",
              "      <td>2688.0</td>\n",
              "      <td>91.7328</td>\n",
              "      <td>204.2656</td>\n",
              "      <td>276.2848</td>\n",
              "      <td>265.7760</td>\n",
              "      <td>3.694178</td>\n",
              "      <td>5.849020</td>\n",
              "      <td>3.859973</td>\n",
              "      <td>5.251466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>INURIS_clean</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.09799</td>\n",
              "      <td>5.07769</td>\n",
              "      <td>4.57779</td>\n",
              "      <td>253.440</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.077296</td>\n",
              "      <td>0.09856</td>\n",
              "      <td>...</td>\n",
              "      <td>12720.0</td>\n",
              "      <td>13728.0</td>\n",
              "      <td>301.4544</td>\n",
              "      <td>713.6976</td>\n",
              "      <td>1037.0160</td>\n",
              "      <td>1127.9280</td>\n",
              "      <td>1.007227</td>\n",
              "      <td>4.092395</td>\n",
              "      <td>2.032925</td>\n",
              "      <td>3.728986</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>KEDNOY_clean</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.63243</td>\n",
              "      <td>4.98967</td>\n",
              "      <td>4.98020</td>\n",
              "      <td>519.714</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.238159</td>\n",
              "      <td>0.28890</td>\n",
              "      <td>...</td>\n",
              "      <td>2064.0</td>\n",
              "      <td>1920.0</td>\n",
              "      <td>72.0852</td>\n",
              "      <td>153.0144</td>\n",
              "      <td>199.7088</td>\n",
              "      <td>218.5200</td>\n",
              "      <td>3.617103</td>\n",
              "      <td>6.170669</td>\n",
              "      <td>3.924974</td>\n",
              "      <td>4.888655</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 331 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f3baeb48-cd74-4552-b6b1-b8d942ee2219')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f3baeb48-cd74-4552-b6b1-b8d942ee2219 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f3baeb48-cd74-4552-b6b1-b8d942ee2219');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-87c06e43-6dc1-4145-bcff-f4f07c7082ee\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-87c06e43-6dc1-4145-bcff-f4f07c7082ee')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-87c06e43-6dc1-4145-bcff-f4f07c7082ee button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "df.head()\n",
        "#æ˜¾ç¤ºæ•°æ®è¡¨å‰ 5 è¡Œã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0eaVNItpjXG"
      },
      "source": [
        "<details>\n",
        "<summary> <font color='green'>Click here for a hint</font></summary>\n",
        "<ul>\n",
        "    <li>Use something like <code>pd.options.display.max_columns=100</code> to adjust how many columns are shown.<code>pd.options.display.max_columns=100</code>  would show at maximum 100 columns. </li>\n",
        "</ul>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pI66g7K9pjXG"
      },
      "source": [
        "Let's also get some basic information ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndXjE2UxpjXG",
        "outputId": "ce2495f4-faf2-4c0c-91c1-53233a349265"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5014 entries, 0 to 5013\n",
            "Columns: 331 entries, MOFname to CH4 uptake at 65 bar and 298K\n",
            "dtypes: float64(330), object(1)\n",
            "memory usage: 12.7+ MB\n"
          ]
        }
      ],
      "source": [
        "df.info()\n",
        "#å¿«é€ŸæŸ¥çœ‹æ•°æ®çš„æ€»ä½“ä¿¡æ¯ï¼Œå°¤å…¶æ˜¯åˆ—åã€æ•°æ®ç±»å‹ã€éç©ºå€¼æ•°é‡å’Œå†…å­˜ä½¿ç”¨æƒ…å†µã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSxZYT4RpjXG"
      },
      "source": [
        " $\\color{Aqua}{\\textsf{Short question}}$\n",
        "- How many materials are in the dataset?\n",
        "- Which datatypes do we deal with?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXunWl-hpjXH"
      },
      "source": [
        "### 0.5 Understanding the data\n",
        "ä½œä¸ºæè¿°ç¬¦ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å­”éš™å‡ ä½•æè¿°ç¬¦ï¼Œä¾‹å¦‚å¯†åº¦ã€å­”ä½“ç§¯ç­‰ï¼Œä»¥åŠä¿®æ­£è‡ªç›¸å…³å‡½æ•°ï¼ˆRACsï¼‰æ¥æè¿° MOFs çš„åŒ–å­¦ç‰¹æ€§ã€‚æˆ‘ä»¬çš„æ•°æ®é›†åŒ…å« MOFs çš„å››ä¸ªæ€§èƒ½å±æ€§ï¼š\n",
        "\n",
        "* COâ‚‚ åœ¨ 0.15 bar å’Œ 298K ä¸‹çš„å¸é™„é‡\n",
        "* COâ‚‚ åœ¨ 16 bar å’Œ 298K ä¸‹çš„å¸é™„é‡\n",
        "* CHâ‚„ åœ¨ 5.8 bar å’Œ 298K ä¸‹çš„å¸é™„é‡\n",
        "* CHâ‚„ åœ¨ 65 bar å’Œ 298K ä¸‹çš„å¸é™„é‡\n",
        "\n",
        "As descriptors we will use pore geometric descriptors, such as density, pore volume, etc. and [revised autocorrelation functions](https://www.nature.com/articles/s41467-020-17755-8) (RACs) for describing chemistry of MOFs. Our dataset has four properties for MOFs:\n",
        "- CO2 uptake at 0.15 bar and 298K\n",
        "- CO2 uptake at 16 bar and 298K\n",
        "- CH4 uptake at 5.8 bar and 298K\n",
        "- CH4 uptake at 65 bar and 298K\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Let's start with a simple one that is the high-pressure CO$_2$ uptake `df[\"CO2 uptake at 16 bar and 298K\"]`. This is the amount of CO$_2$ (mmol) the MOF can load per gram.\n",
        "\n",
        "æˆ‘ä»¬å…ˆä»ä¸€ä¸ªç®€å•çš„å±æ€§å¼€å§‹ï¼Œå³é«˜å‹ä¸‹çš„ COâ‚‚ å¸é™„é‡ï¼š`df[\"CO2 uptake at 16 bar and 298K\"]`ã€‚å®ƒè¡¨ç¤º MOF æ¯å…‹ææ–™èƒ½å¤Ÿå¸é™„çš„ COâ‚‚ æ•°é‡ï¼ˆå•ä½ï¼šmmolï¼‰ã€‚\n",
        "\n",
        "\n",
        "Below, we define three global variables (hence upper case), which are the *names* of our feature and target columns. We will use the `TARGET` for the actual regression and the `TARGET_BINARY` only for the stratified train/test split. The `FEATURES` variable is a list of column names of our dataframe. We imported the names of descriptors from `MOF_descriptors.py`.\n",
        "\n",
        "\n",
        "ä¸‹é¢ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸‰ä¸ªå…¨å±€å˜é‡ï¼ˆå› æ­¤ä½¿ç”¨å¤§å†™å­—æ¯ï¼‰ï¼Œå®ƒä»¬åˆ†åˆ«æ˜¯ç‰¹å¾åˆ—å’Œç›®æ ‡åˆ—çš„åç§°ã€‚\n",
        "\n",
        "* `TARGET` ç”¨äºå®é™…çš„å›å½’æ¨¡å‹\n",
        "* `TARGET_BINARY` ä»…ç”¨äºåˆ†å±‚è®­ç»ƒ/æµ‹è¯•é›†åˆ’åˆ†\n",
        "* `FEATURES` æ˜¯æ•°æ®æ¡†ä¸­æè¿°ç¬¦åˆ—åçš„åˆ—è¡¨\n",
        "\n",
        "æè¿°ç¬¦çš„åˆ—åæ˜¯ä» `MOF_descriptors.py` æ–‡ä»¶ä¸­å¯¼å…¥çš„ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UqdfJgBpjXH"
      },
      "outputs": [],
      "source": [
        "# name of descriptors\n",
        "#ä» MOF_descriptors.py æ–‡ä»¶ä¸­å¯¼å…¥è‹¥å¹²ä¸ª æè¿°ç¬¦åˆ—è¡¨ã€‚è¿™äº›å˜é‡ä¸­ï¼Œæ¯ä¸€ä¸ªéƒ½ä»£è¡¨ä¸€ç»„ç‰¹å¾ï¼ˆåˆ—åï¼‰ï¼Œç”¨äºè¡¨ç¤º MOF çš„ä¸åŒç»“æ„æˆ–åŒ–å­¦ç‰¹å¾ã€‚\n",
        "from MOF_descriptors import geometric_descriptors, linker_descriptors, metalcenter_descriptors, functionalgroup_descriptors, summed_linker_descriptors, summed_metalcenter_descriptors, summed_functionalgroup_descriptors\n",
        "\n",
        "#å®šä¹‰ç›®æ ‡å˜é‡ï¼ˆä¹Ÿå°±æ˜¯æˆ‘ä»¬è¦é¢„æµ‹çš„å€¼ï¼‰ã€‚åœ¨è¿™é‡Œï¼Œç›®æ ‡æ˜¯ï¼šMOF åœ¨ 16 bar å‹åŠ›ã€298K æ¸©åº¦ä¸‹çš„ COâ‚‚ å¸é™„é‡ã€‚\n",
        "TARGET = \"CO2 uptake at 16 bar and 298K\"\n",
        "\n",
        "\n",
        "\n",
        "#å°†å¤šç»„æè¿°ç¬¦åˆ—è¡¨æ‹¼æ¥ï¼ˆåˆå¹¶ï¼‰åœ¨ä¸€èµ·ï¼Œç”Ÿæˆä¸€ä¸ªå¤§çš„ç‰¹å¾åˆ—è¡¨ã€‚\n",
        "#è¿™äº›ç‰¹å¾åˆ—å°†ä½œä¸ºè¾“å…¥ï¼ˆXï¼‰ï¼ŒTARGET å°†ä½œä¸ºè¾“å‡ºï¼ˆyï¼‰ï¼Œä¾›æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒä½¿ç”¨ã€‚\n",
        "FEATURES = (\n",
        "    geometric_descriptors\n",
        "    + summed_functionalgroup_descriptors\n",
        "    + summed_linker_descriptors\n",
        "    + summed_metalcenter_descriptors\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHCc9AX3pjXH"
      },
      "source": [
        "Examples for pore geometry descriptors (in `geometric_descriptors`) include: $D_i$ (the size of the largest included sphere), $D_f$ (the largest free sphere), and $D_{if}$ (the largest included free sphere) along the pore $-$ three ways of characterizing pore size.\n",
        "\n",
        "å­”éš™å‡ ä½•æè¿°ç¬¦ï¼ˆ`geometric_descriptors`ï¼‰çš„ç¤ºä¾‹åŒ…æ‹¬ï¼š\n",
        "\n",
        "* **ğ·áµ¢**ï¼ˆæœ€å¤§å†…å«çƒç›´å¾„ï¼Œthe size of the largest included sphereï¼‰ï¼Œ\n",
        "* **ğ·ğ‘“**ï¼ˆæœ€å¤§è‡ªç”±çƒç›´å¾„ï¼Œthe largest free sphereï¼‰ï¼Œ\n",
        "* **ğ·áµ¢ğ‘“**ï¼ˆæœ€å¤§å†…å«è‡ªç”±çƒç›´å¾„ï¼Œthe largest included free sphereï¼‰ã€‚\n",
        "\n",
        "è¿™ä¸‰ç§å‚æ•°ç”¨äºä»ä¸åŒè§’åº¦è¡¨å¾å­”å¾„å¤§å°ã€‚\n",
        "\n",
        "\n",
        "![pore diameters](https://github.com/AI4ChemS/CHE-1147/blob/main/assets/spheres.png?raw=1)\n",
        "\n",
        "Also included are the surface area (SA) of the pore, and the probe-occupiable pore volume (POV).\n",
        "More details on the description of pore geometries can be found in [Ongari et al.](https://pubs.acs.org/doi/abs/10.1021/acs.langmuir.7b01682)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOup7KrSpjXH"
      },
      "source": [
        "RACs (in the lists starting with `summed_...`) operate on the structure graph and encode information about the metal center, linkers and the functional groups as differences or products of heuristics that are relevant for inorganic chemistry, such as electronegativity ($\\chi$), connectivity ($T$), identity ($I$), covalent radii ($S$), and nuclear charge ($Z$).\n",
        "\n",
        "RACsï¼ˆåœ¨ä»¥ `summed_` å¼€å¤´çš„åˆ—è¡¨ä¸­ï¼‰åŸºäºç»“æ„å›¾è¿›è¡Œæ“ä½œï¼Œé€šè¿‡è®¡ç®—ä¸æ— æœºåŒ–å­¦ç›¸å…³å¯å‘å¼å‚æ•°çš„**å·®å€¼æˆ–ä¹˜ç§¯**ï¼Œæ¥ç¼–ç é‡‘å±ä¸­å¿ƒã€è¿æ¥åŸºå’Œå®˜èƒ½å›¢çš„ä¿¡æ¯ã€‚\n",
        "è¿™äº›å¯å‘å¼å‚æ•°åŒ…æ‹¬ï¼š\n",
        "\n",
        "* ç”µè´Ÿæ€§ï¼ˆğœ’ï¼‰ã€\n",
        "* è¿æ¥æ€§ï¼ˆğ‘‡ï¼‰ã€\n",
        "* å…ƒç´ æ ‡è¯†ï¼ˆğ¼ï¼‰ã€\n",
        "* å…±ä»·åŠå¾„ï¼ˆğ‘†ï¼‰ã€\n",
        "* æ ¸ç”µè·ï¼ˆğ‘ï¼‰ã€‚\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8O7VQEupjXH"
      },
      "source": [
        "\n",
        "<img src=\"https://github.com/AI4ChemS/CHE-1147/blob/main/assets/racs.png?raw=1\" alt=\"RACs scheme from the lecture\" width=\"700\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhp75r44pjXH"
      },
      "source": [
        "The number in the descriptornames shows the coordination shell that was considered in the calculation of the RACs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TujApr4vpjXI"
      },
      "source": [
        "### 0.6 Basic Data Cleaning and Preparation\n",
        "\n",
        "We perform some data cleaning by removing instances with missing value and removing duplicates.\n",
        "\n",
        "æˆ‘ä»¬é€šè¿‡åˆ é™¤åŒ…å«ç¼ºå¤±å€¼çš„æ ·æœ¬å’Œé‡å¤é¡¹æ¥è¿›è¡Œæ•°æ®æ¸…æ´—ã€‚\n",
        "\n",
        " $\\color{Aqua}{\\textsf{Short question}}$\n",
        "- We we identify a missing value, should we remove that cell, the column or the row with missing value?\n",
        "\n",
        "å½“æˆ‘ä»¬å‘ç°æ•°æ®ä¸­æœ‰ç¼ºå¤±å€¼æ—¶ï¼Œåº”è¯¥åˆ é™¤é‚£ä¸ªå•å…ƒæ ¼ã€æ•´åˆ—ï¼Œè¿˜æ˜¯æ•´è¡Œå‘¢ï¼Ÿ\n",
        "\n",
        "åœ¨æ•°æ®æ¸…æ´—æ—¶ï¼Œé€šå¸¸ä¸ä¼šåªåˆ é™¤å•ä¸ªâ€œå•å…ƒæ ¼â€ï¼ˆå› ä¸ºé‚£æ ·æ•°æ®ä»ç„¶ä¸å®Œæ•´ï¼‰ã€‚\n",
        "ä¸€èˆ¬æœ‰ä¸¤ç§å¸¸è§åšæ³•ï¼š\n",
        "\n",
        "åˆ é™¤æ•´è¡Œï¼ˆrowï¼‰ â€” å¦‚æœè¯¥è¡Œåªç¼ºå°‘å°‘é‡ä¸é‡è¦çš„å€¼ã€‚\n",
        "ğŸ‘‰ é€‚ç”¨äºç¼ºå¤±å€¼å¾ˆå°‘çš„æƒ…å†µã€‚\n",
        "\n",
        "åˆ é™¤æ•´åˆ—ï¼ˆcolumnï¼‰ â€” å¦‚æœè¿™ä¸€åˆ—ç¼ºå¤±å¤ªå¤šæˆ–ä¸é‡è¦ã€‚\n",
        "ğŸ‘‰ é€‚ç”¨äºæŸåˆ—å¤§éƒ¨åˆ†å€¼éƒ½ç¼ºå¤±æ—¶ã€‚\n",
        "\n",
        "æˆ–è€…ï¼Œè¿˜å¯ä»¥ä½¿ç”¨å…¶ä»–æ–¹æ³•ï¼š\n",
        "3. å¡«è¡¥ç¼ºå¤±å€¼ï¼ˆimputationï¼‰ â€” ç”¨å¹³å‡å€¼ã€ä¸­ä½æ•°æˆ–é¢„æµ‹å€¼ä»£æ›¿ç¼ºå¤±å€¼ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCu0G0zdpjXI"
      },
      "outputs": [],
      "source": [
        "#è¿™ä¸¤è¡Œä»£ç æ˜¯ç”¨æ¥ç»Ÿè®¡ DataFrame ä¸­æœ‰ç¼ºå¤±å€¼ï¼ˆNaNï¼‰çš„è¡Œæ•°çš„ã€‚\n",
        "#df.isna()  ä½œç”¨ï¼šæ£€æŸ¥æ•´ä¸ª DataFrame ä¸­å“ªäº›ä½ç½®æ˜¯ç¼ºå¤±å€¼ï¼ˆNaNï¼‰ã€‚\n",
        "#.any(axis=1) ä½œç”¨ï¼šæŒ‰è¡Œï¼ˆaxis=1ï¼‰æ£€æŸ¥æ¯ä¸€è¡Œæ˜¯å¦â€œè‡³å°‘æœ‰ä¸€ä¸ªâ€ç¼ºå¤±å€¼ã€‚\n",
        "#.sum()   å› ä¸º True ä¼šè¢«å½“ä½œ 1ï¼ŒFalse å½“ä½œ 0ï¼Œæ‰€ä»¥å¯¹å¸ƒå°”å€¼æ±‚å’Œå°±èƒ½å¾—åˆ°â€œTrue çš„æ•°é‡â€ã€‚\n",
        "          #âœ… True è¡¨ç¤ºè¿™ä¸ªä½ç½®æ˜¯ç¼ºå¤±å€¼ï¼ˆNaNï¼‰#âŒ False è¡¨ç¤ºè¿™ä¸ªä½ç½®ä¸æ˜¯ç¼ºå¤±å€¼ï¼ˆæœ‰æ•°æ®ï¼‰\n",
        "\n",
        "num_rows_with_nan = df.isna().any(axis=1).sum()\n",
        "print(f\"Number of rows with NaN values: {num_rows_with_nan}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3jHWfIwpjXI"
      },
      "source": [
        "Write a code to remove the NaN:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5SoWG6opjXI"
      },
      "outputs": [],
      "source": [
        "# FILLME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpecWG7dpjXI"
      },
      "source": [
        "Next, we should remove duplicate rows, ensuring dataset contains only unique samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RDuGqVVpjXI"
      },
      "outputs": [],
      "source": [
        "## FILLME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4P0-FvJTpjXJ"
      },
      "source": [
        " $\\color{Aqua}{\\textsf{Short question}}$\n",
        "- How many duplicated entries did we remove?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZ6FD6WmpjXJ"
      },
      "source": [
        "## 1. Split the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gChl4wlupjXJ"
      },
      "source": [
        "As the first step, we split our data into a training set and a test set. In order to prevent *any* information of the test set from leaking into our model, we split *before* starting to analyze or transform our data.\n",
        "> **Note:** Not doing the split at this stage can cause data leakage.\n",
        "\n",
        "ç¬¬ä¸€æ­¥ï¼Œæˆ‘ä»¬å°†æ•°æ®åˆ’åˆ†ä¸ºè®­ç»ƒé›†ï¼ˆtraining setï¼‰å’Œæµ‹è¯•é›†ï¼ˆtest setï¼‰ã€‚\n",
        "ä¸ºäº†é˜²æ­¢æµ‹è¯•é›†ä¸­çš„ä¿¡æ¯æ³„éœ²åˆ°æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬å¿…é¡»åœ¨å¼€å§‹åˆ†ææˆ–è½¬æ¢æ•°æ®ä¹‹å‰å°±è¿›è¡Œåˆ’åˆ†ã€‚\n",
        "\n",
        "âš ï¸ æ³¨æ„ï¼š\n",
        "å¦‚æœä¸åœ¨è¿™ä¸ªé˜¶æ®µè¿›è¡Œåˆ’åˆ†ï¼Œå¯èƒ½ä¼šå¯¼è‡´æ•°æ®æ³„æ¼ï¼ˆdata leakageï¼‰ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jYdBlZApjXJ"
      },
      "source": [
        "## 1.1 Random splitting\n",
        "\n",
        "A common way of splitting data is random splitting. We can use `sklearn's train_test_split` for doing this.\n",
        "\n",
        "\n",
        "\n",
        "ä¸€ç§å¸¸è§çš„æ•°æ®åˆ’åˆ†æ–¹å¼æ˜¯**éšæœºåˆ’åˆ†**ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ **sklearn** åº“ä¸­çš„ `train_test_split` å‡½æ•°æ¥å®ç°ã€‚\n",
        "\n",
        "Depending on the size of the dataset, we might have different splitting ratio. Common split rations are:\n",
        "- **80/20 Split**: 80% for training, 20% for testing.\n",
        "- **Train/Validation/Test Split**: For larger datasets, a common split is 70% training, 15% validation, and 15% testing.\n",
        "\n",
        "æ ¹æ®æ•°æ®é›†çš„å¤§å°ï¼Œå¯ä»¥é‡‡ç”¨ä¸åŒçš„åˆ’åˆ†æ¯”ä¾‹ã€‚å¸¸è§çš„åˆ’åˆ†æ¯”ä¾‹åŒ…æ‹¬ï¼š\n",
        "\n",
        "* **80/20 åˆ’åˆ†**ï¼š80% çš„æ•°æ®ç”¨äºè®­ç»ƒï¼Œ20% çš„æ•°æ®ç”¨äºæµ‹è¯•ã€‚\n",
        "* **è®­ç»ƒ/éªŒè¯/æµ‹è¯•åˆ’åˆ†**ï¼šå¯¹äºè¾ƒå¤§çš„æ•°æ®é›†ï¼Œå¸¸è§çš„æ¯”ä¾‹æ˜¯ 70% ç”¨äºè®­ç»ƒï¼Œ15% ç”¨äºéªŒè¯ï¼Œ15% ç”¨äºæµ‹è¯•ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "gJtKz5-OpjXO",
        "outputId": "dfcf0027-c39c-4f9a-b12f-e9159c745a1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸŒ Loaded data from GitHub, shape = (5014, 331)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'RANDOM_SEED' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2086610501.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m                                  \u001b[0;31m#è¿™æ˜¯è¦åˆ’åˆ†çš„æ•°æ®é›†ã€‚åœ¨è¿™é‡Œï¼Œdf æ˜¯ä½ çš„å®Œæ•´æ•°æ®æ¡†ï¼ˆDataFrameï¼‰ã€‚\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# 80% çš„æ•°æ®å°†è¢«åˆ†é…ç»™è®­ç»ƒé›†ï¼Œ20% çš„æ•°æ®åˆ†é…ç»™æµ‹è¯•é›†ã€‚\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRANDOM_SEED\u001b[0m  \u001b[0;31m# Ensure reproducibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m )\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'RANDOM_SEED' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "#è°ƒç”¨ train_test_split() å‡½æ•°ï¼ŒæŠŠåŸå§‹æ•°æ®é›† df éšæœºåˆ’åˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼š\n",
        "#df_train â€”â€” è®­ç»ƒé›†ï¼Œdf_test â€”â€” æµ‹è¯•é›†\n",
        "df_train, df_test = train_test_split(\n",
        "    df,                                  #è¿™æ˜¯è¦åˆ’åˆ†çš„æ•°æ®é›†ã€‚åœ¨è¿™é‡Œï¼Œdf æ˜¯ä½ çš„å®Œæ•´æ•°æ®æ¡†ï¼ˆDataFrameï¼‰ã€‚\n",
        "    test_size=0.2,  # 80% çš„æ•°æ®å°†è¢«åˆ†é…ç»™è®­ç»ƒé›†ï¼Œ20% çš„æ•°æ®åˆ†é…ç»™æµ‹è¯•é›†ã€‚\n",
        "    random_state=RANDOM_SEED  # Ensure reproducibility\n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0CNHv53pjXO"
      },
      "source": [
        "### 1.1. Split with stratification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiF3trGWpjXO"
      },
      "source": [
        "Random splitting may lead to imbalanced class distributions between training and test sets, which can result in biased model evaluation and poor generalization.\n",
        "[Stratification](https://en.wikipedia.org/wiki/Stratified_sampling) ensures that the class distributions (ratio of \"good\" to \"bad\" materials) are the same in the training and test set.\n",
        "\n",
        " $\\color{Aqua}{\\textsf{Short question}}$\n",
        "\n",
        "- Why is this important? What could happen if we would not do this?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmTqEo0lpjXO"
      },
      "source": [
        "For stratification to work, we to define what makes a \"good\" or a \"bad\" material. This requires knowing the target property of interest. Let's start with developing a model for a simple target, which is CO2 uptake at room temperature and high pressure (16bar)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdJeiTmPpjXO"
      },
      "source": [
        " $\\color{Aqua}{\\textsf{Short question}}$\n",
        "- How can we choose a good value to for classifying \"good\" and \"bad\" materials?  \n",
        "\n",
        "<details>\n",
        "<summary> <font color='green'>Click here for a hint</font></summary>\n",
        "<ul>\n",
        "    You can choose it based on the histogram of the property of interest.\n",
        "</ul>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sn1yBdcpjXP"
      },
      "outputs": [],
      "source": [
        "# add code here\n",
        "# Plot histogram of the target\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.histplot(df[TARGET], kde=True, bins=30, color='blue')\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel(\"{TARGET}\", fontsize=12)\n",
        "plt.ylabel(\"Frequency\", fontsize=12)\n",
        "plt.title(\"Histogram of {TARGET}\", fontsize=14)\n",
        "\n",
        "# Show the plot\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWjc3dIQpjXP"
      },
      "source": [
        "\n",
        "Based on this histogram and the tail of the distribution, we will use 15 mmol CO$_2$ / g as the threshold for the uptake, thus binarizing our continuous target variable. We use this threshold to define a new column with binary values of 0 and 1 for bad and good materials."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_50CydQXpjXP"
      },
      "source": [
        " $\\color{Aqua}{\\textsf{Short question}}$\n",
        " - add a column 'target_binary' that encodes whether a material is low performing (`0`) or high perfoming (`1`) by comparing the uptake with the `THRESHOLD`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufeuIT_JpjXP"
      },
      "source": [
        "<details>\n",
        "<summary> <font color='green'>Click here for a hint</font></summary>\n",
        "<ul>\n",
        "    <li> you can use <a href='https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.cut.html'>pd.cut</a>,\n",
        "    <a href='https://stackoverflow.com/questions/4406389/if-else-in-a-list-comprehension'>list comprehension</a>, the <a href='https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Binarizer.html#sklearn.preprocessing.Binarizer'> binarizer in sklearn </a>...) </li>\n",
        "    <li> a list comprehension example: <code> [1 if value > THRESHOLD else 0 for value in df[TARGET]] </code> </li>\n",
        "</ul>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsuPrkKdpjXP"
      },
      "outputs": [],
      "source": [
        "TARGET_BINARY = \"target_binned\" # name of the new binary target column\n",
        "THRESHOLD = df[TARGET].quantile(0.9)  # Top 10% of the dataset\n",
        "df[TARGET_BINARY] = (df[TARGET] > THRESHOLD).astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvoFpMdgpjXP"
      },
      "source": [
        "Now, we can perform the actual split into training and test set using stratified sampling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c612Q8JcpjXQ"
      },
      "source": [
        " $\\color{Aqua}{\\textsf{Short question}}$\n",
        "- select reasonable values for `XX` and `XY` and then perform the test/train splits. What do you consider when making this decision (think about what you would do with really small and really big datasets, what happens if you have only one test point, what happens to the model performance if you have more test points than training points)?\n",
        "- why do we need to perform the split into a training and test set?\n",
        "- would we use the test set to tune the hyperparameters of our model?\n",
        "\n",
        "<details>\n",
        "<summary> <font color='green'>Click here for a hint</font></summary>\n",
        "<ul>\n",
        "    <li>The `size` arguments can either be integers or, often more convenient, decimals like 0.1</li>\n",
        "    <li>When you perform the split into training and test set you need to trade-off bias (pessimistic bias due to little training data) and variance (due to little test data) </li>\n",
        "    <li>A typical split cloud be 70/30, but for huge dataset the test set might be too big and for small datasets the training set might be too small in this way </li>\n",
        "</ul>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28TFd-L8pjXQ"
      },
      "outputs": [],
      "source": [
        "# add code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ovZHGiGpjXQ"
      },
      "outputs": [],
      "source": [
        "df_train_stratified, df_test_stratified = train_test_split(\n",
        "    df,\n",
        "    train_size=XX,\n",
        "    test_size=YY,\n",
        "    random_state=RANDOM_SEED,\n",
        "    stratify= # FILLME\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OU59qsJ0pjXQ"
      },
      "source": [
        "Check if splitting is reasonable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fe16gek7pjXQ"
      },
      "outputs": [],
      "source": [
        "df_train_stratified.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXaNDqFzpjXQ"
      },
      "outputs": [],
      "source": [
        "df_test_stratified.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkbRXBYypjXR"
      },
      "source": [
        "## 2. Exploratory data analysis (EDA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6x2PiYXKpjXR"
      },
      "source": [
        "After we have put the test set aside, we can give the training set a closer look."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vj6yxttWpjXR"
      },
      "source": [
        "## 2.1 ydata profiling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9L5dMA4pjXR"
      },
      "source": [
        "> this part takes long so let's do it at home!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6B6fx-rpjXR"
      },
      "outputs": [],
      "source": [
        "# profile = ProfileReport(df_train_stratified, title=\"EDA Report\", explorative=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcJ16uXFpjXR"
      },
      "outputs": [],
      "source": [
        "# profile.to_notebook_iframe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhABMZfGpjXR"
      },
      "outputs": [],
      "source": [
        "# Save to an html file\n",
        "# profile.to_file(\"MOF_eda_report.html\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiqQG4FYpjXS"
      },
      "source": [
        "### 2.2 Removing redundant columns\n",
        "\n",
        "We remove any feature that does not have variance across the dataset to reduce dimensionality.\n",
        "Write a code below that makes a list of features (`redundant_features`) with zero variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "potkKq2KpjXS"
      },
      "outputs": [],
      "source": [
        "# FILLME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBo6SnhapjXS"
      },
      "outputs": [],
      "source": [
        "# update feature set\n",
        "print(f\"Number of features before: {len(FEATURES)}\")\n",
        "FEATURES = [feature for feature in FEATURES if not feature in redundant_features]\n",
        "print(f\"Number of features after: {len(FEATURES)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUApztnPpjXS"
      },
      "source": [
        "### 2.3. Correlations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jm7tSuuspjXS"
      },
      "source": [
        " $\\color{Aqua}{\\textsf{Short question}}$\n",
        "- Plot some features against the target property and calculate the Pearson and Spearman correlation coefficient (what is the different between those correlation coefficients?)\n",
        "- What are the strongest correlations? Did you expect them?\n",
        "- What can be a problem when features are correlated?\n",
        "- *Optional:* Do they change if you switch from CO$_2$ uptake at high pressure to low pressure `CO2 uptake at 0.15 bar and 298K`?  Explain your observation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpMpG0RDpjXS"
      },
      "source": [
        "To get the correlation matrices, you can use the `df.corr(method=)`method on your dataframe (`df`). You might want to calculate not the full correlation matrix but just the correlation of the features with the targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GB1O6LXUpjXS"
      },
      "source": [
        "<details>\n",
        "<summary> <font color='green'>Click here for a hint</font></summary>\n",
        "<ul>\n",
        "    <li> To get the correlation with a target, you can use indexing. E.g. <code>df.corr(method='spearman')[TARGET]</code></li>\n",
        "    <li> use <code>.sort_values()</code> method on the output of `df.corr()` to sort by the value of the correlation coefficient  </li>\n",
        "    <li> Scatter plot of TARGET vs. one descriptor (e.g., Density) </li>\n",
        "</ul>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kx8asGZfpjXT"
      },
      "outputs": [],
      "source": [
        "# add code here\n",
        "# Calculate the correlation of all features with the target\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2K0XAbobpjXT"
      },
      "source": [
        "## 3. Baselines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKFXt0DrpjXT"
      },
      "source": [
        "For machine learning, it is important to have some *baselines* to which one then compares the results of a model. Think of a classification model for some rare disease where we only have 1% postives. A classification model that only predictes the negatives *all the time* will still have a amazingly high accuracy. To be able to understand if our model is really better than such a simple prediction we need to make the simple prediction first. This is what we call a baseline.\n",
        "\n",
        "A baseline could be a really simple model, a basic heuristic or the current state of the art (SOTA).\n",
        "this. We will use a heuristic but if you aim for a publication, a baseline for you will be the state of the art model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cibtn4DMpjXT"
      },
      "source": [
        "For this we use sklearn `Dummy` objects that simply calculate the mean, the median or the most frequent case of the training set, when you run the `fit()` method on them (which takes the features matrix $\\mathbf{X}$ and the labels $\\mathbf{y}$ as arguments.\n",
        "This is, the prediction of a `DummyRegressor` with `mean` strategy will always be the mean, independent of the input (it will not look at the feature matrix!).\n",
        "\n",
        "Instead of using those `sklearn` objects you could also just manually compute the the mean or median of the dataset. But we will use those objects as we can learn in this way how to use estimators in `sklearn` and it is also allows you to test your full pipeline with different (baseline) models.\n",
        "What does this mean? In practice this means that you can use all the regression and classification models shown in the figure below in the same way, they will all have a `fit()` method that accepts `X` and `y` and a predict method that accepts `X` and returns the predictions.\n",
        "\n",
        "\n",
        "<img src=\"https://scikit-learn.org/1.3/_static/ml_map.png\" alt=\"ML Map\" width=\"800\"/>\n",
        "\n",
        "The estimator objects can be always used in the same way\n",
        "\n",
        "<img src=\"https://static.packt-cdn.com/products/9781789800265/graphics/d49a2e95-8f22-42ed-89f1-474b3d028787.png\" alt=\"ML Map\" width=\"400\"/>\n",
        "\n",
        "Using these objects, instead of the mean directly, allows you to easily swap them with other models in pipelines, where one chains many data transformation steps (see section 6)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_tAC4u2pjXT"
      },
      "source": [
        "### 3.1. Build dummy models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVTFu1-ypjXT"
      },
      "source": [
        " $\\color{Aqua}{\\textsf{Short question}}$\n",
        "- If you call `.fit(X, y)` on a `DummyRegressor` does it actually use the `X`? If not, why is there still the place for the `X` in the function? If yes, how does it use it?\n",
        "\n",
        " $\\color{Aqua}{\\textsf{Short question}}$\n",
        "- Create [`DummyRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyRegressor.html) instances for  `mean`, `median`. (e.g. `dummyinstance = DummyRegressor(strategy='mean')`)\n",
        "- Train them on the training data (`dummyinstance.fit(df_train[FEATURES], df_train[TARGET])`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ML_yfblypjXT"
      },
      "source": [
        "<details>\n",
        "<summary> <font color='green'>Click here for hints</font></summary>\n",
        "<ul>\n",
        "    <li> to create <code>DummyRegressor</code> you can for example use <code> dummyregressor_mean = DummyRegressor(strategy='mean') </code> </li>\n",
        "    <li> to see the implementation of the <code>DummyRegressor</code> you can check out <a href=\"https://github.com/scikit-learn/scikit-learn/blob/73732e5a0bc9b72c7049dc699d69aaedbb70ef0a/sklearn/dummy.py#L391\"> the source code on GitHub</a> </li>\n",
        "</ul>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8cmldBtpjXU"
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "# Build DummyRegressors\n",
        "dummyregressor_mean = DummyRegressor(strategy='mean')\n",
        "dummyregressor_median = DummyRegressor( #fillme"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XaGrD0ApjXU"
      },
      "outputs": [],
      "source": [
        "# Fit Dummy Regressors\n",
        "dummyregressor_mean.fit(df_train_stratified[FEATURES], df_train_stratified[TARGET])\n",
        "dummyregressor_median. #fillme"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UO2TFVkTpjXU"
      },
      "source": [
        "#### Evaluate the performance of the dummy models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6D9mRvCpjXU"
      },
      "source": [
        " $\\color{Aqua}{\\textsf{Short questions}}$\n",
        "- Calculate maximum error, mean absolute error and mean square error for the dummy regressors on training and test set. What would you expect those numbers to be?\n",
        "- Do the actual values surprise you?\n",
        "- What does this mean in practice for reporting of metrics/the reasoning behind using baseline models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZuHOeNrpjXU"
      },
      "source": [
        "It can be handy to store our metrics of choice in a nested dictionary ([Python dictionaries are key-value pairs](https://www.tutorialspoint.com/python/python_dictionary.htm)):\n",
        "\n",
        "```python\n",
        "{\n",
        "    'dummyestimator1': {\n",
        "                        'metric_a_key': metric_a_value,\n",
        "                        'metric_b_key': metric_b_value\n",
        "                    },\n",
        "    'dummyestimator2': {\n",
        "                        'metric_a_key': metric_a_value,\n",
        "                        'metric_b_key': metric_b_value\n",
        "                    },\n",
        " }\n",
        "```\n",
        "\n",
        "You will now write functions `get_regression_metrics(model, X, y_true)` that compute the metrics and return this dictionary for a given model. The `predict` method takes the feature matrix $\\mathbf{X}$ as input.\n",
        "\n",
        "In them, we calculate\n",
        "\n",
        "$\\mathrm {MAE} =\\frac{\\sum _{i=1}^{n}\\left|Y_{i}-\\hat{y}_{i}\\right|}{n}.$\n",
        "\n",
        ",\n",
        "\n",
        "$\\mathrm {MSE} = {\\frac {1}{n}}\\sum _{i=1}^{n}(Y_{i}-{\\hat {Y_{i}}})^{2}.$\n",
        "\n",
        "$\\mathrm{MAPE} = \\frac{1}{n} \\sum_{i=1}^{n} \\left| \\frac{Y_i - \\hat{Y}_i}{max (\\epsilon,Y_i)} \\right|$\n",
        "\n",
        "where $\\hat{y}$ are the predictions and, $Y_{i}$ the true values.\n",
        "\n",
        "We also include maximum error which is a good indication for generalization in many cases.\n",
        "\n",
        "See more information on [sklearn's Metrics and scoring: quantifying the quality of predictions](https://scikit-learn.org/stable/modules/model_evaluation.html#mean-absolute-percentage-error)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39z-nUqspjXU"
      },
      "source": [
        "<details>\n",
        "<summary> <font color='green'>Click here for hints</font></summary>\n",
        "<ul>\n",
        "    <li> to perform a prediction using a estimator object, you can call <code> classifier.predict(X) </code> </li>\n",
        "    <li> to calculate metrics, you can for example call <code>accuracy_score(true_values, predicted_values) </code> </li>\n",
        "</ul>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmIt45F0pjXU"
      },
      "outputs": [],
      "source": [
        "def get_regression_metrics(model, X, y_true):\n",
        "    \"\"\"\n",
        "    Get a dicionary with regression metrics:\n",
        "\n",
        "    model: sklearn model with predict method\n",
        "    X: feature matrix\n",
        "    y_true: ground truth labels\n",
        "    \"\"\"\n",
        "    y_predicted = #fillme\n",
        "\n",
        "    mae = #fillme\n",
        "    mse = #fillme\n",
        "    maximum_error = #fillme\n",
        "    mape =  #fillme\n",
        "\n",
        "    metrics_dict = {\n",
        "        'mae': mae,\n",
        "        'mse': mse,\n",
        "        'max_error': maximum_error,\n",
        "        'mape': mape\n",
        "    }\n",
        "\n",
        "    return metrics_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFIeciyJpjXV"
      },
      "outputs": [],
      "source": [
        "dummy_regressors = [\n",
        "    ('mean', dummyregressor_mean),\n",
        "    ('median', dummyregressor_median)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hagqeYjVpjXV"
      },
      "outputs": [],
      "source": [
        "dummy_regressor_results_test = {} # initialize empty dictionary\n",
        "dummy_regressor_results_train = {}\n",
        "\n",
        "# loop over the dummy_regressor list\n",
        "# if you have a tuple regressorname, regressor = (a, b) that is automatically expanded into the variables\n",
        "# a = regressorname, b = regressor\n",
        "for regressorname, regressor in dummy_regressors:\n",
        "    print(f\"Calculating metrics for {regressorname}\")\n",
        "    dummy_regressor_results_test[regressorname] = get_regression_metrics(regressor, df_test[FEATURES], df_test[TARGET])\n",
        "    dummy_regressor_results_train[regressorname] = get_regression_metrics(regressor, df_train[FEATURES], df_train[TARGET])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UX0vq9rfpjXV"
      },
      "outputs": [],
      "source": [
        "print(\"Dummy Regressor Results - Train & Test Set\")\n",
        "\n",
        "for regressorname, metrics in dummy_regressor_results_train.items():\n",
        "    print(f\"{regressorname} (Train): {metrics}\")\n",
        "\n",
        "for regressorname, metrics in dummy_regressor_results_test.items():\n",
        "    print(f\"{regressorname} (Test): {metrics}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sT3OS3AApjXV"
      },
      "source": [
        "## 4. Building a linear regression model\n",
        "\n",
        "In practice, we often rely on optimized libraries such as **scikit-learn** for machine learning models.  \n",
        "They are fast, robust, and widely used.  \n",
        "\n",
        "However, to really understand *what happens under the hood*, it is useful to **implement a simple linear regression model ourselves**.  \n",
        "\n",
        "### Goals for this section:\n",
        "- Implement **gradient descent** for linear regression.\n",
        "- Compare results between **gradient descent** and the **closed-form (normal equation)** solution.\n",
        "- Use **scikit-learn**'s `LinearRegression` as a baseline for comparison.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIrzMMXipjXV"
      },
      "source": [
        "### 4.1 sklearn linear regression\n",
        "\n",
        "Let's first use the sklearn model to get a good baseline for comparison.\n",
        "You can see with few lines below, we can train a machine learning model.\n",
        "The code is very flexible and you can essentially replace the linear regression with other models in sklearn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNzPHk2VpjXW"
      },
      "source": [
        "Write the code below to train and evaluate a linear regression model using sklearn's `LinearRegression` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69EsD4ubpjXW"
      },
      "outputs": [],
      "source": [
        "# Initialize the Linear Regression model\n",
        "linear_regressor = LinearRegression()\n",
        "\n",
        "# Train the model on the training data\n",
        "# FILLME\n",
        "\n",
        "# Evaluate the model on the training and test sets\n",
        "# FILLME\n",
        "\n",
        "# Print the results\n",
        "print(\"Linear Regression Results - Train Set:\", linear_regressor_results_train)\n",
        "print(\"Linear Regression Results - Test Set:\", linear_regressor_results_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4oZz-7CpjXW"
      },
      "source": [
        "### 4.1 Linear regression from scratch\n",
        "\n",
        "Now, let's code a linear regression to see if we understand all parts. Specifically, we are going to code the gradient descent for our regressor to fit its parameters.\n",
        "\n",
        "The class below supports:\n",
        "- `method=\"normal\"`: closed-form solution using the normal equation  \n",
        "- `method=\"gd\"`: gradient descent with configurable learning rate and iterations  \n",
        "- Optional standardization of features for smoother optimization  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a76mt55bpjXW"
      },
      "source": [
        "Code the missing part in the gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJOkbpmhpjXW"
      },
      "outputs": [],
      "source": [
        "class MyLinearRegression:\n",
        "    def __init__(self, fit_intercept=True, standardize=False, method=\"normal\",\n",
        "                 lr=1e-2, n_iters=100, tol=1e-8, random_state=0):\n",
        "        \"\"\"\n",
        "        method: \"normal\" (closed-form) or \"gd\" (gradient descent, MSE loss)\n",
        "        \"\"\"\n",
        "        self.fit_intercept = fit_intercept\n",
        "        self.standardize = standardize\n",
        "        self.method = method\n",
        "        self.lr = lr\n",
        "        self.n_iters = n_iters\n",
        "        self.tol = tol\n",
        "        self.random_state = random_state\n",
        "        self.coef_ = None\n",
        "        self.intercept_ = 0.0\n",
        "        self._x_mean = None\n",
        "        self._x_std = None\n",
        "\n",
        "    def _prepare_X(self, X, fit=False):\n",
        "        X = np.asarray(X, dtype=float)\n",
        "        if fit and self.standardize:\n",
        "            self._x_mean = X.mean(axis=0, keepdims=True)\n",
        "            self._x_std = X.std(axis=0, keepdims=True)\n",
        "            self._x_std[self._x_std == 0] = 1.0\n",
        "        if self.standardize:\n",
        "            Xs = (X - self._x_mean) / self._x_std\n",
        "        else:\n",
        "            Xs = X\n",
        "        if self.fit_intercept:\n",
        "            Xs = np.c_[np.ones((Xs.shape[0], 1)), Xs]\n",
        "        return Xs\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        rng = np.random.default_rng(self.random_state)\n",
        "        X = np.asarray(X, dtype=float)\n",
        "        y = np.asarray(y, dtype=float).reshape(-1,)\n",
        "\n",
        "        Xp = self._prepare_X(X, fit=True)\n",
        "\n",
        "        # NORMAL EQUATION\n",
        "        if self.method == \"normal\":\n",
        "            # theta = (X^T X)^{-1} X^T y  with ridge-like jitter for stability\n",
        "            XT_X = Xp.T @ Xp\n",
        "            # add tiny jitter to diagonal to avoid singular matrix in edge cases\n",
        "            jitter = 1e-12 * np.eye(XT_X.shape[0])\n",
        "            theta = np.linalg.pinv(XT_X + jitter) @ (Xp.T @ y)\n",
        "\n",
        "        # GRADIENT DESCENT\n",
        "        elif self.method == \"gd\":\n",
        "            # Add your code here\n",
        "            n_features = Xp.shape[1]\n",
        "            theta = rng.normal(scale=0.01, size=n_features)\n",
        "            prev_loss = np.inf\n",
        "            for i in range(self.n_iters):\n",
        "                y_pred = Xp @ theta\n",
        "                residuals = y_pred - y\n",
        "                loss = #FILLME\n",
        "                if abs(prev_loss - loss) < self.tol:\n",
        "                    break\n",
        "                prev_loss = loss\n",
        "                grad = (Xp.T @ residuals) / len(y)\n",
        "                theta -= #FILLME\n",
        "        else:\n",
        "            raise ValueError(\"method must be 'normal' or 'gd'\")\n",
        "\n",
        "        # unpack theta into intercept and coefficients\n",
        "        if self.fit_intercept:\n",
        "            self.intercept_ = float(theta[0])\n",
        "            self.coef_ = theta[1:]\n",
        "        else:\n",
        "            self.intercept_ = 0.0\n",
        "            self.coef_ = theta\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.asarray(X, dtype=float)\n",
        "        if self.standardize:\n",
        "            Xs = (X - self._x_mean) / self._x_std\n",
        "        else:\n",
        "            Xs = X\n",
        "        return self.intercept_ + Xs @ self.coef_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAVt0NP8pjXX"
      },
      "outputs": [],
      "source": [
        "# Initialize two Linear Regression model with different optimization methods\n",
        "my_linear_regressor_gd = MyLinearRegression(fit_intercept=True, standardize=True, method=\"gd\")\n",
        "my_linear_regressor_cf = MyLinearRegression(fit_intercept=True, standardize=True, method=\"normal\")\n",
        "\n",
        "# Train the model on the training data\n",
        "my_linear_regressor_gd.fit(df_train_stratified[FEATURES], df_train_stratified[TARGET])\n",
        "my_linear_regressor_cf.fit(df_train_stratified[FEATURES], df_train_stratified[TARGET])\n",
        "\n",
        "# Evaluate the model on the training and test sets\n",
        "my_linear_regressor_gd_results_test = get_regression_metrics(\n",
        "    my_linear_regressor_gd, df_train_stratified[FEATURES], df_train_stratified[TARGET]\n",
        ")\n",
        "my_linear_regressor_cf_results_test = get_regression_metrics(\n",
        "    my_linear_regressor_cf, df_test_stratified[FEATURES], df_test_stratified[TARGET]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTDaPhLKpjXX"
      },
      "outputs": [],
      "source": [
        "# comparing results\n",
        "print(\"Linear Regression Results - sklearn:\", linear_regressor_results_test)\n",
        "print(\"Linear Regression Results - cf:\", my_linear_regressor_cf_results_test)\n",
        "print(\"Linear Regression Results - gd:\", my_linear_regressor_gd_results_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHdZ8YKhpjXX"
      },
      "source": [
        "We can see how gradient descent results would be changing its parameters, namely learning rate and number of iterations.\n",
        "Use the sliders to change the **learning rate** and the **number of iterations**.  \n",
        "Weâ€™ll refit our custom gd model, plot the **loss vs. iteration**, and print evaluation metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wyf0jYXFpjXX"
      },
      "outputs": [],
      "source": [
        "my_linear_regressor_gd = MyLinearRegression(fit_intercept=True, standardize=True, method=\"gd\",lr=1e-2, n_iters=1000)\n",
        "my_linear_regressor_gd.fit(df_train_stratified[FEATURES], df_train_stratified[TARGET])\n",
        "my_linear_regressor_gd_results_test = get_regression_metrics(\n",
        "    my_linear_regressor_gd, df_train_stratified[FEATURES], df_train_stratified[TARGET]\n",
        ")\n",
        "print(\"Linear Regression Results - gd:\", my_linear_regressor_gd_results_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8A48G35pjXX"
      },
      "outputs": [],
      "source": [
        "from ipywidgets import interact, FloatLogSlider, IntSlider, fixed\n",
        "from IPython.display import display\n",
        "\n",
        "def run_gd_and_plot(lr, n_iters, df, FEATURES, TARGET, get_regression_metrics):\n",
        "    model = MyLinearRegression(\n",
        "        fit_intercept=True,\n",
        "        standardize=True,\n",
        "        method=\"gd\",\n",
        "        lr=lr,\n",
        "        n_iters=n_iters,\n",
        "        tol=1e-12,\n",
        "        random_state=0\n",
        "    )\n",
        "    model.fit(df[FEATURES].values, df[TARGET].values)\n",
        "\n",
        "    metrics = get_regression_metrics(model, df[FEATURES], df[TARGET])\n",
        "\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.xlabel(\"Iteration\")\n",
        "    plt.ylabel(\"Loss (MSE/2)\")\n",
        "    plt.title(f\"lr={lr:.1e}, iters={n_iters}, MAE={metrics['mae']:.2f}, MAPE={metrics['mape']:.2f}\")\n",
        "    plt.plot(df[TARGET].values, model.predict(df[FEATURES].values), 'o', alpha=0.3)\n",
        "    plt.plot(df[TARGET].values, df[TARGET].values, 'k--', label=\"y=x\")\n",
        "    plt.xlim(0.95*df[TARGET].min(), 1.05*df[TARGET].max())\n",
        "    plt.ylim(0.95*df[TARGET].min(), 1.05*df[TARGET].max())\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ap-J7nqOpjXX"
      },
      "outputs": [],
      "source": [
        "interact(\n",
        "    run_gd_and_plot,\n",
        "    lr=FloatLogSlider(value=1e-4, base=10, min=-5, max=0, step=0.1, description=\"learning rate\"),\n",
        "    n_iters=IntSlider(value=100, min=10, max=10000, step=50, description=\"iterations\"),\n",
        "    df=fixed(df_train_stratified),\n",
        "    FEATURES=fixed(FEATURES),\n",
        "    TARGET=fixed(TARGET),\n",
        "    get_regression_metrics=fixed(get_regression_metrics)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKEJv0DmpjXY"
      },
      "source": [
        "You can find more information about gradient descent in this paper: [A high-bias, low-variance introduction to Machine Learning for physicists](https://doi.org/10.1016/j.physrep.2019.03.001)\n",
        "\n",
        "Below is a schematic illustration from the paper showing how learning rate can affect the outcome of optimization.\n",
        "\n",
        "![Gradient Descent Illustration](https://github.com/AI4ChemS/CHE-1147/blob/main/assets/GD_figure.jpg?raw=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wC2KxnsApjXY"
      },
      "source": [
        "$\\color{Aqua}{\\textsf{Short Exercise}}$\n",
        "- what happens when the learning rate is too small?\n",
        "- What about when it is too large?\n",
        "\n",
        "<details>\n",
        "<summary> <font color='green'>Click here for a hint</font></summary>\n",
        "<ul>\n",
        "    <li> Too small leads to extremely slow convergance and too large can lead to overshoot!</li>\n",
        "</ul>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EojN6-jpjXY"
      },
      "source": [
        "## 5. Build a Kernel Ridge Regression Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9iI5kfGpjXY"
      },
      "source": [
        "Let's build a simple [kernel ridge regression (KRR)](https://emtiyaz.github.io/pcml15/kernel-ridge-regression.pdf) machine learning model and train it with our raw data.\n",
        "You can try different kernels, but we recommend to start with the Gaussian radial basis function ('rbf') kernel.\n",
        "\n",
        " $\\color{Aqua}{\\textsf{Short question}}$\n",
        "- Do you expect this model to perform better than the dummy models?\n",
        "- Train it and then calculate the performance metrics on the training and test set. How do they compare to the performance of the dummy models?\n",
        "- What is the shape of the Kernel and of the weights? (you can check your answer by looking at the `dual_coef_` attribute of the KRR instance. You can get shapes of objects using the `shape` atrribute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9_igu7GpjXY"
      },
      "outputs": [],
      "source": [
        "# Train the model with a Gaussian kernel\n",
        "krr = KernelRidge(kernel='rbf')\n",
        "krr.fit(#fillme)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GLPsC25pjXY"
      },
      "outputs": [],
      "source": [
        "# get the metrics on the train and the test set using the get_regression_metrics functions (as above)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7NI5WhSpjXY"
      },
      "source": [
        "## 6. Evaluate the model performance in detail"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jX6-WKpCpjXZ"
      },
      "source": [
        "We have trained our first machine learning model!\n",
        "We'll first have a closer look at its performance, before learning how to improve it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOqNGs5hpjXZ"
      },
      "source": [
        " $\\color{Aqua}{\\textsf{Short Exercise}}$\n",
        "- Create a parity plot (true values against predictions)for the training and test data\n",
        "- Plot a histogram of the distribution of the training and test errors on the training and test set. Plot the errors also as a function of the true value\n",
        "- Let's assume we would like to use our model for pre-screening a library of millions of porous materials to zoom-in on those with the most promising gas uptake. Could you tolerate the errors of your model?\n",
        "- Compare the parity plots for this model with the ones for the dummy models.\n",
        "Use the plotting functions below the evaluate all the following models you train.\n",
        "\n",
        "For this exercise, it can be handy to save the results in a dictionary, e.g.\n",
        "```(python)\n",
        "res_train = {\n",
        "    'y true': [],\n",
        "    'y pred': []\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuEK9qYXpjXZ"
      },
      "source": [
        "<details>\n",
        "<summary> <font color='green'>Click here for hints for plotting</font></summary>\n",
        "<ul>\n",
        "    <li> If you want to use matplotlib to make the parity plots, you can use the <a href=\"https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.hist2d.html\">hist2d function</a> </li>\n",
        "    <li> To create the frequencies and the edges of a histogram, one can use <code>np.histogram</code></li>\n",
        "</ul>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxrFl0FQpjXZ"
      },
      "outputs": [],
      "source": [
        "# Create dictionaries with training and test results to create parity plots\n",
        "res_train = {\n",
        "    'y true': # fillme using the dataframe,\n",
        "    'y pred': # fillme using the model prediction\n",
        "}\n",
        "\n",
        "res_test = {\n",
        "    'y true': # fillme using the dataframe\n",
        "    'y pred': # fillme using the model prediction\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHo84C35pjXZ"
      },
      "source": [
        "Now, lets calculate the errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYZD5H_IpjXZ"
      },
      "outputs": [],
      "source": [
        "res_train[\"error\"] = res_train[\"y true\"] - res_train[\"y pred\"]\n",
        "res_test[\"error\"] = res_test[\"y true\"] - res_test[\"y pred\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flNhG6N-pjXZ"
      },
      "source": [
        "Now, plot the parity plots and error distributions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGhHF_JppjXZ"
      },
      "source": [
        "<details>\n",
        "<summary> <font color='green'>Click here for hints for plotting</font></summary>\n",
        "If you want interactive plots, you can use the following code:\n",
        "<pre><code>\n",
        "hv.extension(\"bokeh\")\n",
        "hex_train = hv.HexTiles(res_train, [\"y true\", \"y pred\"]).hist(\n",
        "    dimension=[\"y true\", \"y pred\"]\n",
        ")\n",
        "hex_test = hv.HexTiles(res_test, [\"y true\", \"y pred\"]).hist(\n",
        "    dimension=[\"y true\", \"y pred\"]\n",
        ")\n",
        "hex_train + hex_test\n",
        "</code>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJBw5EMypjXZ"
      },
      "outputs": [],
      "source": [
        "# plot it\n",
        "hist_density(res_train['y true'], res_train['y pred'], xlabel='y true', ylabel='y pred', title='Train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AeVmGUirpjXa"
      },
      "outputs": [],
      "source": [
        "hist_density(res_test['y true'], res_test['y pred'], xlabel='y true', ylabel='y pred', title='Test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PV7jZO5hpjXa"
      },
      "source": [
        "## 7. Improve the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uTd7_oTpjXa"
      },
      "source": [
        "Our training set still has a couple of issues you might have noticed:\n",
        "- The feature values are not scaled (different features are measured in different units ...)\n",
        "- Some features are basically constant, i.e. do not contain relevant information and just increase the dimensionality of the problem\n",
        "- Some feature distributions are skewed (which is more relevant for some models than for others ...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oagcFk8PpjXa"
      },
      "source": [
        "$\\color{Aqua}{\\textsf{Short Question}}$\n",
        "- Why might the scaling of the features be relevant for a machine learning model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ok_8SRzspjXa"
      },
      "source": [
        "### 7.1. Standard scaling and building a first pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxtW2qv2pjXa"
      },
      "source": [
        "Given that we will now go beyond training a single model, we will build [Pipelines](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html), which are objects that can collect a selection of transformations and estimators. This makes it quite easy to apply the same set of operations to different datasets. A simple pipeline might be built as follows\n",
        "\n",
        "<img src=\"https://vitalflux.com/wp-content/uploads/2020/08/ML-Pipeline-Page-2-1024x307.png\" alt=\"Pipeline\" width=\"800\"/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_KaZtYmpjXa"
      },
      "source": [
        " $\\color{Aqua}{\\textsf{Short Exercise}}$\n",
        "- Build a pipline that first performs standard scaling and then fits a KRR. Call it `pipe_w_scaling`.\n",
        "- Fit it on the training set\n",
        "- Make predictions, calculate the errors and make the parity plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3p93_TympjXa"
      },
      "source": [
        "<details>\n",
        "<summary> <font color='green'>Click here for hints</font></summary>\n",
        "<ul>\n",
        "    <li> the <code>fit</code>, <code>predict</code> methods also work for pipelines </li>\n",
        "</ul>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ghqPWwXpjXb"
      },
      "outputs": [],
      "source": [
        "pipe_w_scaling = Pipeline(\n",
        "   [\n",
        "       ('scaling', StandardScaler()),\n",
        "       ('krr', #fillme)\n",
        "   ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iz3Ajxm0pjXb"
      },
      "source": [
        "### 7.2. Hyperparameter optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_MUkDMEpjXb"
      },
      "source": [
        "A key component we did not optimize so far are hyperparameters. Those are parameters of the model that we usually cannot learn from the data but have to fix before we train the model.\n",
        "Since we cannot learn those parameters it is not trivial to select them. Hence, what we typically do in practice is to create another set, a \"validation set\", and use it to test models trained with different hyperparameters.\n",
        "\n",
        "The most common approach to hyperparameter optimization is to define a grid of all relevant parameters and to search over the grid for the best model performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjqZYQVDpjXb"
      },
      "source": [
        "$\\color{Aqua}{\\textsf{Short Exercise}}$\n",
        "- Think about which parameters you could optimize in the pipeline. Note that your KRR model has two parameters you can optimize. You can also switch off some steps by setting them to `None'.\n",
        "- For each parameter you need to define a resonable grid to search over.\n",
        "- Recall, what k-fold cross-validation does. Run the hyperparameter optimization using 5-fold cross-validation (you can adjust the number of folds according to your computational resources/impatience. It turns out at k=10 is the [best tradeoff between variance and bias](https://arxiv.org/abs/1811.12808)).\n",
        "Tune the hyperparameters until you are statisfied (e.g., until you cannot improve the cross validated error any more)\n",
        "- Why don't we use the test set for hyperparameter tuning but instead test on the validation set?\n",
        "- Evaluate the model performance by calculating the performance metrics (MAE, MSE, max error) on the training and the test set.\n",
        "- *Optional:* Instead of grid search, try to use random search on the same grid (`RandomizedSearchCV`) and fix the number of evaluations (`n_iter`) to a fraction of the number of evaluations of grid search. What do you observe and conclude?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wf0wzTgfpjXb"
      },
      "source": [
        " $\\color{DarkRed}{\\textsf{Tips}}$\n",
        "- If you want to see what is happening, set the `verbosity` argument of the `GridSearchCV` object to a higher number.\n",
        "\n",
        "- If you want to speed up the optimization, you can run it in parallel by setting the `n_jobs` argument to the number of workers. If you set it to -1 it will use all available cores. *Using all cores might freeze your computer if you do not have enough memory*\n",
        "\n",
        "- If the optimization is too slow, reduce the number of data points in your set, the number of folds or the grid size. Note that it can also be a feasible strategy to first use a coarser grid and the a finer grid for fine-tuning.\n",
        "\n",
        "- For grid search, you need to define a parameter grid, which is a dictionary of the following form:\n",
        "```(python)\n",
        "param_grid = {\n",
        "                    'pipelinestage__parameter': np.logspace(-4,1,10),\n",
        "                    'pipelinestage': [None, TransformerA(), TransformerB()]\n",
        "            }\n",
        "```\n",
        "\n",
        "- After the search, you can access the best model with `.best_estimator_` and the best parameters with `.best_params_` on the GridSearchCV instance. For example `grid_krr.best_estimator_`\n",
        "\n",
        "- If you initialize the GridSearchCV instance with `refit=True` it will automatically train the model with all training data (and not only the training folds from cross-validations)\n",
        "\n",
        "The double underscore (dunder) notation works recursively and specifies the parameters for any pipeline stage.\n",
        "For example, `ovasvm__estimator__cls__C` would specifiy the `C` parameter of the estimator in the one-versus-rest classifier `ovasvm`.\n",
        "\n",
        "You can print all parameters of the pipeline using `print(sorted(pipeline.get_params().keys()))`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBrJTS8RpjXb"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "Be aware that tight grids will drastically increase the number of experiments you will run! In some cases, it can be useful to perform the optimization in steps, i.e., first use a coarse grid and then refine in interesting regions.\n",
        "Alternatively, there are approached like <a href=\"https://www.jmlr.org/papers/volume18/16-558/16-558.pdf\"> hyperband <a> that dynamically adjust the number of data points.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GP8Ji5LJpjXb"
      },
      "source": [
        "<details>\n",
        "<summary> <font color='green'>Click here for hints about pipelines and grid search</font></summary>\n",
        "<ul>\n",
        "    <li> You can use the <code>np.logspace</code> function to generate a grid for values that you want to vary on a logarithmic scale </li>\n",
        "    <li> There are two hyperparameters for KRR: the regularization strength <code>alpha</code> and the Gaussian width  <code>gamma</code> </li>\n",
        "    <li> For the regularization strength, values between 1 and 1e-3 can be reasonable. For gamma you can use the median heuristic, gamma = 1 / median, or values between 1e-3 and 1e3</li>\n",
        "</ul>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32VztEr5pjXc"
      },
      "outputs": [],
      "source": [
        "# Define the parameter grid and the grid search object\n",
        "param_grid = {\n",
        "                    'scaling': [MinMaxScaler(), StandardScaler()], # test different scaling methods\n",
        "                    'krr__alpha': #fillme,\n",
        "                    'krr__#fillme': #fillme\n",
        "            }\n",
        "\n",
        "grid_krr = GridSearchCV(#your pipeline, param_grid=param_grid,\n",
        "                        cv=#number of folds, verbose=2, n_jobs=2)\n",
        "\n",
        "# optional random search\n",
        "#random_krr = RandomizedSearchCV(#your pipeline, param_distributions=param_grid, n_iter=#number of evaluations,\n",
        "#                        cv=#number of folds, verbose=2, n_jobs=2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6JR7YVUpjXc"
      },
      "outputs": [],
      "source": [
        "# run the grid search by calling the fit method\n",
        "grid_krr.fit(#fillme)\n",
        "# optional random search\n",
        "# random_krr.fit(#fillme)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSVrXWYqpjXc"
      },
      "outputs": [],
      "source": [
        "# get the performance metrics\n",
        "get_regression_metrics(#fillme)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQzPpR3VpjXc"
      },
      "source": [
        "<details>\n",
        "<summary> <font color='green'>Click here for some more information about hyperparameter optimization</font></summary>\n",
        "Grid search is not the most efficient way to perform hyperparamter optimization. Even <a href=\"http://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf\">random search was shown to be more efficient</a>. Really efficient though are Bayesian optimization approaches like <a href='https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf)'>TPE</a>. This is implemented in the hyperopt library, which is also installed in your conda environment.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSpCwU0gpjXc"
      },
      "source": [
        "<details>\n",
        "<summary> <font color='green'>Click here for hyperparameter optimization with hyperopt (advanded and optional outlook)</font></summary>\n",
        "    \n",
        "<b>Import the tools we need</b>\n",
        "<code>\n",
        "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, mix, rand, anneal, space_eval\n",
        "from functools import partial\n",
        "</code>    \n",
        "\n",
        "<b>Define the grid</b>\n",
        "<code>\n",
        "param_hyperopt = {\n",
        "    \"krr__alpha\": hp.loguniform(\"krr__alpha\", np.log(0.001), np.log(10)),\n",
        "    \"krr__gamma\": hp.loguniform(\"krr__gamma\", np.log(0.001), np.log(10)),\n",
        "}\n",
        "</code>\n",
        "\n",
        "<b>Define the objective function</b>\n",
        "<code>\n",
        "def objective_function(params):\n",
        "    pipe.set_params(\n",
        "        **{\n",
        "            \"krr__alpha\": params[\"krr__alpha\"],\n",
        "            \"krr__gamma\": params[\"krr__gamma\"],\n",
        "        }\n",
        "    )\n",
        "    score = cross_val_score(\n",
        "        pipe, X_train, y_train, cv=10, scoring=\"neg_mean_absolute_error\"\n",
        "    ).mean()\n",
        "    return {\"loss\": -score, \"status\": STATUS_OK}\n",
        "</code>\n",
        "\n",
        "<b>We will use a search in which we mix random search, annealing and tpe</b>\n",
        "<code>\n",
        "trials = Trials()\n",
        "mix_search = partial(\n",
        "   mix.suggest,\n",
        "   p_suggest=[(0.15, rand.suggest), (0.15, anneal.suggest), (0.7, tpe.suggest)],\n",
        ")\n",
        "</code>\n",
        "\n",
        "<b>Now, we can minimize the objective function.</b>\n",
        "<code>\n",
        "best_param = fmin(\n",
        "        objective_function,\n",
        "        param_hyperopt,\n",
        "        algo=mix_search,\n",
        "        max_evals=MAX_EVALES,\n",
        "        trials=trials,\n",
        "        rstate=np.random.RandomState(RANDOM_SEED),\n",
        "    )\n",
        "</code>\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGdaNdDppjXc"
      },
      "source": [
        "## 10. Influence of Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3VqXghupjXc"
      },
      "source": [
        "## 8. Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAMGeq_fpjXc"
      },
      "source": [
        "Finally, we would like to remove features with low variance. This can be done by setting a variance threshold."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebseeUunpjXd"
      },
      "source": [
        "$\\color{Aqua}{\\textsf{Short Question}}$\n",
        "    \n",
        "- What is the reasoning behind doing this?\n",
        "- When might it go wrong and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uX5CeCuApjXd"
      },
      "source": [
        " $\\color{Aqua}{\\textsf{Short Exercise}}$\n",
        "- Add a variance threshold to the pipeline (select the correct function argument)\n",
        "- Use random search for hyperparameter optimization, retrain the pipeline, and calculate the performance metrics (max error, MAE, MSE) on the training and test set\n",
        "- If you could improve the predictive performance, do not forget to also run the model for the Kaggle competition!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9a_pL_epjXd"
      },
      "outputs": [],
      "source": [
        "# Define the pipeline\n",
        "pipe_variance_threshold = Pipeline(\n",
        "    # fillme with the pipeline steps\n",
        "    [\n",
        "        ('variance_treshold', VarianceThreshold(#fillme with threshold)),\n",
        "        #fillme with remaining pipeline steps\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzo97iPzpjXd"
      },
      "outputs": [],
      "source": [
        "param_grid_variance_threshold = {\n",
        "                    'scaling': [None, StandardScaler()],\n",
        "                    'krr__alpha': #fillme,\n",
        "                    'krr__#fillme': #fillme,\n",
        "                    'variance_treshold__threshold': #fillme\n",
        "            }\n",
        "\n",
        "random_variance_treshold = RandomizedSearchCV(#your pipeline, param_distributions=param_grid, n_iter=#number of evaluations,\n",
        "                        cv=#number of folds, verbose=2, n_jobs=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gqnzTtdpjXd"
      },
      "outputs": [],
      "source": [
        "# Fit the pipeline and run the evaluation\n",
        "random_variance_treshold.fit(#fillme)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oN7GiLOwpjXd"
      },
      "source": [
        "$\\color{Aqua}{\\textsf{Short Exercise (optional)}}$\n",
        "- replace the variance threshold with a model-based feature selection\n",
        "`('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\")))` or [any feature selection method that you would like to try](https://scikit-learn.org/stable/modules/feature_selection.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hi6Ysb1rpjXd"
      },
      "source": [
        "## 9. Saving the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqSaaOREpjXd"
      },
      "source": [
        "Now, that we spent so much time in optimizing our model, we do not want to loose it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPKJGpvypjXd"
      },
      "source": [
        " $\\color{Aqua}{\\textsf{Short Exercise}}$\n",
        "- use the [joblib library](https://scikit-learn.org/stable/modules/model_persistence.html) to save your model\n",
        "- make sure you can load it again\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SI3NWskgpjXe"
      },
      "outputs": [],
      "source": [
        "# Dump your model\n",
        "joblib.dump(model, filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1p952S6pjXe"
      },
      "outputs": [],
      "source": [
        "# Try to load it again\n",
        "model_loaded = joblib.load(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-V3c20N0pjXe"
      },
      "source": [
        " $\\color{Aqua}{\\textsf{Short Exercise}}$\n",
        "- what happens if you set $\\alpha$  to a really small or to large value? Why is this the case explain what the parameter means using the equation derived in the lectures?\n",
        "\n",
        " To test this, fix this value in one of your pipelines, retrain the models (re-optimizing the other hyperparameters) and rerun the performance evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81a6gjt7pjXe"
      },
      "source": [
        "<details>\n",
        "<summary> <font color='green'>Click here for hints</font></summary>\n",
        "<ul>\n",
        "    <li> Check the derivation for ridge regression and KRR in the notes. </li>\n",
        "    <li> Also remember the loss landscapes we discussed in the lectures about LASSO. </li>\n",
        "</ul>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpBnDMjBpjXe"
      },
      "source": [
        "## 11. Submit your best model to Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_khTbXGpjXe"
      },
      "source": [
        "Join the [Kaggle competition](https://www.kaggle.com/competitions/molsim-2024-ml-challenge/host/launch-checklist) for this course!\n",
        "For this you can:\n",
        "- try to continue optimizing your KRR model\n",
        "- try to use a new model ([browse the sklearn documentation](https://scikit-learn.org/) for ideas or check out [xgboost](https://xgboost.readthedocs.io/en/stable/)\n",
        "\n",
        "The important parts for us here are:\n",
        "- that you make an attempt to improve your model, discuss this attempt, and use proper models to measure potential improvement\n",
        "- we will not grade you based on how \"fancy\" or model is or how well it performs but rather on whether you do something reasonable that is well motivated in your discussion\n",
        "- you do not need to try both a model and continue optimizing your model. Doing one of them is, in principle, \"enough\"\n",
        "\n",
        "Use then your best model to create a `submission.csv` with your predictions to join the competition and upload it to the competition site.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJChlU8tpjXe"
      },
      "outputs": [],
      "source": [
        "kaggle_data = pd.read_csv('data/features.csv')\n",
        "kaggle_predictions = #fillme.predict(kaggle_data[FEATURES])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpwyGiVMpjXe"
      },
      "outputs": [],
      "source": [
        "submission = pd.DataFrame({\"id\": kaggle_data[\"id\"], \"prediction\": kaggle_predictions})\n",
        "\n",
        "submission.to_csv(\"submission.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZR3snQ5epjXf"
      },
      "source": [
        "## 12. Interpreting the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeLeB912pjXf"
      },
      "source": [
        "Now, that our model performs decently, we would like to know which features are mainly responsible for this, i.e. how the model performs its reasoning.\n",
        "> In CHE1147, we get back to this part in a couple of weeks. See course schedule for details.\n",
        "\n",
        "One method to do so is the [permutation feature importance technique](https://christophm.github.io/interpretable-ml-book/feature-importance.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUiv4S2mpjXf"
      },
      "source": [
        "$\\color{Aqua}{\\textsf{Short question}}$\n",
        "\n",
        "We use both descriptors that encode the pore geometry (density, pore diameters, surface areas) as well as some that describe the chemistry of the MOF (the RACs).\n",
        "- Would you expect the relative importance of these features to be different for prediction of gas adsorption at high vs low gas pressure?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5o81eyjpjXf"
      },
      "source": [
        "<details>\n",
        "<summary> <font color='green'>Click here for a hint</font></summary>\n",
        "<ul>\n",
        "    <li> <a href=\"https://pubs.acs.org/doi/abs/10.1021/acs.chemmater.8b02257\">An article from Diego et al.</a> (10.1021/acs.chemmater.8b02257) gives some hints.</li>\n",
        "</ul>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qPKxysjpjXf"
      },
      "source": [
        " $\\color{Aqua}{\\textsf{Short Exercise}}$\n",
        "- Complete the function `_calculate_permutation_scores` (which we took from the `sklearn` package) and which is needed to calculate the permutation feature importance using the `permutation_importance` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8kC4gozpjXf"
      },
      "outputs": [],
      "source": [
        "def _calculate_permutation_scores(estimator, X, y, col_idx, random_state,\n",
        "                                  n_repeats, scorer):\n",
        "    \"\"\"Calculate score when `col_idx` is permuted. Based on the sklearn implementation\n",
        "\n",
        "    estimator: sklearn estimator object\n",
        "    X: pd.Dataframe or np.array\n",
        "    y: pd.Dataframe or np.array\n",
        "    col_idx: int\n",
        "    random_state: int\n",
        "    n_repeats: int\n",
        "    scorer: function that takes model, X and y_true as arguments\n",
        "    \"\"\"\n",
        "    random_state = check_random_state(random_state)\n",
        "\n",
        "    X_permuted = X.copy()\n",
        "    scores = np.zeros(n_repeats)\n",
        "    # get the indices\n",
        "    shuffling_idx = np.arange(X.shape[0])\n",
        "    for n_round in range(n_repeats):\n",
        "        # FILL BELOW HERE\n",
        "        # shuffle them (fill in what you want to shuffle)\n",
        "        random_state.shuffle(#fillme)\n",
        "\n",
        "        # Deal with dataframes\n",
        "        if hasattr(X_permuted, \"iloc\"):\n",
        "            # .iloc selects the indices from a dataframe and you give it [row, column]\n",
        "            col = X_permuted.iloc[shuffling_idx, col_idx]\n",
        "            col.index = X_permuted.index\n",
        "            X_permuted.iloc[:, col_idx] = col\n",
        "\n",
        "        # Deal with numpy arrays\n",
        "        else:\n",
        "            # FILL BELOW HERE\n",
        "            # array indexing is [row, column]\n",
        "            X_permuted[:, col_idx] = X_permuted[#fillme]\n",
        "\n",
        "        # Get the scores\n",
        "        feature_score = scorer(estimator, X_permuted, y)\n",
        "\n",
        "        # record the scores in array\n",
        "        scores[n_round] = feature_score\n",
        "\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCnNmxWSpjXf"
      },
      "source": [
        "Nothing to change in the function below, it just call the `_calculate_permutation_scores` function you just completed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLmwcAd8pjXf"
      },
      "outputs": [],
      "source": [
        "def permutation_importance(\n",
        "    estimator,\n",
        "    X,\n",
        "    y,\n",
        "    scoring=\"neg_mean_absolute_error\",\n",
        "    n_repeats=5,\n",
        "    n_jobs=2,\n",
        "    random_state=None,\n",
        "):\n",
        "    \"\"\"Permutation importance for feature evaluation\n",
        "    estimator : object\n",
        "        An estimator that has already been :term:`fitted` and is compatible\n",
        "        with :term:`scorer`.\n",
        "    X : ndarray or DataFrame, shape (n_samples, n_features)\n",
        "        Data on which permutation importance will be computed.\n",
        "    y : array-like or None, shape (n_samples, ) or (n_samples, n_classes)\n",
        "        Targets for supervised or `None` for unsupervised.\n",
        "    scoring : string, callable or None, default=None\n",
        "        Scorer to use. It can be a single\n",
        "        string (see :ref:`scoring_parameter`) or a callable (see\n",
        "        :ref:`scoring`). If None, the estimator's default scorer is used.\n",
        "    n_repeats : int, default=5\n",
        "        Number of times to permute a feature.\n",
        "    n_jobs : int or None, default=2\n",
        "        The number of jobs to use for the computation.\n",
        "        `None` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
        "        `-1` means using all processors. See :term:`Glossary <n_jobs>`\n",
        "        for more details.\n",
        "    random_state : int, RandomState instance, or None, default=None\n",
        "        Pseudo-random number generator to control the permutations of each\n",
        "        feature. See :term:`random_state`.\n",
        "    \"\"\"\n",
        "    # Deal with dataframes\n",
        "    if not hasattr(X, \"iloc\"):\n",
        "        X = check_array(X, force_all_finite=\"allow-nan\", dtype=None)\n",
        "\n",
        "    # Precompute random seed from the random state to be used\n",
        "    # to get a fresh independent RandomState instance for each\n",
        "    # parallel call to _calculate_permutation_scores, irrespective of\n",
        "    # the fact that variables are shared or not depending on the active\n",
        "    # joblib backend (sequential, thread-based or process-based).\n",
        "    random_state = check_random_state(random_state)\n",
        "    random_seed = random_state.randint(np.iinfo(np.int32).max + 1)\n",
        "\n",
        "    # Determine scorer from user options.\n",
        "    scorer = check_scoring(estimator, scoring=scoring)\n",
        "    # get the performance score on the unpermuted data\n",
        "    baseline_score = scorer(estimator, X, y)\n",
        "\n",
        "    # run the permuted evaluations in parallel for each column\n",
        "    scores = Parallel(n_jobs=n_jobs)(\n",
        "        delayed(_calculate_permutation_scores)(\n",
        "            estimator, X, y, col_idx, random_seed, n_repeats, scorer\n",
        "        )\n",
        "        for col_idx in range(X.shape[1])\n",
        "    )\n",
        "\n",
        "    # get difference two\n",
        "    importances = baseline_score - np.array(scores)\n",
        "\n",
        "    # return the results (dictionary)\n",
        "    return Bunch(\n",
        "        importances_mean=np.mean(importances, axis=1),\n",
        "        importances_std=np.std(importances, axis=1),\n",
        "        importances=importances,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FW83-u9pjXg"
      },
      "source": [
        " $\\color{Aqua}{\\textsf{Short Exercise}}$\n",
        "- Use your function to find the five most important features.\n",
        "- Which are they? Did you expect this result?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcMSfCvLpjXg"
      },
      "outputs": [],
      "source": [
        "permutation_results = permutation_importance(#fillme)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKK88mVKpjXg"
      },
      "outputs": [],
      "source": [
        "permutation_results[\"features\"] = FEATURES\n",
        "bars = hv.Bars(\n",
        "    permutation_results, \"features\", [\"importances_mean\", \"importances_std\"]\n",
        ").sort(\"importances_mean\", reverse=True)\n",
        "errors = hv.ErrorBars(\n",
        "    permutation_results, \"features\", vdims=[\"importances_mean\", \"importances_std\"]\n",
        ").sort(\"importances_mean\", reverse=True)\n",
        "\n",
        "bars * errors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nqnelg44pjXg"
      },
      "source": [
        "<details>\n",
        "<summary> <font color='green'>Click here for hints</font></summary>\n",
        "<ul>\n",
        "    <li> To get the top <emph>n</emph> indices of an array <code>a</code>, you can use <code>np.argsort(a)[-n:]</code></li>\n",
        "    <li> Get the feature names from the <code>FEATURES</code> list </li>\n",
        "    <li> combined this might look like <code>np.array(FEATURES)[np.argsort(a)[-n:]]</code></li>\n",
        "</ul>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fbSomZ1pjXg"
      },
      "source": [
        "<details>\n",
        "<summary> <font color='green'>Click here for more information on model interpretation</font></summary>\n",
        "The permutation feature importance technique is not a silver bullet, e.g. there are issues with correlated features.\n",
        "However, it is likely <a href='https://explained.ai/rf-importance/'>a better choice than feature importance, like impurity decrease, derived from random forests</a>).\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iifHn7dpjXg"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUPtkD_BpjXg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gx7LIb8VpjXh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "che1147",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}